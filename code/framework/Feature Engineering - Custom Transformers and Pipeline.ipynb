{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Feature Engineering - Custom Transformers and Pipeline.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PRLLKfNHEfjG","colab_type":"text"},"source":["Project: Forecasting Patient Enrolment for Clinical Trials\n","\n","Supervisor: Niklas Frühauf, Sovanta\n","\n","Authors:\n","*   Luka Biedebach\n","*   Weiyi Chen\n","*   Giang Hoang\n","*   Carolin Holtermann\n","*   Stefan Sousa"]},{"cell_type":"markdown","metadata":{"id":"D_1_RUYwO0jN","colab_type":"text"},"source":["#Preparation and Data Retrieval\n","\n"]},{"cell_type":"code","metadata":{"id":"AP20TRVvDNAL","colab_type":"code","colab":{}},"source":["from tqdm import tqdm                        # show progress in iterations\n","import pandas as pd\n","\n","#pd.set_option('display.max_columns', None)  # show all columns of dataframe"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XHkJ_s0PG23X","colab_type":"text"},"source":["## Initialize MongoDB instances\n","Note: sometimes it does not work directly, you have to restart the runtime"]},{"cell_type":"code","metadata":{"id":"mTF3mAs5BbC2","colab_type":"code","colab":{}},"source":["from pymongo import MongoClient\n","import math\n","!pip install dnspython\n","\n","# MongoDB Collections: trials, country, hospitals, populationDensity\n","client = MongoClient(\"mongodb+srv://sovanta:Si8T8TtsViHYenjx@clinicaltrials-exomh.mongodb.net/test?retryWrites=true&w=majority\")\n","db = client['clinical-trials']\n","\n","# MongoDB Collections: regional, universities, worldrank\n","client2 = MongoClient(\"mongodb+srv://sovanta:14FgeCSTCZF9HNVw@cluster0-3yb0b.mongodb.net/test?retryWrites=true&w=majority\")\n","db2 = client2['clinical-trials']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yb6duDkRjaer","colab_type":"text"},"source":["##Retrieve clinical trials data"]},{"cell_type":"code","metadata":{"id":"cXCeUL9tjTwh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":496},"executionInfo":{"status":"ok","timestamp":1599214077053,"user_tz":-120,"elapsed":26742,"user":{"displayName":"陳維儀","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjADfM2DPAarNhBjfMwzTdbc4NzhB0rPW1yeU7ETNM=s64","userId":"06691591176760802855"}},"outputId":"c3f25dc6-c226-4c1e-e880-389b1532f5a5"},"source":["# Define Criteria and Projection\n","criteria = {\n","    '$and': [\n","      {'LocationCountry': { '$not': {'$size': 0}}},                                         # at least one country\n","      {'LocationFacility': { '$not': {'$size': 0}}},                                        # at least one facility\n","      {'Condition': { '$not': {'$size': 0}}},                                               # at least one condition\n","      {'EnrollmentCount': {'$ne': 0}},                                                      # Number of patients > 0\n","      {'EnrollmentCount': {'$ne': None}},                                                   # EnrollmentCount != Null (None) - Actually this feature doesn't have any missing value\n","      {'EligibilityCriteria': {'$ne': None}},                                               # Only trials with EligibilityCriteria\n","      {'EligibilityCriteria': {'$ne': []}},                                                 # Only trials with EligibilityCriteria\n","      {'EnrollmentDuration': {'$ne': 0}},                                                   # Duration > 0 (month)\n","      {'EnrollmentDuration': {'$ne': None}},                                                # EnrollmentDuration != Null (None)\n","      {'Phase': { '$ne': ['Phase 1'], '$ne': ['Early Phase 1'], '$ne': ['Not Applicable']}}, # Not accepting phase 1, early phase 1 and not applicable. Accepting multiple-phase studies\n","      {'NCTId': {'$ne': 'NCT00001132'}}                                                     # Exception\n","    ]}\n","\n","projectionTrials = {'_id': 0,\n","              \"NCTId\": 1,\n","              \"OfficialTitle\": 1,\n","              \"BriefSummary\": 1,\n","              \"OrgClass\": 1,\n","              \"Condition\": 1,\n","              \"LeadSponsorClass\": 1,\n","              \"CollaboratorClass\": 1,\n","              \"EligibilityCriteria\": 1,\n","              \"EnrollmentType\": 1,\n","              \"EnrollmentCount\": 1,\n","              \"Phase\": 1,\n","              \"HealthyVolunteers\": 1,\n","              \"Gender\": 1,\n","              \"StdAge\": 1,\n","              \"LocationFacility\": 1,\n","              \"LocationCity\": 1,\n","              \"LocationCountry\": 1,\n","              \"IsFDARegulatedDrug\": 1,\n","              \"IsFDARegulatedDevice\": 1,\n","              \"ArmGroupLabel\": 1,\n","              \"EnrollmentDuration\": 1 ,\n","              \"InterventionName\": 1,\n","              \"StartDate\" : 1,   \n","              \"DesignPrimaryPurpose\" : 1,\n","              \"MaximumAge\" : 1,\n","              \"MinimumAge\" : 1,\n","              \"OrgFullName\": 1,\n","              \"ConditionAncestorTerm\": 1,\n","              \"ConditionBrowseBranchAbbrev\":1,\n","              \"ConditionMeshId\":1,\n","              \"LocationState\": 1,\n","              \"LocationZip\": 1,\n","              \"LocationPopulationDensity\": 1,\n","              \"LeadSponsorName\": 1,\n","\n","              # Newly added features\n","              \"InterventionType\": 1,\n","              \"CollaboratorName\": 1,\n","              \"ArmGroupType\" : 1,\n","              \"BaselineDenomCountGroupId\": 1,\n","              \"BaselineMeasureDispersionType\" : 1,\n","              \"DesignAllocation\" : 1,\n","              \"DesignInterventionModel\" : 1,\n","              \"Keyword\" : 1,\n","\n","              \"EventsTimeFrame\": 1,\n","              \"FlowDropWithdrawType\": 1,\n","              \"FlowGroupDescription\": 1,\n","              \"FlowGroupTitle\": 1,\n","              \"FlowMilestoneType\": 1,\n","              \"FlowPeriodTitle\": 1,\n","              \"FlowRecruitmentDetails\": 1,\n","              \"ArmGroupDescription\": 1,\n","              \"ArmGroupInterventionName\": 1,\n","              \"ArmGroupLabel\": 1,\n","              \"BaselineCategoryTitle\": 1,\n","              \"BaselineClassTitle\": 1,\n","              \"BaselineGroupDescription\": 1,\n","              \"BaselineGroupTitle\": 1,\n","              \"BaselineMeasureTitle\": 1,\n","              \"BaselineMeasureUnitOfMeasure\": 1  \n","              }\n","\n","#df_raw = pd.DataFrame(list(db.trials.find(filter=criteria, projection=projectionTrials).limit(1000)))\n","df_raw = pd.DataFrame(list(db.trials.find(filter=criteria, projection=projectionTrials)))\n","\n","print(f\"Number of trials: {len(df_raw)}\")\n","df_raw.head(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of trials: 30973\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>NCTId</th>\n","      <th>OrgFullName</th>\n","      <th>OrgClass</th>\n","      <th>Phase</th>\n","      <th>StartDate</th>\n","      <th>Condition</th>\n","      <th>ConditionAncestorTerm</th>\n","      <th>ConditionBrowseBranchAbbrev</th>\n","      <th>ConditionMeshId</th>\n","      <th>LeadSponsorName</th>\n","      <th>LeadSponsorClass</th>\n","      <th>CollaboratorName</th>\n","      <th>CollaboratorClass</th>\n","      <th>EligibilityCriteria</th>\n","      <th>EnrollmentCount</th>\n","      <th>EnrollmentType</th>\n","      <th>HealthyVolunteers</th>\n","      <th>Gender</th>\n","      <th>StdAge</th>\n","      <th>MinimumAge</th>\n","      <th>MaximumAge</th>\n","      <th>LocationFacility</th>\n","      <th>LocationCity</th>\n","      <th>LocationState</th>\n","      <th>LocationZip</th>\n","      <th>LocationCountry</th>\n","      <th>InterventionType</th>\n","      <th>InterventionName</th>\n","      <th>IsFDARegulatedDevice</th>\n","      <th>IsFDARegulatedDrug</th>\n","      <th>EventsTimeFrame</th>\n","      <th>FlowDropWithdrawType</th>\n","      <th>FlowGroupDescription</th>\n","      <th>FlowGroupTitle</th>\n","      <th>FlowMilestoneType</th>\n","      <th>FlowPeriodTitle</th>\n","      <th>FlowRecruitmentDetails</th>\n","      <th>ArmGroupDescription</th>\n","      <th>ArmGroupInterventionName</th>\n","      <th>ArmGroupLabel</th>\n","      <th>ArmGroupType</th>\n","      <th>BaselineCategoryTitle</th>\n","      <th>BaselineClassTitle</th>\n","      <th>BaselineDenomCountGroupId</th>\n","      <th>BaselineGroupDescription</th>\n","      <th>BaselineGroupTitle</th>\n","      <th>BaselineMeasureDispersionType</th>\n","      <th>BaselineMeasureTitle</th>\n","      <th>BaselineMeasureUnitOfMeasure</th>\n","      <th>DesignAllocation</th>\n","      <th>DesignInterventionModel</th>\n","      <th>DesignPrimaryPurpose</th>\n","      <th>Keyword</th>\n","      <th>EnrollmentDuration</th>\n","      <th>LocationPopulationDensity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>NCT00000143</td>\n","      <td>Johns Hopkins Bloomberg School of Public Health</td>\n","      <td>OTHER</td>\n","      <td>[Phase 3]</td>\n","      <td>May 1997</td>\n","      <td>[Cytomegalovirus Retinitis, HIV Infections]</td>\n","      <td>[Virus Diseases, Retinal Diseases, Eye Disease...</td>\n","      <td>[BC01, All, BC02, BC20, BC11, Rare]</td>\n","      <td>[D000017726, D000012173]</td>\n","      <td>[Johns Hopkins Bloomberg School of Public Health]</td>\n","      <td>[OTHER]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[Inclusion criteria:\\n\\nAge 13 years or older\\...</td>\n","      <td>61</td>\n","      <td>Actual</td>\n","      <td>No</td>\n","      <td>All</td>\n","      <td>[Child, Adult, Older Adult]</td>\n","      <td>13 Years</td>\n","      <td>None</td>\n","      <td>[Department of Ophthalmology, University of Ca...</td>\n","      <td>[Irvine, La Jolla, Los Angeles, Los Angeles, S...</td>\n","      <td>[California, California, California, Californi...</td>\n","      <td>[92697-4375, 92093-0946, 90033, 90095-7003, 94...</td>\n","      <td>[United States, United States, United States, ...</td>\n","      <td>[Device, Drug]</td>\n","      <td>[Ganciclovir implant and oral ganciclovir, Cid...</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>3 years</td>\n","      <td>[]</td>\n","      <td>[Ganciclovir device and oral dose of Ganciclov...</td>\n","      <td>[Ganciclovir Implant and Oral Ganciclovir, Cid...</td>\n","      <td>[STARTED, COMPLETED, NOT COMPLETED]</td>\n","      <td>[Overall Study]</td>\n","      <td>June 1997</td>\n","      <td>[Ganciclovir device and oral dose of Ganciclov...</td>\n","      <td>[Device: Ganciclovir implant and oral ganciclo...</td>\n","      <td>[Ganciclovir implant and oral ganciclovir, Cid...</td>\n","      <td>[Experimental, Experimental]</td>\n","      <td>[&lt;=18 years, Between 18 and 65 years, &gt;=65 yea...</td>\n","      <td>[United States]</td>\n","      <td>[BG000, BG001, BG002]</td>\n","      <td>[Ganciclovir device and oral dose of Ganciclov...</td>\n","      <td>[Ganciclovir Implant and Oral Ganciclovir, Cid...</td>\n","      <td>[]</td>\n","      <td>[Age, Categorical, Sex: Female, Male, Region o...</td>\n","      <td>[Participants, Participants, participants]</td>\n","      <td>Randomized</td>\n","      <td>Parallel Assignment</td>\n","      <td>Treatment</td>\n","      <td>[]</td>\n","      <td>37</td>\n","      <td>[2114.408936, 1189.4198, 2287.767578, 3559.750...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NCT00000170</td>\n","      <td>Jaeb Center for Health Research</td>\n","      <td>OTHER</td>\n","      <td>[Phase 3]</td>\n","      <td>April 1999</td>\n","      <td>[Amblyopia]</td>\n","      <td>[Brain Diseases, Central Nervous System Diseas...</td>\n","      <td>[BC10, BC11, BC23, All]</td>\n","      <td>[D000000550]</td>\n","      <td>[Jaeb Center for Health Research]</td>\n","      <td>[OTHER]</td>\n","      <td>[National Eye Institute (NEI)]</td>\n","      <td>[NIH]</td>\n","      <td>[Inclusion Criteria:\\n\\nPatients must be 7 yea...</td>\n","      <td>419</td>\n","      <td>Actual</td>\n","      <td>No</td>\n","      <td>All</td>\n","      <td>[Child]</td>\n","      <td>None</td>\n","      <td>6 Years</td>\n","      <td>[Wilmer Eye Institute]</td>\n","      <td>[Baltimore]</td>\n","      <td>[Maryland]</td>\n","      <td>[21287-9028]</td>\n","      <td>[United States]</td>\n","      <td>[Drug, Device]</td>\n","      <td>[Atropine, Eye Patch]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>None</td>\n","      <td>[Atropine]</td>\n","      <td>[Device: Eye Patch, Drug: Atropine]</td>\n","      <td>[Patching, Atropine]</td>\n","      <td>[Active Comparator, Active Comparator]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>Randomized</td>\n","      <td>Parallel Assignment</td>\n","      <td>Treatment</td>\n","      <td>[Amblyopia, patching, atropine]</td>\n","      <td>172</td>\n","      <td>[1717.821167]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>NCT00000177</td>\n","      <td>National Institute on Aging (NIA)</td>\n","      <td>NIH</td>\n","      <td>[Phase 3]</td>\n","      <td>October 1995</td>\n","      <td>[Alzheimer Disease]</td>\n","      <td>[Dementia, Brain Diseases, Central Nervous Sys...</td>\n","      <td>[BC10, BXM, All, Rare]</td>\n","      <td>[D000000544]</td>\n","      <td>[National Institute on Aging (NIA)]</td>\n","      <td>[NIH]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[Inclusion Criteria:\\n\\nWomen with a diagnosis...</td>\n","      <td>120</td>\n","      <td>None</td>\n","      <td>No</td>\n","      <td>Female</td>\n","      <td>[Adult, Older Adult]</td>\n","      <td>60 Years</td>\n","      <td>None</td>\n","      <td>[University of Alabama, Birmingham, University...</td>\n","      <td>[Birmingham, San Diego, Jacksonville, Tampa, A...</td>\n","      <td>[Alabama, California, Florida, Florida, Georgi...</td>\n","      <td>[35294-0017, 92093, 32225, 30329, 60612, 62702...</td>\n","      <td>[United States, United States, United States, ...</td>\n","      <td>[Drug]</td>\n","      <td>[Estrogen]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>None</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>Randomized</td>\n","      <td>None</td>\n","      <td>Treatment</td>\n","      <td>[Alzheimer's disease, Estrogen]</td>\n","      <td>39</td>\n","      <td>[230.015213, 1189.4198, 567.188477, 494.567383...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NCT00000188</td>\n","      <td>University of Pennsylvania</td>\n","      <td>OTHER</td>\n","      <td>[Phase 2]</td>\n","      <td>September 1994</td>\n","      <td>[Cocaine-Related Disorders]</td>\n","      <td>[Substance-Related Disorders, Chemically-Induc...</td>\n","      <td>[BC25, BXM, All]</td>\n","      <td>[D000019970]</td>\n","      <td>[University of Pennsylvania]</td>\n","      <td>[OTHER]</td>\n","      <td>[National Institute on Drug Abuse (NIDA)]</td>\n","      <td>[NIH]</td>\n","      <td>[Please contact site for information.]</td>\n","      <td>50</td>\n","      <td>Actual</td>\n","      <td>No</td>\n","      <td>All</td>\n","      <td>[Child, Adult, Older Adult]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>[PDVAMC Treatment Research Center]</td>\n","      <td>[Philadelphia]</td>\n","      <td>[Pennsylvania]</td>\n","      <td>[19104]</td>\n","      <td>[United States]</td>\n","      <td>[Drug]</td>\n","      <td>[Selegiline]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>None</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>Randomized</td>\n","      <td>Parallel Assignment</td>\n","      <td>Treatment</td>\n","      <td>[Cocaine Dependence, Selegiline]</td>\n","      <td>13</td>\n","      <td>[2131.81665]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NCT00000189</td>\n","      <td>University of Pennsylvania</td>\n","      <td>OTHER</td>\n","      <td>[Phase 2]</td>\n","      <td>January 1990</td>\n","      <td>[Cocaine-Related Disorders]</td>\n","      <td>[Substance-Related Disorders, Chemically-Induc...</td>\n","      <td>[BC25, BXM, All]</td>\n","      <td>[D000019970]</td>\n","      <td>[University of Pennsylvania]</td>\n","      <td>[OTHER]</td>\n","      <td>[National Institute on Drug Abuse (NIDA)]</td>\n","      <td>[NIH]</td>\n","      <td>[Please contact site for information.]</td>\n","      <td>41</td>\n","      <td>Actual</td>\n","      <td>No</td>\n","      <td>Male</td>\n","      <td>[Child, Adult, Older Adult]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>[University of Pennsylvania]</td>\n","      <td>[Philadelphia]</td>\n","      <td>[Pennsylvania]</td>\n","      <td>[19104 6178]</td>\n","      <td>[United States]</td>\n","      <td>[Drug]</td>\n","      <td>[Gepirone]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>None</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>Randomized</td>\n","      <td>Parallel Assignment</td>\n","      <td>Treatment</td>\n","      <td>[Cocaine Dependence, Gepirone]</td>\n","      <td>13</td>\n","      <td>[2131.81665]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         NCTId  ...                          LocationPopulationDensity\n","0  NCT00000143  ...  [2114.408936, 1189.4198, 2287.767578, 3559.750...\n","1  NCT00000170  ...                                      [1717.821167]\n","2  NCT00000177  ...  [230.015213, 1189.4198, 567.188477, 494.567383...\n","3  NCT00000188  ...                                       [2131.81665]\n","4  NCT00000189  ...                                       [2131.81665]\n","\n","[5 rows x 55 columns]"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"id":"9RrDlfOjlemh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1599214077683,"user_tz":-120,"elapsed":621,"user":{"displayName":"陳維儀","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjADfM2DPAarNhBjfMwzTdbc4NzhB0rPW1yeU7ETNM=s64","userId":"06691591176760802855"}},"outputId":"4994c96b-36d9-45ec-a6bf-49b296f8df6b"},"source":["df_raw[\"StartDate\"]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0              May 1997\n","1            April 1999\n","2          October 1995\n","3        September 1994\n","4          January 1990\n","              ...      \n","30968         July 2018\n","30969        April 2019\n","30970     November 2018\n","30971         July 2018\n","30972    September 2012\n","Name: StartDate, Length: 30973, dtype: object"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"markdown","metadata":{"id":"Iekpug0OilFv","colab_type":"text"},"source":["##Removing inconsistencies in clinical trials data"]},{"cell_type":"code","metadata":{"id":"9BYDEdRY5LAG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1599213123188,"user_tz":-120,"elapsed":2344,"user":{"displayName":"陳維儀","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjADfM2DPAarNhBjfMwzTdbc4NzhB0rPW1yeU7ETNM=s64","userId":"06691591176760802855"}},"outputId":"6d6b1831-25c6-4e92-9a89-5ef6b2542f6c"},"source":["indexis = set()\n","inc_counter, loc_counter, con_counter = 0, 0, 0\n","for index, row in df_raw.iterrows():\n","    if len(row.LocationFacility) != len(row.LocationCity) or len(row.LocationFacility) != len(row.LocationCountry):\n","            indexis.add(index)\n","            inc_counter += 1\n","\n","    for loc in row.LocationFacility:\n","        if loc.startswith(\"For additional information regarding investigative sites for this trial,\"):\n","            indexis.add(index)\n","            loc_counter += 1\n","    \n","    if row.EligibilityCriteria[0].startswith(\"Please contact site for information\"):\n","        indexis.add(index)\n","        con_counter += 1\n","\n","df_raw.drop(indexis, inplace=True)\n","print(f\"Location Facility/City/Country Inconsistency: {inc_counter}\")\n","print(f\"Location Facility Inconsistency: {loc_counter}\")\n","print(f\"Condition Inconsistency: {con_counter}\")\n","print(f\"Number of trials after removing inconsistencies: {len(df_raw)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Location Facility Inconsistency:  0\n","Condition Inconsistency:  18\n","Number of trials after removing inconsistencies: 982\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9NWbGgoxj6vB","colab_type":"text"},"source":["##Retrieve country data"]},{"cell_type":"code","metadata":{"id":"YVbpBN1QkNTk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":258},"executionInfo":{"status":"ok","timestamp":1599213123595,"user_tz":-120,"elapsed":1333,"user":{"displayName":"陳維儀","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjADfM2DPAarNhBjfMwzTdbc4NzhB0rPW1yeU7ETNM=s64","userId":"06691591176760802855"}},"outputId":"c3ae08c4-9b44-4d9a-fa17-dccb7e9f99e2"},"source":["country_projection = {\"_id\": 0, \n","            \"urbanPopulation\" : 1,\n","            \"countryName\": 1,\n","            \"population\": 1,\n","            \"density\": 1, \n","            \"sizeInKm2\": 1, \n","            \"lifeExpectancy\": 1, \n","            \"GDP\": 1, \n","            \"migrantsNet\": 1, \n","            \"worldshare\": 1,\n","            \"unemploymentRate\": 1,\n","            \"hospitalBed\": 1,\n","            \"healthExpenditure\": 1,\n","            \"fertilityRate\": 1,\n","            \"medianAge\": 1\n","}\n","\n","df_dbcountry = pd.DataFrame(list(db.country.find(filter={}, projection=country_projection)))\n","\n","print(f\"Number of countries: {len(df_dbcountry)}\")\n","df_dbcountry.head(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of countries: 235\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>countryName</th>\n","      <th>population</th>\n","      <th>lifeExpectancy</th>\n","      <th>GDP</th>\n","      <th>unemploymentRate</th>\n","      <th>hospitalBed</th>\n","      <th>healthExpenditure</th>\n","      <th>density</th>\n","      <th>fertilityRate</th>\n","      <th>medianAge</th>\n","      <th>migrantsNet</th>\n","      <th>sizeInKm2</th>\n","      <th>urbanPopulation</th>\n","      <th>worldshare</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Angola</td>\n","      <td>32866272.0</td>\n","      <td>60.0</td>\n","      <td>1.057510e+11</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>26.0</td>\n","      <td>5.6</td>\n","      <td>17.0</td>\n","      <td>6413.0</td>\n","      <td>1246700.0</td>\n","      <td>67.0</td>\n","      <td>0.42</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Bhutan</td>\n","      <td>771608.0</td>\n","      <td>71.0</td>\n","      <td>2.446674e+09</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>20.0</td>\n","      <td>2.0</td>\n","      <td>28.0</td>\n","      <td>320.0</td>\n","      <td>38117.0</td>\n","      <td>46.0</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Colombia</td>\n","      <td>50882891.0</td>\n","      <td>77.0</td>\n","      <td>3.310470e+11</td>\n","      <td>9.0</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>46.0</td>\n","      <td>1.8</td>\n","      <td>31.0</td>\n","      <td>204796.0</td>\n","      <td>1109500.0</td>\n","      <td>80.0</td>\n","      <td>0.65</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Cayman Islands</td>\n","      <td>65722.0</td>\n","      <td>82.0</td>\n","      <td>5.141834e+09</td>\n","      <td>NaN</td>\n","      <td>3.0</td>\n","      <td>NaN</td>\n","      <td>274.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>240.0</td>\n","      <td>97.0</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Spain</td>\n","      <td>46754778.0</td>\n","      <td>83.0</td>\n","      <td>1.419042e+12</td>\n","      <td>13.0</td>\n","      <td>3.0</td>\n","      <td>8.0</td>\n","      <td>94.0</td>\n","      <td>1.3</td>\n","      <td>1.3</td>\n","      <td>40000.0</td>\n","      <td>498800.0</td>\n","      <td>80.0</td>\n","      <td>0.60</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      countryName  population  ...  urbanPopulation  worldshare\n","0          Angola  32866272.0  ...             67.0        0.42\n","1          Bhutan    771608.0  ...             46.0        0.01\n","2        Colombia  50882891.0  ...             80.0        0.65\n","3  Cayman Islands     65722.0  ...             97.0        0.00\n","4           Spain  46754778.0  ...             80.0        0.60\n","\n","[5 rows x 14 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"6Nb2nIR187-N","colab_type":"text"},"source":["##Retrieve hospital data"]},{"cell_type":"code","metadata":{"id":"CKNfxbDMwb1I","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1599213125590,"user_tz":-120,"elapsed":2057,"user":{"displayName":"陳維儀","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjADfM2DPAarNhBjfMwzTdbc4NzhB0rPW1yeU7ETNM=s64","userId":"06691591176760802855"}},"outputId":"a5826da1-5651-4485-f0f3-fb6052d12b80"},"source":["hospitaldata = pd.DataFrame(list(db.hospitals.find(filter={})))\n","\n","#Change column Name\n","df_hospital = hospitaldata[['Name', 'World Rank']]\n","df_hospital.columns = [\"Name\", \"WorldRank\"]\n","\n","print(f\"Number of hospitals: {len(df_hospital)}\")\n","df_hospital.head(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of hospitals: 29259\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Name</th>\n","      <th>WorldRank</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Cleveland Clinic</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>St Jude Children's Research Hospital</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Johns Hopkins Medicine</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Mayo Clinic Scottsdale AZ</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>University of Maryland Medical Center</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                    Name WorldRank\n","0                       Cleveland Clinic         1\n","1   St Jude Children's Research Hospital         2\n","2                 Johns Hopkins Medicine         3\n","3              Mayo Clinic Scottsdale AZ         4\n","4  University of Maryland Medical Center         5"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"Zv4YM2Ozzsne","colab_type":"text"},"source":["##Retrieve regional data"]},{"cell_type":"code","metadata":{"id":"XuT1daq50IDB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1599213126865,"user_tz":-120,"elapsed":2266,"user":{"displayName":"陳維儀","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjADfM2DPAarNhBjfMwzTdbc4NzhB0rPW1yeU7ETNM=s64","userId":"06691591176760802855"}},"outputId":"b5c54d67-52d5-43dc-c3aa-55d72bcd039e"},"source":["df_regional = pd.DataFrame(list(db2.regional.find(filter={})))\n","\n","df_regional[\"Youth\"]= pd.to_numeric(df_regional[\"Youth\"])\n","df_regional[\"Working\"]= pd.to_numeric(df_regional[\"Working\"])\n","df_regional[\"Elderly\"]= pd.to_numeric(df_regional[\"Elderly\"])\n","df_regional = df_regional.drop_duplicates(subset=[\"Area\"])\n","\n","todrop = []\n","for i, row in df_regional.iterrows():\n","    if math.isnan(row[\"Youth\"])==True:\n","        todrop.append(i)\n","\n","df_regional = df_regional.drop(todrop)\n","\n","# is it possible to extract 'Area' locally where it is used?\n","countrylist = df_regional['Area']\n","\n","print(f\"Number of areas: {len(df_regional)}\")\n","df_regional.head(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of areas: 686\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>_id</th>\n","      <th>Area</th>\n","      <th>Density</th>\n","      <th>Youth</th>\n","      <th>Working</th>\n","      <th>Elderly</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5f2963b91268ea2905dbd1ae</td>\n","      <td>San Antonio</td>\n","      <td>115</td>\n","      <td>531242.0</td>\n","      <td>1675347.0</td>\n","      <td>331263.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5f2963b91268ea2905dbd1b3</td>\n","      <td>Palma de Mallorca</td>\n","      <td>340</td>\n","      <td>104704.0</td>\n","      <td>470334.0</td>\n","      <td>110748.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5f2963b91268ea2905dbd1b4</td>\n","      <td>Montreal</td>\n","      <td>372</td>\n","      <td>732421.0</td>\n","      <td>3021024.0</td>\n","      <td>761042.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5f2963b91268ea2905dbd1b5</td>\n","      <td>Portsmouth</td>\n","      <td>2763</td>\n","      <td>92068.0</td>\n","      <td>346692.0</td>\n","      <td>103283.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5f2963b91268ea2905dbd1ac</td>\n","      <td>London</td>\n","      <td>1785</td>\n","      <td>98405.0</td>\n","      <td>400749.0</td>\n","      <td>102920.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                        _id               Area  ...    Working   Elderly\n","0  5f2963b91268ea2905dbd1ae        San Antonio  ...  1675347.0  331263.0\n","1  5f2963b91268ea2905dbd1b3  Palma de Mallorca  ...   470334.0  110748.0\n","2  5f2963b91268ea2905dbd1b4           Montreal  ...  3021024.0  761042.0\n","3  5f2963b91268ea2905dbd1b5         Portsmouth  ...   346692.0  103283.0\n","4  5f2963b91268ea2905dbd1ac             London  ...   400749.0  102920.0\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"-NulDnKAYXvu","colab_type":"text"},"source":["##Retrieve worldrank data"]},{"cell_type":"code","metadata":{"id":"uGFZ3I9QYVlt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":252},"executionInfo":{"status":"ok","timestamp":1599213127563,"user_tz":-120,"elapsed":2578,"user":{"displayName":"陳維儀","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjADfM2DPAarNhBjfMwzTdbc4NzhB0rPW1yeU7ETNM=s64","userId":"06691591176760802855"}},"outputId":"f8d694ab-1d96-43d0-ebea-5a880abaf475"},"source":["df_worldrank = pd.DataFrame(list(db2.worldrank.find(filter={})))\n","worldrank_cols = [\"Facility\", \"WorldRank\"]\n","df_worldrank = df_worldrank[worldrank_cols]\n","df_worldrank.set_index(\"Facility\", inplace = True)\n","\n","print(f\"Number of facilities with worldrank: {len(df_worldrank)}\")\n","df_worldrank.head(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of facilities with worldrank: 4046\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>WorldRank</th>\n","    </tr>\n","    <tr>\n","      <th>Facility</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>massachusetts general hospital</th>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>university california san diego</th>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>medical college wisconsin</th>\n","      <td>536</td>\n","    </tr>\n","    <tr>\n","      <th>university kentucky</th>\n","      <td>132</td>\n","    </tr>\n","    <tr>\n","      <th>wake forest university</th>\n","      <td>349</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 WorldRank\n","Facility                                  \n","massachusetts general hospital           7\n","university california san diego         16\n","medical college wisconsin              536\n","university kentucky                    132\n","wake forest university                 349"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"rLcZ5y1OkfWG","colab_type":"text"},"source":["##Retrieve university data"]},{"cell_type":"code","metadata":{"id":"0Ao99Nx2wJr_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1599213128188,"user_tz":-120,"elapsed":2777,"user":{"displayName":"陳維儀","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjADfM2DPAarNhBjfMwzTdbc4NzhB0rPW1yeU7ETNM=s64","userId":"06691591176760802855"}},"outputId":"e7776489-5720-47d2-d5bb-140dc28be5b0"},"source":["df_uni = pd.DataFrame(list(db2.universities.find(filter={})))\n","df_uni = df_uni[[\"Name\", \"WorldRank\"]]\n","\n","print(f\"Number of universities: {len(df_uni)}\")\n","df_uni.head(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of universities: 12014\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Name</th>\n","      <th>WorldRank</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>University of California Berkeley</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>University of Washington</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Harvard University</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Columbia University New York</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(2) Johns Hopkins University</td>\n","      <td>8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                Name WorldRank\n","0  University of California Berkeley         4\n","1           University of Washington         5\n","2                 Harvard University         1\n","3       Columbia University New York         9\n","4       (2) Johns Hopkins University         8"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"M39rRIZDBGM_","colab_type":"code","colab":{}},"source":["#Create Facility List to search through\n","frames=[df_hospital, df_uni]\n","facility = pd.concat(frames, ignore_index=True)\n","\n","todrop=[]\n","for i, fac in facility.iterrows():\n","    if str(fac[\"WorldRank\"]).isdigit() == False:\n","        todrop.append(i)\n","        \n","facility = facility.drop(todrop)\n","facility[\"WorldRank\"]=pd.to_numeric(facility[\"WorldRank\"])\n","facilitylist= facility[\"Name\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W4CIv1VElD89","colab_type":"text"},"source":["---\n","#Custom Transformers"]},{"cell_type":"code","metadata":{"id":"jD6NXLVD6zKf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":224},"executionInfo":{"status":"ok","timestamp":1599213616043,"user_tz":-120,"elapsed":5920,"user":{"displayName":"陳維儀","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjADfM2DPAarNhBjfMwzTdbc4NzhB0rPW1yeU7ETNM=s64","userId":"06691591176760802855"}},"outputId":"108915fa-86f6-4edb-e6f4-0c8c034bf9eb"},"source":["from sklearn.preprocessing import FunctionTransformer, StandardScaler, RobustScaler\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.feature_extraction import DictVectorizer\n","from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n","from sklearn.pipeline import Pipeline\n","from functools import reduce\n","\n","import numpy as np\n","import string\n","import json\n","import re\n","\n","!pip install python-Levenshtein\n","import Levenshtein"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting python-Levenshtein\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n","\r\u001b[K     |██████▊                         | 10kB 23.0MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 20kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 30kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 40kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (49.6.0)\n","Building wheels for collected packages: python-Levenshtein\n","  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144801 sha256=d0bae39f44980404ac34b9722d99cfcb0d6096c9cb5f67ffc1b695e72c31468a\n","  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n","Successfully built python-Levenshtein\n","Installing collected packages: python-Levenshtein\n","Successfully installed python-Levenshtein-0.12.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4ZDMEzq4lkOF","colab_type":"text"},"source":["##General Transformers"]},{"cell_type":"code","metadata":{"id":"IGVvUgeRljj9","colab_type":"code","colab":{}},"source":["class FeatureSelector(TransformerMixin, BaseEstimator):\n","    \"\"\"\n","    Returns a dataframe with selected features.\n","    \n","    Parameters\n","    --------\n","        cols\n","            columns to be selected.\n","\n","    Attributes\n","    --------\n","\n","    Notes\n","    --------\n","    \"\"\"\n","\n","    def __init__(self, cols):\n","        self.cols = cols\n","\n","    def fit(self, X, y=None):\n","        \"\"\" Do nothing and return the estimator unchanged\n","        \n","        Parameters\n","        ----------\n","        X : pandas dataframe, shape [n_samples, n_features]\n","\n","        y : Ignored\n","        \"\"\"\n","        return self\n","\n","    def transform(self, X):\n","        \"\"\" Selects defined features\n","        \n","        Parameters\n","        ----------\n","        X : pandas dataframe, shape [n_samples, n_features]\n","        \n","        Returns\n","        -------\n","        X_new : pandas dataframe with selected features\n","        \"\"\"\n","        X_new = X[self.cols] # or: X_new = X.loc[:, self.cols].copy()\n","        return X_new\n","\n","class FeatureExcluder(TransformerMixin, BaseEstimator):\n","    \"\"\"\n","    Excludes the defined features.\n","    \n","    Parameters\n","    --------\n","        cols\n","            columns to be excluded.\n","\n","    Attributes\n","    --------\n","        raw_features\n","            columns to be excluded.\n","\n","    Notes\n","    --------\n","    \"\"\"\n","\n","    def __init__(self, cols):\n","        self.raw_features = list(cols)\n","\n","    def fit(self, X, y=None):\n","        \"\"\" Do nothing and return the estimator unchanged.\n","        \n","        Parameters\n","        ----------\n","        X : pandas dataframe, shape [n_samples, n_features]\n","        \n","        y : Ignored\n","        \"\"\"\n","        return self\n","\n","    def transform(self, X):\n","        \"\"\" Excludes defined features.\n","        \n","        Parameters\n","        ----------\n","        X : pandas dataframe, shape [n_samples, n_features]\n","        \n","        Returns\n","        -------\n","        X_new : pandas dataframe without raw features\n","        \"\"\"\n","        tmp = [set(self.raw_features), set(list(X.columns))]\n","        schnitt = set.intersection(*tmp)\n","        X_new = X.drop(list(schnitt), axis=1)\n","\n","        #for attr in X.columns:\n","        #   if \"_x\" in attr or \"_y\" in attr:\n","        #       X_new.drop(attr, axis=1, inplace=True)\n","\n","        return X_new\n","\n","class FeatureUnion(TransformerMixin, BaseEstimator):\n","    \"\"\"\n","    Applies fit and transform on multiple transformers and merges their outputs.\n","    \n","    Parameters\n","    --------\n","        transformer_list\n","            list of transformers\n","\n","    Attributes\n","    --------\n","\n","    Notes\n","    --------\n","    \"\"\"\n","    \n","    def __init__(self, transformer_list):\n","        self.transformer_list = transformer_list\n","\n","    def fit(self, X, y=None):\n","        \"\"\" Fit all transformers.\n","        \n","        Parameters\n","        ----------\n","        X : pandas dataframe, shape [n_samples, n_features]\n","        y : Ignored\n","        \"\"\"\n","        for (name, transformer) in self.transformer_list:\n","            transformer.fit(X, y)\n","        return self\n","      \n","    def transform(self, X):\n","        \"\"\" Run transform on all transformers and merge dataframes into a single one.\n","\n","        Parameters\n","        ----------\n","        X : pandas dataframe, shape [n_samples, n_features]\n","        \n","        Returns\n","        -------\n","        X_new : merged pandas dataframe\n","        \"\"\"\n","        Xts = [transformer.transform(X) for _, transformer in self.transformer_list]\n","        X_new = reduce(lambda X1, X2: pd.merge(X1, X2, left_index=True, right_index=True), Xts)\n","        return X_new\n","\n","class Debug(TransformerMixin, BaseEstimator):\n","    \"\"\"\n","    Can be used as an intermediate step to check the status of the dataframe in the pipeline.\n","    \n","    Parameters\n","    --------\n","        debug\n","            String describing the debug step in pipeline.\n","\n","    Attributes\n","    --------\n","\n","    Notes\n","    --------\n","    - this transformer is used by developers to understand what is happening inside the pipeline\n","    \"\"\"\n","\n","    def __init__(self, debug):\n","        #print(f\"{debug} initiated\")\n","        self.name = debug\n","\n","    def fit(self, X, y=None):\n","        #print(f\"{self.name} - Fit executed\")\n","        #print(\"Dataframe Type: \", type(X))\n","        return self\n","\n","    def transform(self, X):\n","        print(f\"{self.name} - Transform executed\")\n","        #print(\"Dataframe Type: \", type(X))\n","        X.to_csv(f\"dataframe_{self.name}_step.csv\", sep=\";\")\n","        print(list(X.columns))\n","        #display(X.head(5))\n","        return X.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dpzwVOgAPDYQ","colab_type":"text"},"source":["##Transformers for single categorical values"]},{"cell_type":"code","metadata":{"id":"0vNA3DNJBjxr","colab_type":"code","colab":{}},"source":["class MissingStringsTransformer(TransformerMixin, BaseEstimator):\n","    \"\"\"\n","    Replaces missing values using SimpleImputer and returns a pandas dataframe.\n","\n","    Parameters\n","    --------       \n","        strategy : string, default = 'most_frequent'\n","            String value defining the strategy of the simple imputer. Is only used for the simple imputer\n","            One can decide between: \n","            - constant -> filling in a constant value\n","            - most_frequent -> calculate the most frequent value and inserting that value\n","        \n","        fill_value : string, default = ''\n","            Value defining the constant value that will be inserted when the Simple imputation strategy \"constant\" is selected\n","\n","    Attributes\n","    --------\n","        imp \n","          Placeholder to contain the created Imputer instance of the fit method.\n","\n","    Notes\n","    --------\n","    https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n","    \n","    \"\"\"\n","    \n","    def __init__(self, \n","                 strategy = 'most_frequent', \n","                 fill_value= \"\",\n","                 n_neighbors = 2,\n","                 weights = \"uniform\"):\n","        self.strategy = strategy\n","        self.fill_value = fill_value    # in case of strategy 'constant'\n","        self.imp = None\n","\n","    def fit(self, X, y=None):\n","        if self.strategy == 'constant':\n","            self.imp = SimpleImputer(strategy=self.strategy, fill_value=self.fill_value, missing_values=None)\n","        elif self.strategy == 'most_frequent':\n","            self.imp = SimpleImputer(strategy=self.strategy, fill_value='Missing', missing_values=None)\n","        else:\n","            raise Exception(\"Please define one strategy - 'most_frequent' or 'constant'\")\n","        self.imp.fit(X)\n","        return self\n","\n","    def transform(self, X):\n","        X_imp = self.imp.transform(X)\n","        X_new = pd.DataFrame(X_imp, index=X.index, columns=X.columns)\n","        return X_new\n","\n","class SingleOneHotEncoder(TransformerMixin, BaseEstimator):\n","    \"\"\"\n","    Applies One Hot Encoding on features with only single values (no lists).\n","\n","    Parameters\n","    --------\n","\n","    Attributes\n","    --------\n","        dv \n","          Placeholder to contain the created DictVectorizer instance of the fit method.\n","        \n","        nan_cols\n","          Placeholder to contain empty columns to be excluded. Is just set once to make sure that always the same columns are removed.\n","\n","    Notes\n","    --------\n","    https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\n","    \n","    \"\"\"\n","\n","    def __init__(self):\n","        self.dv = None\n","        self.nan_cols = None\n","\n","    def fit(self, X, y=None):\n","        X_dict = X.to_dict('records')\n","        self.dv = DictVectorizer(sparse=False)\n","        self.dv.fit(X_dict)\n","        return self\n","\n","    def transform(self, X):\n","        X_dict = X.to_dict('records')\n","        Xt = self.dv.transform(X_dict)\n","        cols = self.dv.get_feature_names()\n","        X_new = pd.DataFrame(Xt, index=X.index, columns=cols)\n","        \n","        if not self.nan_cols:\n","            self.nan_cols = [c for c in cols if '=' not in c]\n","        \n","        X_new = X_new.drop(self.nan_cols, axis=1)\n","        return X_new"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V8vjlq4NPXYU","colab_type":"text"},"source":["## Transformers for list of categorical values"]},{"cell_type":"code","metadata":{"id":"F3X74AflqbhC","colab_type":"code","colab":{}},"source":["class MultipleOneHotEncoder(TransformerMixin, BaseEstimator):\n","    \"\"\" Applies One Hot Encoding on features with lists as values.\n","\n","    Parameters\n","    --------\n","\n","    Attributes\n","    --------\n","        features \n","          Placeholder to contain a dictionary consisting of feature name and possible values of each feature.\n","\n","    Notes\n","    --------\n","    \n","    \"\"\"\n","\n","    def __init__(self):\n","        self.features = {}\n","\n","    def fit(self, X, y=None):\n","        for feature in list(X.columns):\n","            distinct_values = set()\n","            for index, row in X.iterrows():\n","                for value in row[feature]: \n","                    if value not in distinct_values: distinct_values.add(value)\n","            self.features[feature] = list(distinct_values)\n","        return self\n","\n","    def transform(self, X):\n","        X_new = X.copy()\n","        for index, row in X.iterrows():\n","            for feature in self.features.keys():\n","                for value in self.features[feature]:\n","                    X_new[f\"{feature}={value}\"] = 0\n","                    if value in row[feature]: X_new.at[index, f\"{feature}={value}\"] = 1 \n","\n","        return X_new\n","\n","class MultipleTopOneHotEncoder(TransformerMixin, BaseEstimator):\n","    \"\"\" Applies One Hot Encoding on features with lists as values taking only the top X Values.\n","\n","    Parameters\n","    --------\n","        strategy : string, default = 'top'\n","            String value defining the strategy. One can decide between: \n","            - top : one hot encodes top X values of all possible values according to the occurrence in trials\n","            - min_value : one hot encodes all values, with a minimum of X occurences in trials\n","        \n","        top : integer, default = 20\n","            Number of top values to take for strategy 'top'\n","\n","        min_value : integer, default = None\n","            Number of minimum occurences in order to be considered top, used for stratefy 'min_value'\n","\n","    Attributes\n","    --------\n","        mlb \n","          Placeholder to contain the created MultipleOneHotEncoder instance of the fit method.\n","\n","    Notes\n","    --------\n","    \n","    \"\"\"\n","\n","    def __init__(self, strategie=\"top\", top=20, min_value=None):\n","        self.mlb = None\n","        self.strategie = strategie\n","        self.top = top\n","        self.min_value = min_value\n","\n","    def fit(self, X, y=None):\n","        values = {}\n","        for feature in list(X.columns):\n","            stats = {}\n","            for value_list in X[feature]:\n","                for value in set(value_list):\n","                    if value in stats: stats[value] += 1\n","                    else: stats[value] = 1\n","            if self.strategie == \"min_value\" and self.min_value is not None:\n","                values[feature] = {k: v for k, v in sorted(stats.items(),reverse=True, key=lambda item: item[1]) if v > self.min_value}\n","            elif self.strategie == \"top\" and self.top is not None:\n","                values[feature] = {k: v for k, v in sorted(stats.items(),reverse=True, key=lambda item: item[1])[:self.top]}\n","            else:\n","                raise Exception(\"Please define one strategy - 'top' or 'min_value'\") \n","        self.values = values\n","        #print(f\"Ranking: \")\n","        #print(json.dumps(values, indent=2))\n","\n","        X_tmp = X.copy()\n","        self.mlb = MultipleOneHotEncoder()\n","        new_columns = []\n","        for feature in self.values.keys():\n","            new_columns.append(f\"{feature}_top\")\n","            X_tmp[f\"{feature}_top\"] = [[x for x in set(value) if x in list(self.values[feature].keys())] for value in X[feature]]\n","        self.mlb.fit(X_tmp[new_columns])\n","        return self\n","\n","    def transform(self, X):\n","        X_tmp = X.copy()\n","        new_columns = []\n","        for feature in self.values.keys():\n","            new_columns.append(f\"{feature}_top\")\n","            X_tmp[f\"{feature}_top\"] = [[x for x in set(value) if x in list(self.values[feature].keys())] for value in X[feature]]\n","        \n","        X_new_partial = self.mlb.transform(X_tmp[new_columns])\n","        X_new_partial.drop(new_columns, axis=1, inplace=True)\n","        X_new = X.merge(X_new_partial, left_index=True, right_index=True)\n","        return X_new"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_f2lKomKP3D9","colab_type":"text"},"source":["##Transformers for numerical values"]},{"cell_type":"code","metadata":{"id":"gZ1e4Ij3P6WF","colab_type":"code","colab":{}},"source":["class MissingValuesTransformer(TransformerMixin, BaseEstimator):\n","    \"\"\"\n","    Replaces missing values using SimpleImputer, KNNImputer or IterativeImputer and returns a pandas dataframe.\n","\n","    Parameters\n","    --------\n","\n","        imputer : string, default = 'SimpleImputer'\n","            String value defining the impution strategy that should be applied.\n","            One can decide between:\n","            - SimpleImputer = impute values in the i-th feature dimension using only non-missing values \n","                              in that feature dimension\n","            - KNNImputer = missing values are imputed using the mean value from n_neighbors nearest neighbors\n","            - IterativeImputer = multivariate imputation algorithm using entire set of available feature \n","                                 dimensions to estimate the missing values\n","            \n","        strategy : string, default = 'mean'\n","            String value defining the strategy of the simple imputer. Is only used for the simple imputer\n","            One can decide between: \n","            - mean -> calculate the mean value and inserting that value\n","            - median -> calculate the mean value and inserting that value\n","        \n","        fill_value : string, default = ''\n","            Value defining the constant value that will be inserted when the Simple imputation strategy \"constant\" is selected\n","\n","        n_neighbors : integer, default = 2\n","            Integer value of the number of neighbors that is taken into account to determine the most frequent or average value of the k nearest neighbors.\n","\n","        weights : string, default = \"uniform\"\n","            Parameter that defines how the values of the k nearest neighbors are weighted. \n","            One can choose between:\n","            - uniform -> all neighbors are equally important\n","            - distance -> closer neighbors are higher weighted than distanced ones\n","        \n","        max_iter : int\n","            Maximum number of imputation rounds to perform before returning the imputations.\n","            \n","        initial_strategy : str, default=’mean’\n","            Which strategy to use to initialize the missing values in iterative imputer. \n","            One can decide between: \n","            - mean -> calculate the mean value and inserting that value\n","            - median -> calculate the mean value and inserting that value\n","\n","    Attributes\n","    --------\n","        imp \n","          Placeholder to contain the created Imputer instance of the fit method.\n","\n","    Notes\n","    --------\n","\n","    https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n","    https://levelup.gitconnected.com/scikit-learn-python-6-useful-tricks-for-data-scientists-1a0a502a6aa3\n","    \n","    \"\"\"\n","    \n","    def __init__(self, \n","                 imputer = 'SimpleImputer',\n","                 strategy = 'mean', \n","                 fill_value= \"\",\n","                 n_neighbors = 2,\n","                 weights = \"uniform\",\n","                 max_iter = 10,\n","                 initial_strategy = 'mean'):\n","        self.imputer = imputer\n","        self.strategy = strategy\n","        self.fill_value = fill_value    # in case of strategy 'constant'\n","        self.n_neighbors = n_neighbors\n","        self.weights = weights\n","        self.max_iter = max_iter\n","        self.initial_strategy = initial_strategy\n","        self.imp = None\n","\n","    def fit(self, X, y=None):\n","        if self.imputer == 'SimpleImputer':\n","            if self.strategy == 'mean':\n","                self.imp = SimpleImputer(strategy=self.strategy)\n","            elif self.strategy == 'median':\n","                self.imp = SimpleImputer(strategy=self.strategy)\n","            else:\n","                raise Exception(\"Please define one strategy - 'mean' or 'median'\")\n","        elif self.imputer == 'KNNImputer':\n","            if self.weights not in ['uniform', 'distance']:\n","                raise Exception(\"Please provide a valid weighting: 'uniform' or 'distance'\")\n","            if not isinstance(self.n_neighbors, (int, float, complex)):\n","                raise Exception(\"Please provide a valid number for n_neighbors\")\n","            self.imp = KNNImputer(n_neighbors=self.n_neighbors, weights=self.weights) \n","        elif self.imputer == 'IterativeImputer':\n","            if self.initial_strategy not in ['mean', 'median']:\n","                raise Exception(\"Please define one strategy - 'mean' or 'median'\")\n","            if not isinstance(self.max_iter, (int, float, complex)):\n","                raise Exception(\"Please provide a valid number for max_iter\")\n","            self.imp = IterativeImputer(max_iter=self.max_iter, random_state=0, initial_strategy=self.initial_strategy)\n","        else: \n","            raise Exception(\"Please define one imputer - 'SimpleImputer', 'KNNImputer' or 'IterativeImputer'\")\n","        self.imp.fit(X)\n","        return self\n","\n","    def transform(self, X):\n","        X_imp = self.imp.transform(X)\n","        new_columns = [f\"{col}_new\" for col in X.columns]\n","        X_new = pd.DataFrame(X_imp, index=X.index, columns=new_columns)\n","        return X_new"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cf_Jfv-8mwt5","colab_type":"text"},"source":["## Transformers for age values"]},{"cell_type":"code","metadata":{"id":"ktSrTI8rm6c8","colab_type":"code","colab":{}},"source":["class ToYearTransformer(TransformerMixin, BaseEstimator):\n","    \"\"\" Replaces different time frame with a number, save it as factors then multiply them,\n","      returns a pandas dataframe \"\"\"\n","    def __init__(self, conversion_factors= {\n","                                'Years': 1,\n","                                'Year': 1,\n","                                'Months': 1/12,\n","                                'Month': 1/12,\n","                                'Weeks': 1/52,\n","                                'Week': 1/52,\n","                                'Days': 1/365 ,\n","                                'Day': 1/365,\n","                                'Hours': 1/8760,\n","                                'Hour': 1/8760\n","                                }):\n","\n","        self._conversion_factors = conversion_factors\n","    def fit(self,X,y = None):\n","        return self\n","    \n","    def transform(self, X):\n","\n","        for feature in X.columns:\n","            X_new = X.copy()\n","            X_new[feature] = X_new[feature].astype(str)\n","            for i in range(len(X_new)):\n","                for string, factor in self._conversion_factors.items(): \n","                    for n in range(len(X.columns)):\n","                        if string in (X_new.iloc[i][n]):              \n","                            X_new.iloc[i][n] = (X_new.iloc[i][n]).replace(string, '')\n","                            X_new.iloc[i][n] = (X_new.iloc[i][n]).strip()\n","                            X_new.iloc[i][n] = float(X_new.iloc[i][n]) * factor\n","                            X_new.iloc[i][n] = str(X_new.iloc[i][n])\n","\n","\n","            X_new[\"MaximumAgeValue\"] = X_new['MaximumAge'].astype(float)\n","            X_new[\"MinimumAgeValue\"] = X_new['MinimumAge'].astype(float)\n","\n","        return X_new"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xnn4K6UAtDG2","colab_type":"text"},"source":["## Transformer for StartMonth"]},{"cell_type":"code","metadata":{"id":"tLZY_TzetHV1","colab_type":"code","colab":{}},"source":["class StartMonthTransformer(TransformerMixin, BaseEstimator):\n","    \"\"\"\n","    Class that extracts the month from the start date and creates a new column \"StartMonth\" in the data set containing\n","    the extracted month as float value.\n","    \n","    \"\"\"\n","    def __init__(self):\n","        pass\n","    \n","    def fit(self,X,y = None):\n","        return self\n","    \n","    def transform(self, X):\n","        # Copy the passed data frame\n","        X_new = X.copy()\n","        # Create a new column \"StartMonth\" and remove the year from the StartDate feature\n","        X_new['StartMonth'] = X_new['StartDate'].str.replace('0|1|2|3|4|5|6|7|8|9| |  ','')\n","        \n","        # Transform each month to a float value\n","        X_new['StartMonth'] = X_new['StartMonth'].str.replace('January','1')\n","        X_new['StartMonth'] = X_new['StartMonth'].str.replace('February','2')\n","        X_new['StartMonth'] = X_new['StartMonth'].str.replace('March','3')\n","        X_new['StartMonth'] = X_new['StartMonth'].str.replace('April','4')\n","        X_new['StartMonth'] = X_new['StartMonth'].str.replace('May','5')\n","        X_new['StartMonth'] = X_new['StartMonth'].str.replace('June','6')\n","        X_new['StartMonth'] = X_new['StartMonth'].str.replace('July','7')\n","        X_new['StartMonth'] = X_new['StartMonth'].str.replace('August','8')\n","        X_new['StartMonth'] = X_new['StartMonth'].str.replace('September','9')\n","        X_new['StartMonth'] = X_new['StartMonth'].str.replace('October','10')\n","        X_new['StartMonth'] = X_new['StartMonth'].str.replace('November','11')\n","        X_new['StartMonth'] = X_new['StartMonth'].str.replace('December','12')\n","        X_new['StartMonth'] = X_new['StartMonth'].astype(float)\n","        return X_new"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nTuERiJQyeyI","colab_type":"text"},"source":["## Transformer for StartYear "]},{"cell_type":"code","metadata":{"id":"ayX2p6wdsYK-","colab_type":"code","colab":{}},"source":["class StartYearTransformer(TransformerMixin, BaseEstimator):\n","    \"\"\"\n","    Class that extracts the year from the start date and creates a new column \"StartYear\" in the data set containing\n","    the extracted startyear.\n","    \n","    \"\"\"\n","    def __init__(self):\n","          pass\n","    \n","    def fit(self,X,y = None):\n","        return self\n","    \n","    def transform(self, X):\n","        # Copy the passed data frame\n","        X_new = X.copy()\n","        \n","        # Remove the month from the StartDate feature\n","        X_new['StartYear'] = X_new['StartDate'].str.replace('January|February|March|April|May|June|July|August|September|October|November|December', '')\n","        \n","        # Save the year as float value\n","        X_new['StartYear'] = (X_new['StartYear']).astype(float)\n","        X_new = X_new.drop(columns=['StartDate'])\n","        return X_new"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UJrVe6QEAVAg","colab_type":"text"},"source":["##Transformer to remove outliers in Start and End Date"]},{"cell_type":"code","metadata":{"id":"ZloHAuSMnOw2","colab_type":"code","colab":{}},"source":["#Custom transformer that transforms data set to only contain data after the timeperiod defined\n","class TimeOutlierRemover( BaseEstimator, TransformerMixin ):\n","    \"\"\"\n","    Class that removes records from the data set outside a predefined time range.\n","\n","    Parameters\n","    --------\n","\n","        startYear : integer, default = 1995\n","            Integer defining the first year the data set should contain study records of. \n","            \n","        endYear : integer, default = 2018\n","            Integer defining the last year the data set should contain study records of. \n","            \n","    \"\"\"\n","    \n","    #Class constructor method that takes in a list of values as its argument\n","    def __init__(self, startYear = 1995, endYear = 2018):\n","        self._startYear = startYear\n","        self._endYear = endYear\n","\n","    def fit( self, X, y = None ):\n","        return self\n","    \n","    def transform(self, X , y = None ):\n","        # Copy the passed data frame\n","        X_new = X.copy()\n","        # Exclude all data that is not in the range of start and end year\n","        X_new = X_new[(X_new['StartYear'] >= self._startYear) & (X_new['StartYear'] <= self._endYear)]\n","        # Reset the index of the remaining data set\n","        X_new = X_new.reset_index(drop=True)\n","        return X_new"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JgCmjN33A00Z","colab_type":"text"},"source":["##Transformer to remove outliers in Enrollment Count"]},{"cell_type":"code","metadata":{"id":"CTnKg2EdAzBg","colab_type":"code","colab":{}},"source":["#Custom transformer that transforms data set to remove outliers in the EnrollmentCount\n","# !!! Must be called after OHE\n","class EnrollmentOutlierRemover( BaseEstimator, TransformerMixin ):\n","    \"\"\"\n","    Class for performing outlier removal in the feature \"EnrollmentCount\" based on basic statistics of the MAD or IQR.\n","    To take into account the difference in the enrollment count of studies in different phases, the MAD and IQR is \n","    calculated for each possible phase individually.\n","\n","    Parameters\n","    --------\n","\n","        strategy : string, default = \"IQR\"\n","            String value defining the strategy of how the range of \"normal\" values is determined. \n","            One can choose between two strategies:\n","                - \"IQR\" = Interquartile range\n","                    Here, after the median value of each phase was calculated, the interquartile range between the 25% and\n","                    the 75% quartile is calculated. The \"normal\" range of values is determined by taking the \n","                    median +/- 1.5 * IQR.\n","                - \"MAD\" = Median Absolute Deviation\n","                    Here, after the median value of each phase was calculated, the absolute deviation of all values to the\n","                    median is calculated and the median of those values forms the MAD. The \"normal\" range of values is \n","                    determined by taking the median +/- 2 * MAD.\n","        \n","\n","    Attributes\n","    --------\n","\n","    range : dict\n","        Dictionary storing the precalculated \"normal\" value ranges for each of the phases.\n","\n","    phases : list\n","        List of strings defining the keys for the different phases in order to create a unique key in the dictionary.\n","        \n","    phase_series : dict\n","        Dictionary storing the filtered series of EnrollmentCount values for the different phases.\n","        \n","        \"\"\"\n","    \n","    #Class constructor method that takes in a list of values as its argument\n","    def __init__(self, strategy = \"IQR\"):\n","        self._strategy = strategy\n","        self._range = {}\n","        self._phases = ['phase1', 'phase1_2', 'phase2', 'phase2_3', 'phase3']\n","        self._phase_series = {}\n","\n","    def fit(self, X, y = None ):\n","        # Get data of the different phases\n","        self._phase_series['phase1'] = X.loc[(X['Phase=Phase 1'] == 1) & (X[\"Phase=Phase 2\"] == 0)]['EnrollmentCount_new']\n","        self._phase_series['phase1_2'] = X.loc[(X['Phase=Phase 1'] == 1) & (X[\"Phase=Phase 2\"] == 1)]['EnrollmentCount_new']\n","        self._phase_series['phase2'] = X.loc[(X['Phase=Phase 1'] == 0) & (X[\"Phase=Phase 2\"] == 1) & (X[\"Phase=Phase 3\"] == 0)]['EnrollmentCount_new']\n","        self._phase_series['phase2_3'] = X.loc[ (X[\"Phase=Phase 2\"] == 1) & (X[\"Phase=Phase 3\"] == 1)]['EnrollmentCount_new']\n","        self._phase_series['phase3'] = X.loc[ (X[\"Phase=Phase 2\"] == 0) & (X[\"Phase=Phase 3\"] == 1)]['EnrollmentCount_new']\n","        \n","        # Calculcate outlier ranges\n","        if self._strategy == \"IQR\":\n","            for i in self._phases:\n","                IQR = self._phase_series[i].quantile(0.75) - self._phase_series[i].quantile(0.25)\n","                self._range[i] = [self._phase_series[i].median() - (1.5 * IQR), self._phase_series[i].median() + (1.5 * IQR)]\n","                \n","                \n","        elif self._strategy == \"MAD\":\n","            for i in self._phases:\n","                MAD = self._phase_series[i].mad()\n","                self._range[i] = [self._phase_series[i].median() - (2 * MAD), self._phase_series[i].median() + (2 * MAD)]\n","        \n","        else:\n","            raise ValueError(\"\"\"Strategy must be either 'IQR' or 'MAD' \"\"\")\n","            \n","        \n","        return self\n","\n","    \n","    def transform(self, X , y = None ):\n","        X_new = X.copy()\n","        # Remove outliers that don't lie in the specified value range of \"normal\" values\n","        for index, row in X_new.iterrows():\n","            if row['Phase=Phase 1'] == 1 and row['Phase=Phase 2'] == 0:\n","                if row['EnrollmentCount_new'] < self._range['phase1'][0] or row['EnrollmentCount_new'] > self._range['phase1'][1]:\n","                    X_new.drop([index], inplace = True)\n","            if row['Phase=Phase 1'] == 1 and row['Phase=Phase 2'] == 1:\n","                if row['EnrollmentCount_new'] < self._range['phase1_2'][0] or row['EnrollmentCount_new'] > self._range['phase1_2'][1]:\n","                    X_new.drop([index], inplace = True)\n","            if row['Phase=Phase 1'] == 0 and row['Phase=Phase 2'] == 1 and row['Phase=Phase 3'] == 0:\n","                if row['EnrollmentCount_new'] < self._range['phase2'][0] or row['EnrollmentCount_new'] > self._range['phase2'][1]:\n","                    X_new.drop([index], inplace = True)\n","            if row['Phase=Phase 2'] == 1 and row['Phase=Phase 3'] == 1:\n","                if row['EnrollmentCount_new'] < self._range['phase2_3'][0] or row['EnrollmentCount_new'] > self._range['phase2_3'][1]:\n","                    X_new.drop([index], inplace = True)\n","            if row['Phase=Phase 2'] == 0 and row['Phase=Phase 3'] == 1:\n","                if row['EnrollmentCount_new'] < self._range['phase3'][0] or row['EnrollmentCount_new'] > self._range['phase3'][1]:\n","                    X_new.drop([index], inplace = True)\n","        return X_new"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nDetYrYTXyXG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"error","timestamp":1599213147410,"user_tz":-120,"elapsed":321,"user":{"displayName":"陳維儀","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjADfM2DPAarNhBjfMwzTdbc4NzhB0rPW1yeU7ETNM=s64","userId":"06691591176760802855"}},"outputId":"db4587fe-a774-483d-b2d2-7e6bd784eba6"},"source":["df_test = df_raw[['Phase']]\n","instance = MultipleOneHotEncoder()\n","new = instance.fit_transform(df_test)\n","display(new)\n","new.drop(columns = ['Phase'], inplace = True)\n","df_test = df_raw.copy()\n","df_test = df_test.join(new)\n","df_test.drop(columns = ['Phase'], inplace = True)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-182d95a7626b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Phase'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultipleOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Phase'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'MultipleOneHotEncoder' is not defined"]}]},{"cell_type":"code","metadata":{"id":"YJ7RfGs9BA-w","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1599207314924,"user_tz":-120,"elapsed":34399,"user":{"displayName":"Stefan Sousa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnGBw3kCtWt-XAq5nTYjp1DY0_T5bIfSzFTajnfg=s64","userId":"08251656603606889983"}},"outputId":"b05a0e12-b734-49ee-9f3f-8c28f17014e7"},"source":["display(df_test)\n","display(df_test.dtypes)\n","\n","instance = EnrollmentOutlierRemover(strategy = 'IQR')\n","new = instance.fit_transform(df_test)\n","display(new)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>NCTId</th>\n","      <th>OrgFullName</th>\n","      <th>OrgClass</th>\n","      <th>StartDate</th>\n","      <th>Condition</th>\n","      <th>ConditionAncestorTerm</th>\n","      <th>ConditionBrowseBranchAbbrev</th>\n","      <th>ConditionMeshId</th>\n","      <th>LeadSponsorName</th>\n","      <th>LeadSponsorClass</th>\n","      <th>CollaboratorName</th>\n","      <th>CollaboratorClass</th>\n","      <th>EligibilityCriteria</th>\n","      <th>EnrollmentCount</th>\n","      <th>EnrollmentType</th>\n","      <th>HealthyVolunteers</th>\n","      <th>Gender</th>\n","      <th>StdAge</th>\n","      <th>MinimumAge</th>\n","      <th>MaximumAge</th>\n","      <th>LocationFacility</th>\n","      <th>LocationCity</th>\n","      <th>LocationState</th>\n","      <th>LocationZip</th>\n","      <th>LocationCountry</th>\n","      <th>InterventionType</th>\n","      <th>InterventionName</th>\n","      <th>IsFDARegulatedDevice</th>\n","      <th>IsFDARegulatedDrug</th>\n","      <th>EventsTimeFrame</th>\n","      <th>FlowDropWithdrawType</th>\n","      <th>FlowGroupDescription</th>\n","      <th>FlowGroupTitle</th>\n","      <th>FlowMilestoneType</th>\n","      <th>FlowPeriodTitle</th>\n","      <th>FlowRecruitmentDetails</th>\n","      <th>ArmGroupDescription</th>\n","      <th>ArmGroupInterventionName</th>\n","      <th>ArmGroupLabel</th>\n","      <th>ArmGroupType</th>\n","      <th>BaselineCategoryTitle</th>\n","      <th>BaselineClassTitle</th>\n","      <th>BaselineDenomCountGroupId</th>\n","      <th>BaselineGroupDescription</th>\n","      <th>BaselineGroupTitle</th>\n","      <th>BaselineMeasureDispersionType</th>\n","      <th>BaselineMeasureTitle</th>\n","      <th>BaselineMeasureUnitOfMeasure</th>\n","      <th>DesignAllocation</th>\n","      <th>DesignInterventionModel</th>\n","      <th>DesignPrimaryPurpose</th>\n","      <th>Keyword</th>\n","      <th>EnrollmentDuration</th>\n","      <th>LocationPopulationDensity</th>\n","      <th>Phase=Phase 3</th>\n","      <th>Phase=Phase 2</th>\n","      <th>Phase=Phase 1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>NCT00000143</td>\n","      <td>Johns Hopkins Bloomberg School of Public Health</td>\n","      <td>OTHER</td>\n","      <td>May 1997</td>\n","      <td>[Cytomegalovirus Retinitis, HIV Infections]</td>\n","      <td>[Virus Diseases, Retinal Diseases, Eye Disease...</td>\n","      <td>[BC01, All, BC02, BC20, BC11, Rare]</td>\n","      <td>[D000017726, D000012173]</td>\n","      <td>[Johns Hopkins Bloomberg School of Public Health]</td>\n","      <td>[OTHER]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[Inclusion criteria:\\n\\nAge 13 years or older\\...</td>\n","      <td>61</td>\n","      <td>Actual</td>\n","      <td>No</td>\n","      <td>All</td>\n","      <td>[Child, Adult, Older Adult]</td>\n","      <td>13 Years</td>\n","      <td>None</td>\n","      <td>[Department of Ophthalmology, University of Ca...</td>\n","      <td>[Irvine, La Jolla, Los Angeles, Los Angeles, S...</td>\n","      <td>[California, California, California, Californi...</td>\n","      <td>[92697-4375, 92093-0946, 90033, 90095-7003, 94...</td>\n","      <td>[United States, United States, United States, ...</td>\n","      <td>[Device, Drug]</td>\n","      <td>[Ganciclovir implant and oral ganciclovir, Cid...</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>3 years</td>\n","      <td>[]</td>\n","      <td>[Ganciclovir device and oral dose of Ganciclov...</td>\n","      <td>[Ganciclovir Implant and Oral Ganciclovir, Cid...</td>\n","      <td>[STARTED, COMPLETED, NOT COMPLETED]</td>\n","      <td>[Overall Study]</td>\n","      <td>June 1997</td>\n","      <td>[Ganciclovir device and oral dose of Ganciclov...</td>\n","      <td>[Device: Ganciclovir implant and oral ganciclo...</td>\n","      <td>[Ganciclovir implant and oral ganciclovir, Cid...</td>\n","      <td>[Experimental, Experimental]</td>\n","      <td>[&lt;=18 years, Between 18 and 65 years, &gt;=65 yea...</td>\n","      <td>[United States]</td>\n","      <td>[BG000, BG001, BG002]</td>\n","      <td>[Ganciclovir device and oral dose of Ganciclov...</td>\n","      <td>[Ganciclovir Implant and Oral Ganciclovir, Cid...</td>\n","      <td>[]</td>\n","      <td>[Age, Categorical, Sex: Female, Male, Region o...</td>\n","      <td>[Participants, Participants, participants]</td>\n","      <td>Randomized</td>\n","      <td>Parallel Assignment</td>\n","      <td>Treatment</td>\n","      <td>[]</td>\n","      <td>37</td>\n","      <td>[2114.408936, 1189.4198, 2287.767578, 3559.750...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NCT00000170</td>\n","      <td>Jaeb Center for Health Research</td>\n","      <td>OTHER</td>\n","      <td>April 1999</td>\n","      <td>[Amblyopia]</td>\n","      <td>[Brain Diseases, Central Nervous System Diseas...</td>\n","      <td>[BC10, BC11, BC23, All]</td>\n","      <td>[D000000550]</td>\n","      <td>[Jaeb Center for Health Research]</td>\n","      <td>[OTHER]</td>\n","      <td>[National Eye Institute (NEI)]</td>\n","      <td>[NIH]</td>\n","      <td>[Inclusion Criteria:\\n\\nPatients must be 7 yea...</td>\n","      <td>419</td>\n","      <td>Actual</td>\n","      <td>No</td>\n","      <td>All</td>\n","      <td>[Child]</td>\n","      <td>None</td>\n","      <td>6 Years</td>\n","      <td>[Wilmer Eye Institute]</td>\n","      <td>[Baltimore]</td>\n","      <td>[Maryland]</td>\n","      <td>[21287-9028]</td>\n","      <td>[United States]</td>\n","      <td>[Drug, Device]</td>\n","      <td>[Atropine, Eye Patch]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>None</td>\n","      <td>[Atropine]</td>\n","      <td>[Device: Eye Patch, Drug: Atropine]</td>\n","      <td>[Patching, Atropine]</td>\n","      <td>[Active Comparator, Active Comparator]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>Randomized</td>\n","      <td>Parallel Assignment</td>\n","      <td>Treatment</td>\n","      <td>[Amblyopia, patching, atropine]</td>\n","      <td>172</td>\n","      <td>[1717.821167]</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>NCT00000177</td>\n","      <td>National Institute on Aging (NIA)</td>\n","      <td>NIH</td>\n","      <td>October 1995</td>\n","      <td>[Alzheimer Disease]</td>\n","      <td>[Dementia, Brain Diseases, Central Nervous Sys...</td>\n","      <td>[BC10, BXM, All, Rare]</td>\n","      <td>[D000000544]</td>\n","      <td>[National Institute on Aging (NIA)]</td>\n","      <td>[NIH]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[Inclusion Criteria:\\n\\nWomen with a diagnosis...</td>\n","      <td>120</td>\n","      <td>None</td>\n","      <td>No</td>\n","      <td>Female</td>\n","      <td>[Adult, Older Adult]</td>\n","      <td>60 Years</td>\n","      <td>None</td>\n","      <td>[University of Alabama, Birmingham, University...</td>\n","      <td>[Birmingham, San Diego, Jacksonville, Tampa, A...</td>\n","      <td>[Alabama, California, Florida, Florida, Georgi...</td>\n","      <td>[35294-0017, 92093, 32225, 30329, 60612, 62702...</td>\n","      <td>[United States, United States, United States, ...</td>\n","      <td>[Drug]</td>\n","      <td>[Estrogen]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>None</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>Randomized</td>\n","      <td>None</td>\n","      <td>Treatment</td>\n","      <td>[Alzheimer's disease, Estrogen]</td>\n","      <td>39</td>\n","      <td>[230.015213, 1189.4198, 567.188477, 494.567383...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>NCT00000271</td>\n","      <td>New York State Psychiatric Institute</td>\n","      <td>OTHER</td>\n","      <td>January 1995</td>\n","      <td>[Cocaine-Related Disorders, Substance-Related ...</td>\n","      <td>[Pathologic Processes, Chemically-Induced Diso...</td>\n","      <td>[BXM, All, BC25]</td>\n","      <td>[D000004194, D000019966, D000019970]</td>\n","      <td>[New York State Psychiatric Institute]</td>\n","      <td>[OTHER]</td>\n","      <td>[National Institute on Drug Abuse (NIDA)]</td>\n","      <td>[NIH]</td>\n","      <td>[Inclusion:\\n\\nMeets DSM-IV criteria for curre...</td>\n","      <td>111</td>\n","      <td>Actual</td>\n","      <td>No</td>\n","      <td>All</td>\n","      <td>[Adult]</td>\n","      <td>18 Years</td>\n","      <td>60 Years</td>\n","      <td>[Research Foundation for Mental Hygiene]</td>\n","      <td>[New York]</td>\n","      <td>[New York]</td>\n","      <td>[10032]</td>\n","      <td>[United States]</td>\n","      <td>[Drug, Drug]</td>\n","      <td>[Desipramine, Placebo]</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>None</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>None</td>\n","      <td>[Participants were treated with desipramine, u...</td>\n","      <td>[Drug: Desipramine, Drug: Placebo]</td>\n","      <td>[Desipramine, Placebo]</td>\n","      <td>[Experimental, Placebo Comparator]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>Randomized</td>\n","      <td>Parallel Assignment</td>\n","      <td>Treatment</td>\n","      <td>[Desipramine, cocaine, depression]</td>\n","      <td>93</td>\n","      <td>[241.777069]</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>NCT00000273</td>\n","      <td>New York State Psychiatric Institute</td>\n","      <td>OTHER</td>\n","      <td>August 1995</td>\n","      <td>[Heroin Dependence, Opioid-Related Disorders, ...</td>\n","      <td>[Pathologic Processes, Chemically-Induced Diso...</td>\n","      <td>[BC25, BXM, All]</td>\n","      <td>[D000004194, D000019966, D000009293, D000006556]</td>\n","      <td>[New York State Psychiatric Institute]</td>\n","      <td>[OTHER]</td>\n","      <td>[National Institute on Drug Abuse (NIDA)]</td>\n","      <td>[NIH]</td>\n","      <td>[Inclusion Criterion\\n\\nDSM IV criteria for op...</td>\n","      <td>8</td>\n","      <td>Actual</td>\n","      <td>No</td>\n","      <td>All</td>\n","      <td>[Adult]</td>\n","      <td>18 Years</td>\n","      <td>45 Years</td>\n","      <td>[Columbia University, New York State Psychiatr...</td>\n","      <td>[New York, New York]</td>\n","      <td>[New York, New York]</td>\n","      <td>[10032, 10032]</td>\n","      <td>[United States, United States]</td>\n","      <td>[Drug]</td>\n","      <td>[opiates]</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>None</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>None</td>\n","      <td>[Opiate-dependent individuals who were current...</td>\n","      <td>[Drug: opiates]</td>\n","      <td>[Opiates]</td>\n","      <td>[Experimental]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>None</td>\n","      <td>Single Group Assignment</td>\n","      <td>Treatment</td>\n","      <td>[heroin, opioid disorders, substance related d...</td>\n","      <td>123</td>\n","      <td>[7901.219238, 7901.219238]</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>NCT00023777</td>\n","      <td>Southwest Oncology Group</td>\n","      <td>NETWORK</td>\n","      <td>August 2001</td>\n","      <td>[Leukemia]</td>\n","      <td>[Neoplasms by Histologic Type, Neoplasms]</td>\n","      <td>[BC04, All, BC15, Rare]</td>\n","      <td>[D000007938, D000007951, D000015470]</td>\n","      <td>[Southwest Oncology Group]</td>\n","      <td>[NETWORK]</td>\n","      <td>[National Cancer Institute (NCI)]</td>\n","      <td>[NIH]</td>\n","      <td>[DISEASE CHARACTERISTICS:\\n\\nHistologically co...</td>\n","      <td>71</td>\n","      <td>Actual</td>\n","      <td>No</td>\n","      <td>All</td>\n","      <td>[Adult, Older Adult]</td>\n","      <td>56 Years</td>\n","      <td>None</td>\n","      <td>[MBCCOP - Gulf Coast, CCOP - Greater Phoenix, ...</td>\n","      <td>[Mobile, Phoenix, Phoenix, Tucson, Tucson, Lit...</td>\n","      <td>[Alabama, Arizona, Arizona, Arizona, Arizona, ...</td>\n","      <td>[36688, 85006-2726, 85012, 85723, 85724, 72205...</td>\n","      <td>[United States, United States, United States, ...</td>\n","      <td>[Biological, Biological, Drug, Drug]</td>\n","      <td>[filgrastim, sargramostim, cytarabine, daunoru...</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>None</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>Non-Randomized</td>\n","      <td>Single Group Assignment</td>\n","      <td>Treatment</td>\n","      <td>[untreated adult acute myeloid leukemia, adult...</td>\n","      <td>80</td>\n","      <td>[106.749603, 415.544464, 707.451721, 554.12750...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>NCT00023829</td>\n","      <td>Radiation Therapy Oncology Group</td>\n","      <td>NETWORK</td>\n","      <td>August 2001</td>\n","      <td>[Prostate Cancer]</td>\n","      <td>[Genital Neoplasms, Male, Urogenital Neoplasms...</td>\n","      <td>[BC04, BXS, All]</td>\n","      <td>[D000011471]</td>\n","      <td>[Radiation Therapy Oncology Group]</td>\n","      <td>[NETWORK]</td>\n","      <td>[National Cancer Institute (NCI), NCIC Clinica...</td>\n","      <td>[NIH, NETWORK]</td>\n","      <td>[DISEASE CHARACTERISTICS:\\n\\nHistologically co...</td>\n","      <td>67</td>\n","      <td>Actual</td>\n","      <td>No</td>\n","      <td>Male</td>\n","      <td>[Child, Adult, Older Adult]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>[Toronto Sunnybrook Regional Cancer Centre]</td>\n","      <td>[Toronto]</td>\n","      <td>[Ontario]</td>\n","      <td>[M4N 3M5]</td>\n","      <td>[Canada]</td>\n","      <td>[Drug, Drug, Drug, Procedure, Radiation]</td>\n","      <td>[bicalutamide, flutamide, releasing hormone ag...</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>None</td>\n","      <td>[Luteinizing hormone-releasing hormone (LH-RH)...</td>\n","      <td>[Drug: bicalutamide, Drug: flutamide, Drug: re...</td>\n","      <td>[LH-RH agonist plus radiation therapy, Radiati...</td>\n","      <td>[Experimental, Active Comparator, Active Compa...</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>Randomized</td>\n","      <td>Parallel Assignment</td>\n","      <td>Treatment</td>\n","      <td>[stage III prostate cancer, stage IIB prostate...</td>\n","      <td>34</td>\n","      <td>[5237.469727]</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>NCT00023998</td>\n","      <td>National Cancer Institute (NCI)</td>\n","      <td>NIH</td>\n","      <td>July 2001</td>\n","      <td>[Metastatic Osteosarcoma]</td>\n","      <td>[Neoplasms, Bone Tissue, Neoplasms, Connective...</td>\n","      <td>[BC04, All, BC17, Rare]</td>\n","      <td>[D000012516]</td>\n","      <td>[National Cancer Institute (NCI)]</td>\n","      <td>[NIH]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[Inclusion Criteria:\\n\\nHistologically confirm...</td>\n","      <td>80</td>\n","      <td>Actual</td>\n","      <td>No</td>\n","      <td>All</td>\n","      <td>[Child, Adult]</td>\n","      <td>None</td>\n","      <td>30 Years</td>\n","      <td>[Children's Oncology Group]</td>\n","      <td>[Arcadia]</td>\n","      <td>[California]</td>\n","      <td>[91006-3776]</td>\n","      <td>[United States]</td>\n","      <td>[Drug, Drug, Drug, Drug, Biological, Procedure...</td>\n","      <td>[doxorubicin hydrochloride, cisplatin, methotr...</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>None</td>\n","      <td>[See detailed description.]</td>\n","      <td>[Drug: doxorubicin hydrochloride, Drug: cispla...</td>\n","      <td>[Treatment (combination chemotherapy)]</td>\n","      <td>[Experimental]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>None</td>\n","      <td>Single Group Assignment</td>\n","      <td>Treatment</td>\n","      <td>[]</td>\n","      <td>70</td>\n","      <td>[1168.816162]</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>NCT00024102</td>\n","      <td>Alliance for Clinical Trials in Oncology</td>\n","      <td>OTHER</td>\n","      <td>September 2001</td>\n","      <td>[Breast Cancer]</td>\n","      <td>[Neoplasms by Site, Neoplasms, Breast Diseases...</td>\n","      <td>[BC04, BC17, All]</td>\n","      <td>[D000001943]</td>\n","      <td>[Alliance for Clinical Trials in Oncology]</td>\n","      <td>[OTHER]</td>\n","      <td>[National Cancer Institute (NCI), NCIC Clinica...</td>\n","      <td>[NIH, NETWORK]</td>\n","      <td>[Patients with operable, histologically confir...</td>\n","      <td>633</td>\n","      <td>Actual</td>\n","      <td>No</td>\n","      <td>Female</td>\n","      <td>[Older Adult]</td>\n","      <td>65 Years</td>\n","      <td>None</td>\n","      <td>[University of Alabama at Birmingham Comprehen...</td>\n","      <td>[Birmingham, Mobile, Scottsdale, Tucson, Hot S...</td>\n","      <td>[Alabama, Alabama, Arizona, Arizona, Arkansas,...</td>\n","      <td>[35294, 36652-2144, 85259-5499, 85724-5024, 71...</td>\n","      <td>[United States, United States, United States, ...</td>\n","      <td>[Drug, Drug]</td>\n","      <td>[Standard Treatment, capecitabine]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>Reported at the end of each cycle while patien...</td>\n","      <td>[]</td>\n","      <td>[Patient/Physician choice of:\\n\\nCMF: cyclopho...</td>\n","      <td>[Standard Chemotherapy, Capecitabine]</td>\n","      <td>[STARTED, COMPLETED, NOT COMPLETED]</td>\n","      <td>[Overall Study]</td>\n","      <td>This was an intergroup study led by the CALGB....</td>\n","      <td>[Patient/Physician choice of cyclophosphamide ...</td>\n","      <td>[Drug: Standard Treatment, Drug: capecitabine]</td>\n","      <td>[Standard Chemotherapy, Capecitabine]</td>\n","      <td>[Active Comparator, Experimental]</td>\n","      <td>[Female, Male]</td>\n","      <td>[65-69 years, 70-79 years, &gt;=80 years, United ...</td>\n","      <td>[BG000, BG001, BG002]</td>\n","      <td>[Patient/Physician choice of:\\n\\nCMF: cyclopho...</td>\n","      <td>[Standard Chemotherapy, Capecitabine, Total]</td>\n","      <td>[]</td>\n","      <td>[Age, Customized, Sex: Female, Male, Region of...</td>\n","      <td>[participants, Participants, participants, par...</td>\n","      <td>Randomized</td>\n","      <td>Parallel Assignment</td>\n","      <td>Treatment</td>\n","      <td>[stage I breast cancer, stage II breast cancer...</td>\n","      <td>134</td>\n","      <td>[230.015213, 496.689728, 354.67746, 290.867279...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>NCT00024258</td>\n","      <td>Memorial Sloan Kettering Cancer Center</td>\n","      <td>OTHER</td>\n","      <td>March 2001</td>\n","      <td>[Brain and Central Nervous System Tumors, Chil...</td>\n","      <td>[Neoplasms, Connective and Soft Tissue, Neopla...</td>\n","      <td>[BC04, BC11, All, BXS, BC06, BC19, BC16, BC10,...</td>\n","      <td>[D000012509, D000009447, D000008113, D00000937...</td>\n","      <td>[Memorial Sloan Kettering Cancer Center]</td>\n","      <td>[OTHER]</td>\n","      <td>[National Cancer Institute (NCI)]</td>\n","      <td>[NIH]</td>\n","      <td>[DISEASE CHARACTERISTICS:\\n\\nHistologically co...</td>\n","      <td>22</td>\n","      <td>Actual</td>\n","      <td>No</td>\n","      <td>All</td>\n","      <td>[Child, Adult]</td>\n","      <td>None</td>\n","      <td>40 Years</td>\n","      <td>[Memorial Sloan-Kettering Cancer Center]</td>\n","      <td>[New York]</td>\n","      <td>[New York]</td>\n","      <td>[10021]</td>\n","      <td>[United States]</td>\n","      <td>[Drug]</td>\n","      <td>[arsenic trioxide]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>[Withdrawal by Subject, Disease Progression]</td>\n","      <td>[Patients receive arsenic trioxide IV over 1-4...</td>\n","      <td>[Arsenic Trioxide]</td>\n","      <td>[STARTED, COMPLETED, NOT COMPLETED]</td>\n","      <td>[Overall Study]</td>\n","      <td>None</td>\n","      <td>[Patients receive arsenic trioxide IV over 1-4...</td>\n","      <td>[Drug: arsenic trioxide]</td>\n","      <td>[Arsenic Trioxide]</td>\n","      <td>[Experimental]</td>\n","      <td>[&lt;=18 years, Between 18 and 65 years, &gt;=65 yea...</td>\n","      <td>[United States]</td>\n","      <td>[BG000]</td>\n","      <td>[Patients receive arsenic trioxide IV over 1-4...</td>\n","      <td>[Arsenic Trioxide]</td>\n","      <td>[]</td>\n","      <td>[Age, Categorical, Sex: Female, Male, Region o...</td>\n","      <td>[Participants, Participants, participants]</td>\n","      <td>None</td>\n","      <td>Single Group Assignment</td>\n","      <td>Treatment</td>\n","      <td>[metastatic osteosarcoma, recurrent childhood ...</td>\n","      <td>98</td>\n","      <td>[7901.219238]</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>982 rows × 57 columns</p>\n","</div>"],"text/plain":["           NCTId  ... Phase=Phase 1\n","0    NCT00000143  ...           0.0\n","1    NCT00000170  ...           0.0\n","2    NCT00000177  ...           0.0\n","15   NCT00000271  ...           0.0\n","16   NCT00000273  ...           0.0\n","..           ...  ...           ...\n","995  NCT00023777  ...           0.0\n","996  NCT00023829  ...           0.0\n","997  NCT00023998  ...           0.0\n","998  NCT00024102  ...           0.0\n","999  NCT00024258  ...           0.0\n","\n","[982 rows x 57 columns]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["NCTId                             object\n","OrgFullName                       object\n","OrgClass                          object\n","StartDate                         object\n","Condition                         object\n","ConditionAncestorTerm             object\n","ConditionBrowseBranchAbbrev       object\n","ConditionMeshId                   object\n","LeadSponsorName                   object\n","LeadSponsorClass                  object\n","CollaboratorName                  object\n","CollaboratorClass                 object\n","EligibilityCriteria               object\n","EnrollmentCount                    int64\n","EnrollmentType                    object\n","HealthyVolunteers                 object\n","Gender                            object\n","StdAge                            object\n","MinimumAge                        object\n","MaximumAge                        object\n","LocationFacility                  object\n","LocationCity                      object\n","LocationState                     object\n","LocationZip                       object\n","LocationCountry                   object\n","InterventionType                  object\n","InterventionName                  object\n","IsFDARegulatedDevice              object\n","IsFDARegulatedDrug                object\n","EventsTimeFrame                   object\n","FlowDropWithdrawType              object\n","FlowGroupDescription              object\n","FlowGroupTitle                    object\n","FlowMilestoneType                 object\n","FlowPeriodTitle                   object\n","FlowRecruitmentDetails            object\n","ArmGroupDescription               object\n","ArmGroupInterventionName          object\n","ArmGroupLabel                     object\n","ArmGroupType                      object\n","BaselineCategoryTitle             object\n","BaselineClassTitle                object\n","BaselineDenomCountGroupId         object\n","BaselineGroupDescription          object\n","BaselineGroupTitle                object\n","BaselineMeasureDispersionType     object\n","BaselineMeasureTitle              object\n","BaselineMeasureUnitOfMeasure      object\n","DesignAllocation                  object\n","DesignInterventionModel           object\n","DesignPrimaryPurpose              object\n","Keyword                           object\n","EnrollmentDuration                 int64\n","LocationPopulationDensity         object\n","Phase=Phase 3                    float64\n","Phase=Phase 2                    float64\n","Phase=Phase 1                    float64\n","dtype: object"]},"metadata":{"tags":[]}},{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'EnrollmentCount_new'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-ca993d7d2ca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnrollmentOutlierRemover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'IQR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-d9349ccbc545>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Get data of the different phases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_phase_series\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phase1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Phase=Phase 1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Phase=Phase 2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EnrollmentCount_new'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_phase_series\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phase1_2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Phase=Phase 1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Phase=Phase 2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EnrollmentCount_new'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_phase_series\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phase2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Phase=Phase 1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Phase=Phase 2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Phase=Phase 3\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EnrollmentCount_new'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'EnrollmentCount_new'"]}]},{"cell_type":"markdown","metadata":{"id":"QUjYCpiyspiI","colab_type":"text"},"source":["##Transformers to count number of distinct values\n","\n"]},{"cell_type":"code","metadata":{"id":"zzjNNh74s3VT","colab_type":"code","colab":{}},"source":["class DistinctCounter(TransformerMixin, BaseEstimator):\n","    \"\"\" Creates new features with the number of items of features containing a list. \"EligibilityCriteria\" is an exception where the number of characters is counted.\n","\n","    Parameters\n","    --------\n","\n","    Attributes\n","    --------\n","\n","    Notes\n","    --------\n","    \n","    \"\"\"\n","    \n","    def __init__(self):\n","        pass\n","    \n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def transform(self, X):\n","        X_new = X.copy()\n","        for feature in X.columns:\n","            if feature == \"EligibilityCriteria\":\n","                values = []\n","                for index, row in X.iterrows():\n","                    if len(row.EligibilityCriteria): values.append(len(row.EligibilityCriteria[0]))\n","                    else: values.append(0)\n","                X_new[\"#EligiCriteria\"] = values\n","            else:\n","                name = f\"#Diff{feature}\"\n","                values = []\n","                for index, row in X.iterrows():\n","                    if len(row[feature]): values.append(len(row[feature]))\n","                    else: values.append(0)\n","                X_new[name] = values\n","        return X_new"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h9r8F7x_spLk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1599207452137,"user_tz":-120,"elapsed":2182,"user":{"displayName":"Stefan Sousa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnGBw3kCtWt-XAq5nTYjp1DY0_T5bIfSzFTajnfg=s64","userId":"08251656603606889983"}},"outputId":"c7cb0a43-0ca5-4d6e-82e8-0dbc6acc2a6c"},"source":["TO_COUNT_FEATS = ['Condition', 'ConditionAncestorTerm', 'EligibilityCriteria', 'LocationFacility', 'LocationCity', 'LocationCountry', 'ArmGroupLabel']\n","\n","display(df_raw[TO_COUNT_FEATS].head(10))\n","\n","pipeline = Pipeline([\n","            ('extract', FeatureSelector(TO_COUNT_FEATS)),\n","            ('counter', DistinctCounter())\n","        ])\n","\n","pipeline.fit_transform(df_raw[TO_COUNT_FEATS]).head(10)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Condition</th>\n","      <th>ConditionAncestorTerm</th>\n","      <th>EligibilityCriteria</th>\n","      <th>LocationFacility</th>\n","      <th>LocationCity</th>\n","      <th>LocationCountry</th>\n","      <th>ArmGroupLabel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[Cytomegalovirus Retinitis, HIV Infections]</td>\n","      <td>[Virus Diseases, Retinal Diseases, Eye Disease...</td>\n","      <td>[Inclusion criteria:\\n\\nAge 13 years or older\\...</td>\n","      <td>[Department of Ophthalmology, University of Ca...</td>\n","      <td>[Irvine, La Jolla, Los Angeles, Los Angeles, S...</td>\n","      <td>[United States, United States, United States, ...</td>\n","      <td>[Ganciclovir implant and oral ganciclovir, Cid...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[Amblyopia]</td>\n","      <td>[Brain Diseases, Central Nervous System Diseas...</td>\n","      <td>[Inclusion Criteria:\\n\\nPatients must be 7 yea...</td>\n","      <td>[Wilmer Eye Institute]</td>\n","      <td>[Baltimore]</td>\n","      <td>[United States]</td>\n","      <td>[Patching, Atropine]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[Alzheimer Disease]</td>\n","      <td>[Dementia, Brain Diseases, Central Nervous Sys...</td>\n","      <td>[Inclusion Criteria:\\n\\nWomen with a diagnosis...</td>\n","      <td>[University of Alabama, Birmingham, University...</td>\n","      <td>[Birmingham, San Diego, Jacksonville, Tampa, A...</td>\n","      <td>[United States, United States, United States, ...</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>[Cocaine-Related Disorders, Substance-Related ...</td>\n","      <td>[Pathologic Processes, Chemically-Induced Diso...</td>\n","      <td>[Inclusion:\\n\\nMeets DSM-IV criteria for curre...</td>\n","      <td>[Research Foundation for Mental Hygiene]</td>\n","      <td>[New York]</td>\n","      <td>[United States]</td>\n","      <td>[Desipramine, Placebo]</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>[Heroin Dependence, Opioid-Related Disorders, ...</td>\n","      <td>[Pathologic Processes, Chemically-Induced Diso...</td>\n","      <td>[Inclusion Criterion\\n\\nDSM IV criteria for op...</td>\n","      <td>[Columbia University, New York State Psychiatr...</td>\n","      <td>[New York, New York]</td>\n","      <td>[United States, United States]</td>\n","      <td>[Opiates]</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>[Opioid-Related Disorders]</td>\n","      <td>[Substance-Related Disorders, Chemically-Induc...</td>\n","      <td>[Inclusion Criteria:\\n\\nMales/Females, ages 21...</td>\n","      <td>[Friends Research Institute]</td>\n","      <td>[Los Angeles]</td>\n","      <td>[United States]</td>\n","      <td>[buprenorphine, buprenorphine and ultra-low do...</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>[Cocaine-Related Disorders, Opioid-Related Dis...</td>\n","      <td>[Pathologic Processes, Substance-Related Disor...</td>\n","      <td>[Inclusion Criteria- Subject must:\\n\\nExhibit ...</td>\n","      <td>[University of Texas Health Science Center]</td>\n","      <td>[Houston]</td>\n","      <td>[United States]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>[Cocaine-Related Disorders, Substance-Related ...</td>\n","      <td>[Pathologic Processes, Chemically-Induced Diso...</td>\n","      <td>[Inclusion:\\n\\ngood standing at methadone main...</td>\n","      <td>[NYS Psychiatric Institute]</td>\n","      <td>[New York]</td>\n","      <td>[United States]</td>\n","      <td>[PLacebo, Risperidone]</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>[Opioid-Related Disorders]</td>\n","      <td>[Substance-Related Disorders, Chemically-Induc...</td>\n","      <td>[Inclusion Criteria:\\n\\nM/F ages 18-65. Meet D...</td>\n","      <td>[Friends Research Institute]</td>\n","      <td>[Los Angeles]</td>\n","      <td>[United States]</td>\n","      <td>[1; liquid formulation, 2; tablet formulation]</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>[Cocaine-Related Disorders]</td>\n","      <td>[Substance-Related Disorders, Chemically-Induc...</td>\n","      <td>[Inclusion Criteria:\\n\\nCocaine dependent\\n\\nE...</td>\n","      <td>[Johns Hopkins University School of Medicine]</td>\n","      <td>[Baltimore]</td>\n","      <td>[United States]</td>\n","      <td>[]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Condition  ...                                      ArmGroupLabel\n","0         [Cytomegalovirus Retinitis, HIV Infections]  ...  [Ganciclovir implant and oral ganciclovir, Cid...\n","1                                         [Amblyopia]  ...                               [Patching, Atropine]\n","2                                 [Alzheimer Disease]  ...                                                 []\n","15  [Cocaine-Related Disorders, Substance-Related ...  ...                             [Desipramine, Placebo]\n","16  [Heroin Dependence, Opioid-Related Disorders, ...  ...                                          [Opiates]\n","19                         [Opioid-Related Disorders]  ...  [buprenorphine, buprenorphine and ultra-low do...\n","21  [Cocaine-Related Disorders, Opioid-Related Dis...  ...                                                 []\n","24  [Cocaine-Related Disorders, Substance-Related ...  ...                             [PLacebo, Risperidone]\n","26                         [Opioid-Related Disorders]  ...     [1; liquid formulation, 2; tablet formulation]\n","27                        [Cocaine-Related Disorders]  ...                                                 []\n","\n","[10 rows x 7 columns]"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Condition</th>\n","      <th>ConditionAncestorTerm</th>\n","      <th>EligibilityCriteria</th>\n","      <th>LocationFacility</th>\n","      <th>LocationCity</th>\n","      <th>LocationCountry</th>\n","      <th>ArmGroupLabel</th>\n","      <th>#DiffCondition</th>\n","      <th>#DiffConditionAncestorTerm</th>\n","      <th>#EligiCriteria</th>\n","      <th>#DiffLocationFacility</th>\n","      <th>#DiffLocationCity</th>\n","      <th>#DiffLocationCountry</th>\n","      <th>#DiffArmGroupLabel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[Cytomegalovirus Retinitis, HIV Infections]</td>\n","      <td>[Virus Diseases, Retinal Diseases, Eye Disease...</td>\n","      <td>[Inclusion criteria:\\n\\nAge 13 years or older\\...</td>\n","      <td>[Department of Ophthalmology, University of Ca...</td>\n","      <td>[Irvine, La Jolla, Los Angeles, Los Angeles, S...</td>\n","      <td>[United States, United States, United States, ...</td>\n","      <td>[Ganciclovir implant and oral ganciclovir, Cid...</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>1213</td>\n","      <td>19</td>\n","      <td>19</td>\n","      <td>19</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[Amblyopia]</td>\n","      <td>[Brain Diseases, Central Nervous System Diseas...</td>\n","      <td>[Inclusion Criteria:\\n\\nPatients must be 7 yea...</td>\n","      <td>[Wilmer Eye Institute]</td>\n","      <td>[Baltimore]</td>\n","      <td>[United States]</td>\n","      <td>[Patching, Atropine]</td>\n","      <td>1</td>\n","      <td>8</td>\n","      <td>400</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[Alzheimer Disease]</td>\n","      <td>[Dementia, Brain Diseases, Central Nervous Sys...</td>\n","      <td>[Inclusion Criteria:\\n\\nWomen with a diagnosis...</td>\n","      <td>[University of Alabama, Birmingham, University...</td>\n","      <td>[Birmingham, San Diego, Jacksonville, Tampa, A...</td>\n","      <td>[United States, United States, United States, ...</td>\n","      <td>[]</td>\n","      <td>1</td>\n","      <td>8</td>\n","      <td>318</td>\n","      <td>25</td>\n","      <td>25</td>\n","      <td>25</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>[Cocaine-Related Disorders, Substance-Related ...</td>\n","      <td>[Pathologic Processes, Chemically-Induced Diso...</td>\n","      <td>[Inclusion:\\n\\nMeets DSM-IV criteria for curre...</td>\n","      <td>[Research Foundation for Mental Hygiene]</td>\n","      <td>[New York]</td>\n","      <td>[United States]</td>\n","      <td>[Desipramine, Placebo]</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>1477</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>[Heroin Dependence, Opioid-Related Disorders, ...</td>\n","      <td>[Pathologic Processes, Chemically-Induced Diso...</td>\n","      <td>[Inclusion Criterion\\n\\nDSM IV criteria for op...</td>\n","      <td>[Columbia University, New York State Psychiatr...</td>\n","      <td>[New York, New York]</td>\n","      <td>[United States, United States]</td>\n","      <td>[Opiates]</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>903</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>[Opioid-Related Disorders]</td>\n","      <td>[Substance-Related Disorders, Chemically-Induc...</td>\n","      <td>[Inclusion Criteria:\\n\\nMales/Females, ages 21...</td>\n","      <td>[Friends Research Institute]</td>\n","      <td>[Los Angeles]</td>\n","      <td>[United States]</td>\n","      <td>[buprenorphine, buprenorphine and ultra-low do...</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>475</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>[Cocaine-Related Disorders, Opioid-Related Dis...</td>\n","      <td>[Pathologic Processes, Substance-Related Disor...</td>\n","      <td>[Inclusion Criteria- Subject must:\\n\\nExhibit ...</td>\n","      <td>[University of Texas Health Science Center]</td>\n","      <td>[Houston]</td>\n","      <td>[United States]</td>\n","      <td>[]</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>956</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>[Cocaine-Related Disorders, Substance-Related ...</td>\n","      <td>[Pathologic Processes, Chemically-Induced Diso...</td>\n","      <td>[Inclusion:\\n\\ngood standing at methadone main...</td>\n","      <td>[NYS Psychiatric Institute]</td>\n","      <td>[New York]</td>\n","      <td>[United States]</td>\n","      <td>[PLacebo, Risperidone]</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>1097</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>[Opioid-Related Disorders]</td>\n","      <td>[Substance-Related Disorders, Chemically-Induc...</td>\n","      <td>[Inclusion Criteria:\\n\\nM/F ages 18-65. Meet D...</td>\n","      <td>[Friends Research Institute]</td>\n","      <td>[Los Angeles]</td>\n","      <td>[United States]</td>\n","      <td>[1; liquid formulation, 2; tablet formulation]</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>422</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>[Cocaine-Related Disorders]</td>\n","      <td>[Substance-Related Disorders, Chemically-Induc...</td>\n","      <td>[Inclusion Criteria:\\n\\nCocaine dependent\\n\\nE...</td>\n","      <td>[Johns Hopkins University School of Medicine]</td>\n","      <td>[Baltimore]</td>\n","      <td>[United States]</td>\n","      <td>[]</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>205</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Condition  ... #DiffArmGroupLabel\n","0         [Cytomegalovirus Retinitis, HIV Infections]  ...                  2\n","1                                         [Amblyopia]  ...                  2\n","2                                 [Alzheimer Disease]  ...                  0\n","15  [Cocaine-Related Disorders, Substance-Related ...  ...                  2\n","16  [Heroin Dependence, Opioid-Related Disorders, ...  ...                  1\n","19                         [Opioid-Related Disorders]  ...                  2\n","21  [Cocaine-Related Disorders, Opioid-Related Dis...  ...                  0\n","24  [Cocaine-Related Disorders, Substance-Related ...  ...                  2\n","26                         [Opioid-Related Disorders]  ...                  2\n","27                        [Cocaine-Related Disorders]  ...                  0\n","\n","[10 rows x 14 columns]"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"0mCbJ9K8r1iX","colab_type":"text"},"source":["##Transformers for patients distribution"]},{"cell_type":"code","metadata":{"id":"pP4dwLi8r1Lg","colab_type":"code","colab":{}},"source":["class PatientsDistributionTransformer(TransformerMixin, BaseEstimator):\n","    \"\"\" Distributes the number of patients over number of facilities and number of countries.\n","\n","    Parameters\n","    --------\n","\n","    Attributes\n","    --------\n","\n","    Notes\n","    --------\n","    \n","    \"\"\"\n","    def __init__(self):\n","        pass\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        X_new = X.copy()\n","        features = ['LocationFacility', 'LocationCountry']\n","        for feature in features:\n","            name = f\"#Pts/#Diff{feature}\"\n","            s = pd.Series(X['EnrollmentCount']/[len(set(value)) for value in X[feature]])\n","            X_new[name] = pd.to_numeric(s, errors='coerce').fillna(0, downcast='infer')\n","        return X_new"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4Xud_bTsFXL","colab_type":"code","colab":{}},"source":["transformer = PatientsDistributionTransformer()\n","transformer.fit_transform(df_raw).head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hwCto5eI2EX6","colab_type":"text"},"source":["## Transformer integrating country and regional data\n","\n","A combination transformer from all alternative ones"]},{"cell_type":"code","metadata":{"id":"64WjhjL54Eti","colab_type":"code","colab":{}},"source":["class LocationDataTransformer(TransformerMixin, BaseEstimator):\n","    \"\"\" This transformer contains all alternative transformers relating to location (CountryDataTransformer - CityPopulation - PerCountryTransformer - PerCityTransformer)\n","        transformer:\n","            totalCombine: a combination between CountryDataTransformer and CityPopulationTransformer (Basically just add one more feature to CountryDataTransformer) => default\n","            perCity: divide a study into multiple rows, equivalent to list of cities in the study\n","            perCountry: divide a study into multiple rows, equaivalent to list of country in the study\n","        strategy:\n","            simple = just calculates the normal average of each country attribute of all different countries involved / average population density for each city => default\n","            weighted = calculates the average of each country attribute according to the number of occurences / average population density in the list of distinct cities\n","        mean:\n","            trials = uses the mean value of all trials to fill missing country values\n","            worldwide = uses the worldwide mean for this specific attribute\n","    \"\"\"\n","    def __init__(self, country_data, transformer='totalCombine', strategy='simple', mean='trials', debug=False):\n","        self.transformer = transformer\n","        self.country_data = country_data.fillna(0)\n","        self.strategy = strategy\n","        self.mean = mean\n","\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def most_frequent(self, List): # Returning array of most frequent elements in a list\n","        obj = {}\n","        arr = []\n","        for item in List:\n","            obj[item] = List.count(item)\n","        maxNum = obj[max(obj)]\n","        for key, value in obj.items():\n","            if value == maxNum:\n","                arr.append(key)\n","        return arr\n","\n","    def transform(self, X):\n","        X_new = X.reset_index(drop=True)\n","\n","        if self.transformer == 'totalCombine':           \n","\n","            special = {\n","                'Former Yugoslavia': 'Serbia',\n","                'Former Serbia and Montenegro': 'Serbia',\n","                'Holy See (Vatican City State)': 'Italy',\n","                'Macedonia, The Former Yugoslav Republic of': 'North Macedonia',\n","                'Swaziland': 'Eswatini',\n","                'Netherlands Antilles': 'Curacao'\n","            }\n","\n","            # creating columns for each country attribute\n","            columns = self.country_data.drop('countryName', axis=1).columns\n","            for attr in columns:\n","                X_new[f\"avg_{attr}\"], X_new[f\"min_{attr}\"], X_new[f\"max_{attr}\"], X_new[f\"main_country_{attr}\"] = None, None, None, None\n","                X_new[f\"avg_{attr}\"] = X_new[f\"avg_{attr}\"].astype(np.float64)\n","                X_new[f\"min_{attr}\"] = X_new[f\"min_{attr}\"].astype(np.float64)\n","                X_new[f\"max_{attr}\"] = X_new[f\"max_{attr}\"].astype(np.float64)\n","                X_new[f\"main_country_{attr}\"] = X_new[f\"main_country_{attr}\"].astype(np.float64)\n","\n","            \n","            X_new['avg_city_population'], X_new['min_city_population'], X_new['max_city_population'] = 0, 0, 0\n","            for index, locationCity, locationCountry, locationPopulationDensity in zip(X_new.index, X_new['LocationCity'], X_new['LocationCountry'], X_new['LocationPopulationDensity']):\n","\n","                # Add main country data\n","                if len(set(locationCountry)) == 1:\n","                    main_country = locationCountry[0]\n","                    for attr in columns:\n","                        if main_country in special.keys():\n","                            main_country = special[main_country]\n","                        X_new.at[index, f\"main_country_{attr}\"] = self.country_data[self.country_data.countryName == main_country][attr]\n","                        \n","                else: # If there are more than one country have highest number of facilities, calculate average of country features\n","                    mostFrequentCountry = self.most_frequent(locationCountry)\n","                    mainCountryDataTemp = 0\n","                    \n","                    for country in mostFrequentCountry:\n","                        if country in special.keys():\n","                            country = special[country]\n","                        for attr in columns:\n","                            mainCountryDataTemp += self.country_data[self.country_data.countryName == country][attr]\n","                            X_new.at[index, f\"main_country_{attr}\"] = mainCountryDataTemp / len(mostFrequentCountry)\n","\n","                cityList = []\n","                for i in range(len(locationCity)):\n","                    cityList.append(f\"{locationCity[i]} --- {locationCountry[i]}\")\n","\n","                if self.strategy == 'weighted':\n","                    tmp_countries = list(row['LocationCountry'])\n","                    tmp_cities = list(cityList) # Get list of involved city\n","                else:\n","                    tmp_countries = list(set(row['LocationCountry']))\n","                    tmp_cities = list(set(cityList)) # Get unique list of city\n","                    \n","                # iterate over cities and get density information\n","                density_sum = 0\n","                density_min, density_max = None, None\n","                for city in tmp_cities:\n","                    id = locationCity.index(city.split(\" --- \")[0])\n","                    tmp_density = locationPopulationDensity[id]\n","                    density_sum += tmp_density\n","                    if density_min:\n","                        if density_min > tmp_density: density_min = tmp_density\n","                    else: density_min = tmp_density\n","                    if density_max:\n","                        if density_max < tmp_density: density_max = tmp_density\n","                    else: density_max = tmp_density\n","\n","                X_new.at[index, \"avg_city_population\"] = density_sum/len(cityList)\n","                X_new.at[index, \"min_city_population\"] = density_min\n","                X_new.at[index, \"max_city_population\"] = density_max\n","\n","                # iterate over countries and get country data\n","                data_avg, data_min, data_max = {}, {}, {}\n","                countries_not_found = []\n","                for country in tmp_countries:\n","                    if country in self.country_data.values:\n","                        tmp = self.country_data[self.country_data.countryName == country]\n","                        for attr in columns:\n","                            if attr in data_avg.keys(): data_avg[attr] += float(tmp[attr])\n","                            else: data_avg[attr] = float(tmp[attr])\n","\n","                            if attr in data_min.keys():\n","                                if data_min[attr] > float(tmp[attr]): data_min[attr] = float(tmp[attr])\n","                            else: data_min[attr] = float(tmp[attr])\n","\n","                            if attr in data_max.keys():\n","                                if data_max[attr] < float(tmp[attr]): data_max[attr] = float(tmp[attr])\n","                            else: data_max[attr] = float(tmp[attr])\n","                    elif country in special.keys():\n","                        tmp = self.country_data[self.country_data.countryName == special[country]]\n","                        for attr in columns:\n","                            if attr in data_avg.keys(): data_avg[attr] += float(tmp[attr])\n","                            else: data_avg[attr] = float(tmp[attr])\n","\n","                            if attr in data_min.keys():\n","                                if data_min[attr] > float(tmp[attr]): data_min[attr] = float(tmp[attr])\n","                            else: data_min[attr] = float(tmp[attr])\n","\n","                            if attr in data_max.keys():\n","                                if data_max[attr] < float(tmp[attr]): data_max[attr] = float(tmp[attr])\n","                            else: data_max[attr] = float(tmp[attr])\n","                    else:\n","                        countries_not_found.append(country)\n","                        tmp_countries.remove(country)\n","\n","                for attr in data_avg.keys():\n","                    X_new.at[index, f\"avg_{attr}\"] = data_avg[attr]/len(tmp_countries)\n","                    X_new.at[index, f\"min_{attr}\"] = data_min[attr]\n","                    X_new.at[index, f\"max_{attr}\"] = data_max[attr]\n","                \n","            # fill empty values in other attributes\n","            for attr in data_avg.keys():\n","                #print(f\"Avg {attr}: \", X_new[f\"avg_{attr}\"].isnull().sum())\n","                #print(f\"Min {attr}: \", X_new[f\"min_{attr}\"].isnull().sum())\n","                #print(f\"Max {attr}: \", X_new[f\"max_{attr}\"].isnull().sum())\n","                if self.mean == 'worldwide':\n","                    X_new[f\"avg_{attr}\"] = X_new[f\"avg_{attr}\"].fillna(self.country_data[attr].mean())\n","                    X_new[f\"min_{attr}\"] = X_new[f\"min_{attr}\"].fillna(self.country_data[attr].min())\n","                    X_new[f\"max_{attr}\"] = X_new[f\"max_{attr}\"].fillna(self.country_data[attr].max())\n","                    X_new[f\"main_country_{attr}\"] = X_new[f\"avg_{attr}\"].fillna(self.country_data[attr].mean())\n","                else:\n","                    X_new[f\"avg_{attr}\"] = X_new[f\"avg_{attr}\"].fillna(X_new[f'avg_{attr}'].mean())\n","                    X_new[f\"avg_{attr}\"] = X_new[f\"avg_{attr}\"].astype(np.float64)\n","                    X_new[f\"min_{attr}\"] = X_new[f\"min_{attr}\"].fillna(X_new[f'min_{attr}'].min())\n","                    X_new[f\"min_{attr}\"] = X_new[f\"min_{attr}\"].astype(np.float64)\n","                    X_new[f\"max_{attr}\"] = X_new[f\"max_{attr}\"].fillna(X_new[f'max_{attr}'].max())\n","                    X_new[f\"max_{attr}\"] = X_new[f\"max_{attr}\"].astype(np.float64)\n","                    X_new[f\"main_country_{attr}\"] = X_new[f\"avg_{attr}\"].fillna(X_new[f'avg_{attr}'].mean())\n","                    X_new[f\"main_country_{attr}\"] = X_new[f\"avg_{attr}\"].astype(np.float64)\n","\n","            if len(countries_not_found): print(\"Countries not found: \", countries_not_found) # Print not found countries\n","        \n","        elif self.transformer == 'perCity':\n","            # Drop population country\n","            self.country_data.drop(columns=[\"population\"], inplace = True)\n","\n","            # Rename density to CountryDensity\n","            self.country_data.rename(columns={\"density\": \"CountryDensity\"}, inplace = True)\n","\n","            result = []\n","\n","            for index, row in X_new.iterrows():\n","                densitySum = 0\n","                cityList = []\n","\n","                for i in range(len(row['LocationCity'])):\n","                    cityList.append(f\"{row['LocationCity'][i]} --- {row['LocationCountry'][i]}\")\n","                    densitySum += row['LocationPopulationDensity'][i]\n","                \n","                tmp_cities = list(set(cityList)) # Get unique list of city\n","                \n","                for city in tmp_cities:\n","                    id = row['LocationCity'].index(city.split(\" --- \")[0])\n","                    tempCity = row.copy()\n","\n","                    # Create new column only containing unique countries\n","                    tempCity['city'] = row['LocationCity'][id]\n","                    tempCity['country'] = row['LocationCountry'][id]\n","                    tempCity['CityPopulationDensity'] = row['LocationPopulationDensity'][id]\n","\n","                    # Calculate Enrollment Count per city\n","                    cityCount = row['LocationCity'].count(tempCity['city'])\n","                    tempCity['EnrollmentCountPerCity'] = round(cityCount * tempCity['CityPopulationDensity'] * tempCity['EnrollmentCount'] / densitySum)\n","                    \n","                    del tempCity['LocationCity']\n","                    del tempCity['LocationCountry']\n","                    result.append(tempCity)\n","\n","            df_per_city = pd.DataFrame(result)\n","            X_new = pd.merge(left=df_per_city, right=self.country_data, how='left', left_on='country', right_on='countryName')\n","            X_new.drop(columns=[\"countryName\"], inplace = True)\n","\n","        elif self.transformer == \"perCountry\":\n","            self._subset = ['LocationCountry', 'LocationCity', 'LocationFacility', 'LocationState', 'LocationZip', 'NCTId']\n","\n","            # Calculate a factor for the worldshare of a country\n","            self.country_data['worldshareFactor'] = 0.0\n","            for index, row in self.country_data.iterrows():\n","                if row['worldshare'] < 0.01 or row['worldshare'] == 0.0:\n","                    self.country_data.at[index, 'worldshareFactor'] = 0.2\n","                if row['worldshare'] < 0.05 and row['worldshare']  >= 0.01:\n","                    self.country_data.at[index, 'worldshareFactor'] = 0.4\n","                if row['worldshare'] < 0.2 and row['worldshare']  >= 0.05:\n","                    self.country_data.at[index, 'worldshareFactor'] = 0.6\n","                if row['worldshare'] < 1 and row['worldshare']  >= 0.2:\n","                    self.country_data.at[index, 'worldshareFactor'] = 0.8\n","                if row['worldshare'] >= 1:\n","                    self.country_data.at[index, 'worldshareFactor'] = 1.0\n","\n","            # Clean up data set for join\n","            self.country_data.rename(columns={\"countryName\": \"country\"}, inplace = True)\n","            dataSubset = X[self._subset]\n","            # Create new column only containing unique countries\n","            dataSubset['DistinctCountries_x'] = dataSubset['LocationCountry']\n","\n","            for index, row in dataSubset.iterrows():\n","                if row['LocationCountry']:\n","                    dataSubset.at[index, 'DistinctCountries_x'] = reduce(lambda l, x: l if x in l else l+[x], row['LocationCountry'], [])\n","            else:\n","                dataSubset.at[index, 'DistinctCountries_x'] = []\n","\n","            for index, row in dataSubset.iterrows():\n","                if isinstance(row['DistinctCountries_x'], str):\n","                    dataSubset.at[index, 'DistinctCountries_x'] = [row['DistinctCountries_x']]\n","            \n","            X_new = pd.DataFrame()\n","            X_new['facilities'] = \"\"\n","            X_new['LocationZips'] = \"\"\n","            X_new['LocationCities'] = \"\"\n","            counter = 0\n","            for index, row in dataSubset.iterrows():\n","                countries = row['DistinctCountries_x']\n","                for i in range(len(countries)):\n","                    X_new.at[counter, 'NCTId'] = row['NCTId']\n","                    X_new.at[counter, 'country'] = countries[i]\n","                    locationsCount = row['LocationCountry'].count(countries[i])\n","                    X_new.at[counter, 'locations'] = locationsCount\n","                    X_new.at[counter, 'facilities'] = row['LocationFacility'][0:locationsCount]\n","                    X_new.at[counter, 'LocationZips'] = row['LocationZip'][0:locationsCount]\n","                    X_new.at[counter, 'LocationCities'] = row['LocationCity'][0:locationsCount]\n","                    row['LocationFacility'] = row['LocationFacility'][locationsCount:]\n","                    row['LocationZip'] = row['LocationZip'][locationsCount:]\n","                    row['LocationCity'] = row['LocationCity'][locationsCount:]\n","                    counter = counter + 1\n","                    \n","            X_new = pd.merge(left=X_new, right=X, how='outer', on='NCTId')\n","                    \n","            # Clean country data\n","            X_new.loc[X_new['country'] == 'Former Yugoslavia', 'country'] = 'Serbia'\n","            X_new.loc[X_new['country'] == 'Former Serbia and Montenegro', 'country'] = 'Serbia'\n","            X_new.loc[X_new['country'] == 'Holy See (Vatican City State)', 'country'] = 'Italy'\n","            X_new.loc[X_new['country'] == 'Macedonia, The Former Yugoslav Republic of', 'country'] = 'North Macedonia'\n","            X_new.loc[X_new['country'] == 'Swaziland', 'country'] = 'Eswatini'\n","            X_new.loc[X_new['country'] == 'Netherlands Antilles', 'country'] = 'Curacao'\n","                    \n","            # Join with country data\n","            X_new = pd.merge(left=X_new, right=self.country_data, how='left', left_on='country', right_on='country')\n","            \n","            # Calculate per country Enrollment Count\n","            self._subset = self._subset + ['worldshareFactor', 'locations', 'EnrollmentCount', 'country']\n","            dataSubset = X_new[self._subset]\n","            dataSubset = dataSubset[dataSubset['country'].notna()]\n","\n","            for index, row in dataSubset.iterrows():\n","                dataSubset.at[index, 'temp'] = row['locations'] * row['worldshareFactor']\n","        \n","            dataSubset['totalTemp'] = dataSubset.groupby('NCTId', sort=False)[\"temp\"].transform('sum')\n","\n","            # Calculate enrollment distribution \n","            for index, row in dataSubset.iterrows():\n","                countryenrollment = round(((row['locations'] *  row['worldshareFactor']) /  row['totalTemp']) * row['EnrollmentCount'])\n","                if countryenrollment == 0:\n","                    countryenrollment = 1\n","                dataSubset.at[index, 'enrollmentPerCountry'] = countryenrollment\n","                dataSubset.at[index, 'enrollmentPercentage'] = ((row['locations'] *  row['worldshareFactor']) /  row['totalTemp'])* 100\n","        \n","        \n","            dataSubset['EnrollmentCheck'] = dataSubset.groupby('NCTId', sort=False)[\"enrollmentPerCountry\"].transform('sum')\n","            dataSubset['HighestEnrollment'] = dataSubset.groupby('NCTId', sort=False)[\"enrollmentPerCountry\"].transform('max')\n","\n","            for index, row in dataSubset.iterrows():\n","                if row['EnrollmentCheck'] > row['EnrollmentCount'] and row['HighestEnrollment'] == row['enrollmentPerCountry']:\n","                    difference = row['EnrollmentCheck'] - row['EnrollmentCount']\n","                    dataSubset.at[index, 'enrollmentPerCountry'] = row['enrollmentPerCountry'] - difference\n","\n","            dataSubset.drop(columns = ['worldshareFactor', 'locations','temp', 'totalTemp', 'EnrollmentCheck', 'HighestEnrollment'], inplace = True)\n","\n","            # Join with rest of the data\n","            X_new = pd.merge(left=X_new, right=dataSubset, how='left', on=['NCTId', 'country'])\n","\n","        return X_new"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0eS6xASw4KnJ","colab_type":"code","colab":{}},"source":["# test\n","transformer = LocationDataTransformer(df_dbcountry, transformer='totalCombine', strategy='weighted', mean='worldwide')\n","test = transformer.fit_transform(df_raw[['LocationCountry', 'LocationCity', 'LocationFacility', 'LocationState', 'LocationZip', 'NCTId', 'LocationPopulationDensity']]).tail(10)\n","test[['main_country_population', 'main_country_lifeExpectancy', 'main_country_GDP', 'main_country_density', 'main_country_fertilityRate', 'main_country_medianAge', 'main_country_migrantsNet', 'main_country_sizeInKm2', 'main_country_urbanPopulation', 'main_country_unemploymentRate', 'main_country_hospitalBed', 'main_country_healthExpenditure']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mhI9SKF77fV-","colab_type":"text"},"source":["##Transformers for free text values"]},{"cell_type":"code","metadata":{"id":"7h7ya6Eqx0oC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1599213167411,"user_tz":-120,"elapsed":815,"user":{"displayName":"陳維儀","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjADfM2DPAarNhBjfMwzTdbc4NzhB0rPW1yeU7ETNM=s64","userId":"06691591176760802855"}},"outputId":"280a9f36-6035-482a-fea6-13e480bbe9ae"},"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize \n","\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"7LpFyvjb7r58","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1599213167608,"user_tz":-120,"elapsed":765,"user":{"displayName":"陳維儀","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjADfM2DPAarNhBjfMwzTdbc4NzhB0rPW1yeU7ETNM=s64","userId":"06691591176760802855"}},"outputId":"ee68493f-e3f3-4ba5-ddaa-b5a4960149c7"},"source":["from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize \n","import nltk\n","nltk.download('stopwords')\n","import re, string, timeit\n","\n","#Custom transformer that deals with textual features by extracting keywords\n","class TextualFeatureTransformer( TransformerMixin, BaseEstimator ):\n","    \"\"\"\n","    Class for performing the feature transformation for textual features. \n","    After performing several NLP preprocessing steps, the n most frequently mentioned keywords are identified \n","    and a one-hot coding is performed for them. \n","\n","    Parameters\n","    --------\n","\n","        remove : array of strings\n","            Array of strings defining the NLP preprocessing steps that should be carried out \n","            before the keyword extraction can take place.\n","            You can choose from the following steps: \n","                - 'upper'\n","                - 'numbers'\n","                - 'special'\n","                \n","            By default, all of them are applied\n","        n_keywords : integer\n","            Integer defining the number of keywords that should be extracted\n","        stop_words : list or set\n","            List of stopwords that are to be eliminated prior to the keyword extraction.\n","        \n","\n","    Attributes\n","    --------\n","    \n","    features : dict\n","        Dictionary of the feature subset that is to be transformed\n","    top_keywords: dict\n","        Dictionary to store the top n identified keywords of each feature\n","    df_transformedFeatures : dataframe\n","        Dataframe of the transformed features to apply one hot encoding on with regard to the preprocessed entries\n","     \"\"\"\n","    \n","    #Class constructor method that takes in a list of values as its argument\n","    def __init__(self, \n","                 remove = ['upper', 'numbers', 'special'],\n","                 n_keywords = 20, \n","                 stop_words = set(stopwords.words('english'))):\n","        self._remove = remove\n","        self._n_keywords = n_keywords\n","        self._stop_words = stop_words\n","        self.features = {}\n","        self.top_keywords = {}\n","        self.df_transformedFeatures = pd.DataFrame()\n","\n","    #Helper function to remove numbers \n","    def remove_numbers(self, list):\n","        \"\"\" Removes all numbers in the list \"\"\"\n","        pattern = '[0-9]'\n","        list = [re.sub(pattern, '', i) for i in list] \n","        return list\n","\n","    #Helper function to remove special characters\n","    def remove_special(self, list): \n","        \"\"\" Removes all special characters in the list \"\"\"\n","        pattern = '[^A-Za-z0-9]+'\n","        list = [re.sub(pattern, ' ', i) for i in list] \n","        return list\n","    \n","    #Helper function to remove uppercase letter and replace with lower case\n","    def remove_upper(self, obj):\n","        \"\"\" Converts all letters to lower case in the list \"\"\"\n","        obj = list(map(str.lower, obj))\n","        return obj\n","    \n","    #Helper function to create one hot encoding\n","    def one_hot (self, row, entry, attr):\n","        if str(entry) in row[attr]:\n","            return 1\n","        else:\n","            return 0\n","        \n","    def fit( self, X, y = None ):\n","        # Apply NLP preprocessing\n","        self.features = X.to_dict()\n","        for attr in X.columns:\n","            if isinstance(X[attr][0], list):    \n","                for spec in self._remove:\n","                    for k, v in self.features[attr].items():\n","                        exec(\"self.features['{}'][{}] = self.remove_{}({})\".format(attr, k, spec, v ) )\n","            else:\n","                for spec in self._remove:\n","                    for k, v in self.features[attr].items():\n","                        if self.features[attr][k]:\n","                            if spec == 'upper':\n","                                self.features[attr][k] = v.lower()\n","                            if spec == 'numbers':\n","                                self.features[attr][k] = ''.join([i for i in self.features[attr][k] if not i.isdigit()])\n","                            if spec == 'special':\n","                                exclude = set(string.punctuation)\n","                                self.features[attr][k] = ''.join(ch for ch in self.features[attr][k] if ch not in exclude)\n","            \n","            # Convert list of strings or strings into list of tokens consisting of only one word\n","            for k, v in self.features[attr].items():\n","                tokens = []\n","                if isinstance(v, list):\n","                    for entry in v:\n","                        partial = entry.split()\n","                        tokens = tokens + partial\n","                else:\n","                    tokens = str(v).split(' ')\n","                tokens = [w for w in tokens if not w in self._stop_words]\n","                self.features[attr][k] = tokens\n","        \n","        # extract keywords\n","        self.df_transformedFeatures = pd.DataFrame.from_dict(self.features)\n","        for attr in self.df_transformedFeatures.columns:\n","            self.top_keywords[attr] = self.df_transformedFeatures[attr].explode().value_counts()\n","            self.top_keywords[attr].drop(['mg', 'g', '&', 'kg', 'b', None, 'None', ' ', ''], \n","                                         inplace = True, \n","                                         errors='ignore')\n","            self.top_keywords[attr] = self.top_keywords[attr].nlargest(20)\n","        #print(self.top_keywords)\n","        return self\n","\n","    def transform(self, X , y = None ):\n","        X_new = self.df_transformedFeatures.copy()\n","        # Apply one hot according to keywords                       \n","        for attr in X.columns:\n","            for entry in self.top_keywords[attr].index:\n","                columnName = attr + '_' + str(entry)\n","                X_new[columnName] = X_new.apply (lambda row: self.one_hot(row, entry, attr), axis=1)\n","\n","        X_new.drop(columns = attr)\n","        return X_new"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pIyXB6vb79W5","colab_type":"code","colab":{}},"source":["# # test\n","textual_features = ['InterventionName', 'OrgFullName', 'LeadSponsorName', 'CollaboratorName', 'EventsTimeFrame', 'FlowDropWithdrawType', \n","                    'FlowGroupDescription', 'FlowGroupTitle', 'FlowMilestoneType', 'FlowPeriodTitle', 'FlowRecruitmentDetails', 'ArmGroupDescription', \n","                    'ArmGroupInterventionName', 'ArmGroupLabel', 'BaselineCategoryTitle', 'BaselineClassTitle', 'BaselineGroupDescription', 'BaselineGroupTitle', \n","                    'BaselineMeasureTitle', 'BaselineMeasureUnitOfMeasure']\n","\n","# df_test = df_raw[textual_features].copy()\n","# # display(df_test)\n","# # display(df_test.dtypes)\n","\n","# instance = TextualFeatureTransformer()\n","# new = instance.transform(df_test)\n","# new.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L-K0exlE8MO7","colab_type":"code","colab":{}},"source":["# pipeline\n","#textual_features = ['InterventionName', 'OrgFullName']\n","\n","textual_pipeline = Pipeline( steps = [( 'text_transformer', TextualFeatureTransformer())])\n","\n","textual_pipeline.fit_transform(df_raw[textual_features]).head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"siXuV5RN1C8J","colab_type":"text"},"source":["##Regional Age Structure"]},{"cell_type":"code","metadata":{"id":"eNYp8rAQ1vhz","colab_type":"code","colab":{}},"source":["class RegionalAgeTransformer( BaseEstimator, TransformerMixin ):\n","    '''\n","    Transformer that matches the Location and required Age with a data set that contains absolute population numbers for the age groups \"Youth\", \"Working\" and \"Elderly\" in different regions.\n","    Required Columns: MinimumAge, MaximumAge, LocationCity, LocationState\n","    Output Column: AvgLocalAge\n","    '''\n","\n","    def __init__( self, countrylist, df_regional, debug=False):\n","        self._debug = debug\n","        self._countrylist = countrylist\n","        self._df_regional = df_regional\n","\n","    def fit( self, X, y = None ):\n","        return self \n","    \n","    def transform(self, X , y = None ):\n","\n","        X_new = pd.DataFrame(X)\n","        \n","        #Fill empty values with minimal and maximal age\n","        X_new[\"MinimumAge\"].loc[X_new[\"MinimumAge\"].isna()==True]=\"0 Years\"\n","        X_new[\"MaximumAge\"].loc[X_new[\"MaximumAge\"].isna()==True]=\"100 Years\"\n","        \n","        #Define Conversion Factors\n","        conversion_factors= {\n","                                'Years': 1,\n","                                'Year': 1,\n","                                'Months': 1/12,\n","                                'Month': 1/12,\n","                                'Weeks': 1/52,\n","                                'Week': 1/52,\n","                                'Days': 1/365 ,\n","                                'Day': 1/365,\n","                                'Hours': 1/8760,\n","                                'Hour': 1/8760\n","                                }\n","        \n","        #Convert time string to float: Minimum Age\n","        for i in range(len(X_new[\"MinimumAge\"])):\n","                    for string, factor in conversion_factors.items():   \n","                        if string in (X_new[\"MinimumAge\"].iloc[i]):\n","                            X_new[\"MinimumAge\"].iloc[i] = (X_new[\"MinimumAge\"].iloc[i]).replace(string, '')\n","                            X_new[\"MinimumAge\"].iloc[i] = (X_new[\"MinimumAge\"].iloc[i]).strip()\n","                            X_new[\"MinimumAge\"].iloc[i] = float(X_new[\"MinimumAge\"].iloc[i]) * factor\n","                            X_new[\"MinimumAge\"].iloc[i] = str(X_new[\"MinimumAge\"].iloc[i] )\n","        X_new[\"MinimumAge\"] = X_new[\"MinimumAge\"].astype(float)\n","\n","        #Convert time string to float: Maximum Age\n","        for i in range(len(X_new[\"MaximumAge\"])):\n","                    for string, factor in conversion_factors.items():   \n","                        if string in (X_new[\"MaximumAge\"].iloc[i]):\n","                            X_new[\"MaximumAge\"].iloc[i] = (X_new[\"MaximumAge\"].iloc[i]).replace(string, '')\n","                            X_new[\"MaximumAge\"].iloc[i] = (X_new[\"MaximumAge\"].iloc[i]).strip()\n","                            X_new[\"MaximumAge\"].iloc[i] = float(X_new[\"MaximumAge\"].iloc[i]) * factor\n","                            X_new[\"MaximumAge\"].iloc[i] = str(X_new[\"MaximumAge\"].iloc[i] )          \n","        X_new[\"MaximumAge\"] = X_new[\"MaximumAge\"].astype(float)\n","        \n","        #Create Age column\n","        X_new[\"Age\"]=np.empty((len(X_new), 0)).tolist()\n","        for i, row in X_new.iterrows():\n","            if row[\"MinimumAge\"] <= 16:\n","                row[\"Age\"].append(\"Youth\")\n","            if row[\"MinimumAge\"] < 65 and row[\"MaximumAge\"] > 16:\n","                row[\"Age\"].append(\"Working\")\n","            if row[\"MaximumAge\"] >= 65:\n","                row[\"Age\"].append(\"Elderly\")  \n","\n","        #Add additional local attributes to dataframe\n","        X_new[\"LocalAge\"]=np.empty((len(X_new), 0)).tolist()\n","        X_new[\"AvgLocalAge\"]=0\n","\n","        #Check wheter city is in regional data and match with Age, if not match with State\n","        for i, row in X_new.iterrows():\n","            for j,city in enumerate(row[\"LocationCity\"]):\n","                if city in self._countrylist.unique():\n","                    if \"Youth\" in row[\"Age\"]:\n","                        row[\"LocalAge\"].append(int(self._df_regional[\"Youth\"].loc[self._df_regional[\"Area\"]==city]))\n","                    if \"Working\" in row[\"Age\"]:\n","                        row[\"LocalAge\"].append(int(self._df_regional[\"Working\"].loc[self._df_regional[\"Area\"]==city]))\n","                    if \"Elderly\" in row[\"Age\"]:\n","                        row[\"LocalAge\"].append(int(self._df_regional[\"Elderly\"].loc[self._df_regional[\"Area\"]==city]))\n","                else:\n","                    if len(row[\"LocationCity\"])==len(row[\"LocationState\"]):\n","                        if row[\"LocationState\"][j] in self._countrylist.unique():\n","                            if \"Youth\" in row[\"Age\"]:\n","                                row[\"LocalAge\"].append(int(self._df_regional[\"Youth\"].loc[self._df_regional[\"Area\"]==row[\"LocationState\"][j]]))\n","                            if \"Working\" in row[\"Age\"]:\n","                                row[\"LocalAge\"].append(int(self._df_regional[\"Working\"].loc[self._df_regional[\"Area\"]==row[\"LocationState\"][j]]))\n","                            if \"Elderly\" in row[\"Age\"]:\n","                                row[\"LocalAge\"].append(int(self._df_regional[\"Elderly\"].loc[self._df_regional[\"Area\"]==row[\"LocationState\"][j]]))\n","            \n","            #Take the sum of all matching population\n","            if len(X_new[\"LocalAge\"][i])>0:\n","                X_new[\"AvgLocalAge\"][i]=np.sum(X_new[\"LocalAge\"][i])    # TODO throws a warning\n","        \n","        #Drop all unneccessary features\n","        todropfeatures = [\"LocationCity\", \"LocationState\", \"MinimumAge\", \"MaximumAge\", \"Age\", \"LocalAge\"]\n","        X_new=X_new.drop(todropfeatures, axis=1)\n","\n","        return X_new\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8-biJ_W21X9T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":486},"executionInfo":{"status":"ok","timestamp":1599207489899,"user_tz":-120,"elapsed":820,"user":{"displayName":"Stefan Sousa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnGBw3kCtWt-XAq5nTYjp1DY0_T5bIfSzFTajnfg=s64","userId":"08251656603606889983"}},"outputId":"9bb7a661-f936-4ccc-c094-bf5a418da987"},"source":["#Test\n","regional_age_Features = ['LocationCity', 'LocationState', \"MinimumAge\", \"MaximumAge\"]\n","\n","df_test = df_raw[regional_age_Features].copy()[:100]\n","display(df_test)\n","display(df_test.dtypes)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>LocationCity</th>\n","      <th>LocationState</th>\n","      <th>MinimumAge</th>\n","      <th>MaximumAge</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[Irvine, La Jolla, Los Angeles, Los Angeles, S...</td>\n","      <td>[California, California, California, Californi...</td>\n","      <td>13 Years</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[Baltimore]</td>\n","      <td>[Maryland]</td>\n","      <td>None</td>\n","      <td>6 Years</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[Birmingham, San Diego, Jacksonville, Tampa, A...</td>\n","      <td>[Alabama, California, Florida, Florida, Georgi...</td>\n","      <td>60 Years</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>[New York]</td>\n","      <td>[New York]</td>\n","      <td>18 Years</td>\n","      <td>60 Years</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>[New York, New York]</td>\n","      <td>[New York, New York]</td>\n","      <td>18 Years</td>\n","      <td>45 Years</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>113</th>\n","      <td>[Bethesda]</td>\n","      <td>[Maryland]</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>114</th>\n","      <td>[Bethesda]</td>\n","      <td>[Maryland]</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>115</th>\n","      <td>[Bethesda]</td>\n","      <td>[Maryland]</td>\n","      <td>18 Years</td>\n","      <td>80 Years</td>\n","    </tr>\n","    <tr>\n","      <th>116</th>\n","      <td>[Bethesda]</td>\n","      <td>[Maryland]</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>117</th>\n","      <td>[Bethesda]</td>\n","      <td>[Maryland]</td>\n","      <td>18 Years</td>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 4 columns</p>\n","</div>"],"text/plain":["                                          LocationCity  ... MaximumAge\n","0    [Irvine, La Jolla, Los Angeles, Los Angeles, S...  ...       None\n","1                                          [Baltimore]  ...    6 Years\n","2    [Birmingham, San Diego, Jacksonville, Tampa, A...  ...       None\n","15                                          [New York]  ...   60 Years\n","16                                [New York, New York]  ...   45 Years\n","..                                                 ...  ...        ...\n","113                                         [Bethesda]  ...       None\n","114                                         [Bethesda]  ...       None\n","115                                         [Bethesda]  ...   80 Years\n","116                                         [Bethesda]  ...       None\n","117                                         [Bethesda]  ...       None\n","\n","[100 rows x 4 columns]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["LocationCity     object\n","LocationState    object\n","MinimumAge       object\n","MaximumAge       object\n","dtype: object"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ZLmwC2eT1g0q","colab_type":"code","colab":{}},"source":["instance = RegionalAgeTransformer()\n","df_new = instance.transform(df_test)\n","df_new.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W7ZcsXpKvjjx","colab_type":"text"},"source":["\n","##Transformer to Assign Worldrank to Facility"]},{"cell_type":"code","metadata":{"id":"s98fBcetK3Sj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1599209841973,"user_tz":-120,"elapsed":908,"user":{"displayName":"Carolin Holtermann","photoUrl":"","userId":"03090939707808142119"}},"outputId":"78c6acfb-fed6-46c7-bd6d-91c8f36c77a1"},"source":["nltk.download('stopwords')\n","stop = stopwords.words('english')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RiydlO7Yvimg","colab_type":"code","colab":{}},"source":["class WorldRankTransformer( BaseEstimator, TransformerMixin ):\n","    \"\"\"\n","    Class that matches the LocationFacility with a data set that ranks Universitites and Hospitals \n","    by their research activities.\n","\n","    Parameters\n","    --------\n","\n","        worldrankData : dataframe\n","            A dataframe containing information about the worldrank of the facilities.\n","            \n","        stopwords: set\n","            Set of stopwords that should be excluded when preprocessing the facility names.\n","        \n","        debug: boolean\n","            A boolean value defining whether or not the transformer execution should be debugged.\n","\n","    Notes\n","    --------\n","        - Required columns = LocationFacility\n","        - Required information = Worldrank data set\n","        - Output columns = AvgFacilityRank for all Facilities involved\n","        \n","        \"\"\"\n","\n","\n","    def __init__( self, \n","                 worldrankData = pd.DataFrame(),\n","                 stopwords = set(stopwords.words('english')),\n","                 debug=False):\n","        self._worldrankData = worldrankData.copy()\n","        self._debug = debug\n","        self._stopwords = stopwords\n","\n","    def fit( self, X, y = None ):\n","        return self \n","    \n","    #Define Preprocessing Function\n","    def clean_string(self, text):\n","        text = ''.join([word for word in text if word not in string.punctuation])\n","        text = text.lower()\n","        text = ' '.join([word for word in text.split() if word not in self._stopwords])\n","        return text\n","    \n","    def transform(self, X , y = None ):\n","        #Copy df\n","        X_new = pd.DataFrame(X)\n","        \n","        #Create emtpy list columns\n","        X_new[\"FacilityWorldRank\"]=np.empty((len(X_new), 0)).tolist()\n","        X_new[\"floatFacilityWorldRank\"]=np.empty((len(X_new), 0)).tolist()\n","        X_new[\"CleanFacility\"]=np.empty((len(X_new), 0)).tolist()\n","        X_new[\"AvgFacilityRank\"]=0\n","        \n","\n","        for index, row in X_new.iterrows():\n","            rowFacilities = []\n","\n","            #Preprocess data\n","            for fac in row[\"LocationFacility\"]:\n","                row[\"CleanFacility\"].append(self.clean_string(fac))\n","\n","            #Match facilities with worldrank data set\n","            for element in row['CleanFacility']:\n","                for fac in self._worldrankData.index:\n","                    if element == fac:\n","                        rowFacilities.append(self._worldrankData.loc[fac]['WorldRank'])\n","            X_new.at[index, 'FacilityWorldRank'] = rowFacilities\n","            \n","        # Calculate the mean world rank of all facilities a worldrank could be identified for\n","        for i, row in X_new.iterrows():\n","            if row[\"FacilityWorldRank\"]:\n","                X_new.at[i, \"AvgFacilityRank\"] = sum(row[\"FacilityWorldRank\"]) / len(row[\"FacilityWorldRank\"])\n","            \n","        \n","        # Duplicated 'LocationFacility'\n","        X_new.drop(columns=['LocationFacility', 'floatFacilityWorldRank', 'CleanFacility', 'FacilityWorldRank'], inplace = True)\n","\n","        return X_new"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RayIblMmvzoy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1599210126820,"user_tz":-120,"elapsed":12907,"user":{"displayName":"Carolin Holtermann","photoUrl":"","userId":"03090939707808142119"}},"outputId":"b9975865-6fc4-4a0a-9de8-50739abccf2c"},"source":["#Test\n","hospital_features = [\"LocationFacility\"]\n","\n","df_test = df_raw[hospital_features].copy()\n","display(df_test)\n","display(df_test.dtypes)\n","\n","nltk.download('stopwords')\n","stop = stopwords.words('english')\n","instance = WorldRankTransformer(worldrankData = df_worldrank, stopwords = stop)\n","df_new = instance.transform(df_test)\n","df_new.head(50)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>LocationFacility</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[Department of Ophthalmology, University of Ca...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[Wilmer Eye Institute]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[University of Alabama, Birmingham, University...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>[Research Foundation for Mental Hygiene]</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>[Columbia University, New York State Psychiatr...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>[MBCCOP - Gulf Coast, CCOP - Greater Phoenix, ...</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>[Toronto Sunnybrook Regional Cancer Centre]</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>[Children's Oncology Group]</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>[University of Alabama at Birmingham Comprehen...</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>[Memorial Sloan-Kettering Cancer Center]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>982 rows × 1 columns</p>\n","</div>"],"text/plain":["                                      LocationFacility\n","0    [Department of Ophthalmology, University of Ca...\n","1                               [Wilmer Eye Institute]\n","2    [University of Alabama, Birmingham, University...\n","15            [Research Foundation for Mental Hygiene]\n","16   [Columbia University, New York State Psychiatr...\n","..                                                 ...\n","995  [MBCCOP - Gulf Coast, CCOP - Greater Phoenix, ...\n","996        [Toronto Sunnybrook Regional Cancer Centre]\n","997                        [Children's Oncology Group]\n","998  [University of Alabama at Birmingham Comprehen...\n","999           [Memorial Sloan-Kettering Cancer Center]\n","\n","[982 rows x 1 columns]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["LocationFacility    object\n","dtype: object"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>AvgFacilityRank</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>945</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>168</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>93</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>825</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>536</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>707</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>5817</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>161</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>59</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>60</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>61</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>62</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>63</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>66</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>67</th>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    AvgFacilityRank\n","0                26\n","1                 0\n","2               945\n","15                0\n","16                0\n","19                0\n","21                0\n","24                0\n","26                0\n","27                0\n","28                0\n","29                0\n","30                0\n","31                0\n","32              168\n","33                0\n","34                7\n","35                0\n","36                0\n","37               93\n","38                0\n","39              825\n","40              536\n","41                0\n","42                0\n","43              707\n","44                7\n","45                0\n","46                0\n","47             5817\n","48                0\n","49                0\n","50              161\n","51                0\n","52                0\n","53                0\n","54                0\n","55                0\n","56                0\n","57                0\n","58                0\n","59                0\n","60                0\n","61                0\n","62                0\n","63                0\n","64                0\n","65                0\n","66                0\n","67                0"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"U6tYo_K9gjff","colab_type":"text"},"source":["##Transformer for MeshID"]},{"cell_type":"code","metadata":{"id":"jJbeHKe1ghkA","colab_type":"code","colab":{}},"source":["class MeshIDTransformer( TransformerMixin, BaseEstimator ):\n","    def __init__( self, debug=False):\n","        self._debug = debug\n","\n","    def fit( self, X, y = None ):\n","        return self \n","    \n","    def transform(self, X , y = None ):\n","        #X_new = X.copy()\n","        X_new = pd.DataFrame(X)\n","        \n","        X_new[\"MeshID1\"]=0\n","        X_new[\"MeshID2\"]=0\n","        X_new[\"MeshID3\"]=0\n","\n","        for i, row in X_new.iterrows():\n","            if len(X_new[\"ConditionMeshId\"][i])==1:\n","                X_new.at[i, \"MeshID1\"] = int(X_new[\"ConditionMeshId\"][i][0].strip(\"D\"))\n","            if len(X_new[\"ConditionMeshId\"][i])==2:\n","                X_new.at[i, \"MeshID1\"] = int(X_new[\"ConditionMeshId\"][i][0].strip(\"D\"))\n","                X_new.at[i, \"MeshID2\"] = int(X_new[\"ConditionMeshId\"][i][1].strip(\"D\"))\n","            if len(X_new[\"ConditionMeshId\"][i])==3:\n","                X_new.at[i, \"MeshID1\"] = int(X_new[\"ConditionMeshId\"][i][0].strip(\"D\"))\n","                X_new.at[i, \"MeshID2\"] = int(X_new[\"ConditionMeshId\"][i][1].strip(\"D\"))\n","                X_new.at[i, \"MeshID3\"] = int(X_new[\"ConditionMeshId\"][i][2].strip(\"D\"))\n","        \n","        X_new = X_new.drop(\"ConditionMeshId\", axis=1)\n","        return X_new"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hnGvCFqwg5PR","colab_type":"code","colab":{}},"source":["#Test\n","special_field_features_meshID = ['ConditionMeshId']\n","\n","df_test = df_raw[special_field_features_meshID].copy()[:5]\n","display(df_test)\n","display(df_test.dtypes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V4jh2GMdg-Bo","colab_type":"code","colab":{}},"source":["instance = MeshIDTransformer()\n","df_new = instance.transform(df_test)\n","df_new.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8rR_EMw6YTV3","colab_type":"text"},"source":["##Transformers for feature selection"]},{"cell_type":"markdown","metadata":{"id":"HLKDGH-zYY1-","colab_type":"text"},"source":["### Feature selection by forward and backward selection"]},{"cell_type":"code","metadata":{"id":"TfqW7h41Yg10","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1599213184270,"user_tz":-120,"elapsed":734,"user":{"displayName":"陳維儀","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjADfM2DPAarNhBjfMwzTdbc4NzhB0rPW1yeU7ETNM=s64","userId":"06691591176760802855"}},"outputId":"3dd344de-2a0e-4cbe-a795-5b313987814d"},"source":["#Custom transformer that selects features that boost performance most  \n","class FeatureSelectorTransformerModels( BaseEstimator, TransformerMixin ):\n","    \"\"\"\n","    Transformer for performing feature selection for machine learning or data preprocessing based on model performance\n","        \n","    Parameters\n","    --------\n","        strategy : String, default = forward\n","            A string value defining the strategy of the feature selection = forward or backward\n","        model_type : String, default = linear\n","            A string value defining the model that should be used (i.e. linear, logistic, ... )\n","        elimination_criteria : String, default = aic\n","            A string value defining the elimination criteria based on which the feature selection is done.\n","            Examples:\n","            - aic = Akaike information criterion\n","            - bic = Bayesian information criterion\n","            - adjr2 = Adjusted R squared\n","            - r2 = R squared\n","        varchar_process : String, default = dummy_dropfirst\n","            Possible values:\n","            - drop\n","            - dummy\n","            - dummy_dropfirst\n","        targetColumn : Series\n","            A Series defining the target column\n","    \"\"\"\n","    #Class constructor method that takes in a list of values as its argument\n","    def __init__(self, \n","                 strategy = \"backward\", \n","                 model_type = \"linear\",\n","                 elimination_criteria = \"aic\",\n","                 varchar_process = \"dummy_dropfirst\",\n","                 sl = 0.05,\n","                 targetColumn = pd.Series()):\n","        self._strategy = strategy\n","        self._model_type = model_type\n","        self._elimination_criteria = elimination_criteria\n","        self._sl = sl\n","        self._iterations_log = \"\"\n","        self._last_eliminated = \"\"\n","        self._varchar_process = varchar_process\n","        self._targetColumn = targetColumn\n"," \n","        \n","    def fit(self, X, y = None): \n","        \n","        return self\n","    \n","    def regressor(self, y, X):\n","        if self._model_type ==\"linear\":\n","            regressor = sm.OLS(y, X).fit()\n","        elif self._model_type == \"logistic\":\n","            regressor = sm.Logit(y, X).fit()\n","        else:\n","            #print(\"\\nWrong Model Type : \"+ self._model_type +\"\\nLinear model type is seleted.\")\n","            self._model_type = \"linear\"\n","            regressor = sm.OLS(y, X).fit()\n","        return regressor \n","    \n","    def varcharProcessing(self, X, varchar_process = \"dummy_dropfirst\"):   \n","        dtypes = X.dtypes\n","        if varchar_process == \"drop\":   \n","            X = X.drop(columns = dtypes[dtypes == np.object].index.tolist())\n","            #print(\"Character Variables (Dropped):\", dtypes[dtypes == np.object].index.tolist())\n","        elif varchar_process == \"dummy\":\n","            X = pd.get_dummies(X,drop_first=False)\n","            #print(\"Character Variables (Dummies Generated):\", dtypes[dtypes == np.object].index.tolist())\n","        elif varchar_process == \"dummy_dropfirst\":\n","            X = pd.get_dummies(X,drop_first=True)\n","            #print(\"Character Variables (Dummies Generated, First Dummies Dropped):\", dtypes[dtypes == np.object].index.tolist())\n","        else: \n","            X = pd.get_dummies(X,drop_first=True)\n","            #print(\"Character Variables (Dummies Generated, First Dummies Dropped):\", dtypes[dtypes == np.object].index.tolist())\n","\n","        X[\"intercept\"] = 1\n","        cols = X.columns.tolist()\n","        cols = cols[-1:] + cols[:-1]\n","        X = X[cols]\n","\n","        return X\n","\n","    \n","    def transform(self, X):\n","        \n","        X = self.varcharProcessing(X, varchar_process = self._varchar_process)\n","        y = self._targetColumn\n","        #X.drop(columns = [self._targetColumn], inplace = True)\n","        self._cols = X.columns.tolist()\n","        X_new = X.copy()\n","        \n","        if self._strategy == \"backward\":   \n","            for i in range(X_new.shape[1]):\n","                if i != 0 :          \n","                    if self._elimination_criteria == \"aic\":\n","                        criteria = model.aic\n","                        new_model = self.regressor(y = y, X = X_new)\n","                        new_criteria = new_model.aic\n","                        if criteria < new_criteria:\n","                            #print(\"Regained : \", self._last_eliminated)\n","                            self._iterations_log += \"\\n\"+str(new_model.summary())+\"\\nAIC: \"+ str(new_model.aic) + \"\\nBIC: \"+ str(new_model.bic)+\"\\n\"\n","                            self._iterations_log += str(\"\\n\\nRegained : \"+ self._last_eliminated + \"\\n\\n\")\n","                            break  \n","                    elif self._elimination_criteria == \"bic\":\n","                        criteria = model.bic\n","                        new_model = self.regressor(y = y, X = X_new)\n","                        new_criteria = new_model.bic\n","                        if criteria < new_criteria:\n","                            #print(\"Regained : \", self._last_eliminated)\n","                            self._iterations_log += \"\\n\"+str(new_model.summary())+\"\\nAIC: \"+ str(new_model.aic) + \"\\nBIC: \"+ str(new_model.bic)+\"\\n\"\n","                            self._iterations_log += str(\"\\n\\nRegained : \"+ self._last_eliminated + \"\\n\\n\")\n","                            break  \n","                    elif self._elimination_criteria == \"adjr2\" and model_type ==\"linear\":\n","                        criteria = model.rsquared_adj\n","                        new_model = self.regressor(y = y,X = X_new)\n","                        new_criteria = new_model.rsquared_adj\n","                        if criteria > new_criteria:\n","                            #print(\"Regained : \", self._last_eliminated)\n","                            self._iterations_log += \"\\n\" + str(new_model.summary()) + \"\\nAIC: \" + str(new_model.aic) + \"\\nBIC: \"+ str(new_model.bic)+\"\\n\"\n","                            self._iterations_log += str(\"\\n\\nRegained : \"+ self._last_eliminated + \"\\n\\n\")\n","                            break  \n","                    elif self._elimination_criteria == \"r2\" and model_type ==\"linear\":\n","                        criteria = model.rsquared\n","                        new_model = self.regressor(y,X_new)\n","                        new_criteria = new_model.rsquared\n","                        if criteria > new_criteria:\n","                            #print(\"Regained : \", self._last_eliminated)\n","                            self._iterations_log += \"\\n\"+str(new_model.summary())+\"\\nAIC: \"+ str(new_model.aic) + \"\\nBIC: \"+ str(new_model.bic)+\"\\n\"\n","                            self._iterations_log += str(\"\\n\\nRegained : \"+ self._last_eliminated + \"\\n\\n\")\n","                            break   \n","                    else: \n","                        new_model = self.regressor(y = y,X = X_new)\n","                    model = new_model\n","                    self._iterations_log += \"\\n\"+str(model.summary())+\"\\nAIC: \"+ str(model.aic) + \"\\nBIC: \"+ str(model.bic)+\"\\n\"\n","                else:\n","                    model = self.regressor(y = y, X = X_new)\n","                    self._iterations_log += \"\\n\"+str(model.summary())+\"\\nAIC: \"+ str(model.aic) + \"\\nBIC: \"+ str(model.bic)+\"\\n\"\n","                maxPval = max(model.pvalues)\n","                selected_cols = X_new.columns.tolist()\n","                if maxPval > self._sl:\n","                    for j in selected_cols:\n","                        if (model.pvalues[j] == maxPval):\n","                            #print(\"Eliminated :\" ,j)\n","                            self._iterations_log += str(\"\\n\\nEliminated : \"+ j + \"\\n\\n\")\n","\n","                            del X_new[j]\n","                            self._last_eliminated = j\n","                else:\n","                    break\n","            #print(str(model.summary())+\"\\nAIC: \"+ str(model.aic) + \"\\nBIC: \"+ str(model.bic))\n","            #print(\"Final Variables:\", selected_cols)\n","            self._iterations_log += \"\\n\"+str(model.summary())+\"\\nAIC: \"+ str(model.aic) + \"\\nBIC: \"+ str(model.bic)+\"\\n\"\n","        \n","\n","        elif self._strategy == \"forward\":\n","            selected_cols = [\"intercept\"]\n","            other_cols = self._cols.copy()\n","            other_cols.remove(\"intercept\")\n","            \n","            for i in range(X_new.shape[1]):\n","                pvals = pd.DataFrame(columns = [\"Cols\",\"Pval\"])\n","                for j in other_cols:\n","                    model = self.regressor(y, X_new[selected_cols+[j]])\n","                    pvals = pvals.append(pd.DataFrame([[j, model.pvalues[j]]],columns = [\"Cols\",\"Pval\"]),ignore_index=True)\n","                pvals = pvals.sort_values(by = [\"Pval\"]).reset_index(drop=True)\n","                pvals = pvals[pvals.Pval<=self._sl]\n","                if pvals.shape[0] > 0:\n","\n","                    model = self.regressor(y, X_new[selected_cols+[pvals[\"Cols\"][0]]])\n","                    self._iterations_log += str(\"\\nEntered : \"+pvals[\"Cols\"][0] + \"\\n\")    \n","                    self._iterations_log += \"\\n\\n\"+str(model.summary())+\"\\nAIC: \"+ str(model.aic) + \"\\nBIC: \"+ str(model.bic)+\"\\n\\n\"\n","\n","\n","                    if  self._elimination_criteria == \"aic\":\n","                        new_criteria = model.aic\n","                        if new_criteria < criteria:\n","                            #print(\"Entered :\", pvals[\"Cols\"][0], \"\\tAIC :\", model.aic)\n","                            selected_cols.append(pvals[\"Cols\"][0])\n","                            other_cols.remove(pvals[\"Cols\"][0])\n","                            criteria = new_criteria\n","                        else:\n","                            #print(\"break : Criteria\")\n","                            break\n","                    elif  self._elimination_criteria == \"bic\":\n","                        new_criteria = model.bic\n","                        if new_criteria < criteria:\n","                            #print(\"Entered :\", pvals[\"Cols\"][0], \"\\tBIC :\", model.bic)\n","                            selected_cols.append(pvals[\"Cols\"][0])\n","                            other_cols.remove(pvals[\"Cols\"][0])\n","                            criteria = new_criteria\n","                        else:\n","                            #print(\"break : Criteria\")\n","                            break        \n","                    elif  self._elimination_criteria == \"r2\" and model_type ==\"linear\":\n","                        new_criteria = model.rsquared\n","                        if new_criteria > criteria:\n","                            #print(\"Entered :\", pvals[\"Cols\"][0], \"\\tR2 :\", model.rsquared)\n","                            selected_cols.append(pvals[\"Cols\"][0])\n","                            other_cols.remove(pvals[\"Cols\"][0])\n","                            criteria = new_criteria\n","                        else:\n","                            #print(\"break : Criteria\")\n","                            break           \n","                    elif  self._elimination_criteria == \"adjr2\" and model_type ==\"linear\":\n","                        new_criteria = model.rsquared_adj\n","                        if new_criteria > criteria:\n","                            #print(\"Entered :\", pvals[\"Cols\"][0], \"\\tAdjR2 :\", model.rsquared_adj)\n","                            selected_cols.append(pvals[\"Cols\"][0])\n","                            other_cols.remove(pvals[\"Cols\"][0])\n","                            criteria = new_criteria\n","                        else:\n","                            #print(\"Break : Criteria\")\n","                            break\n","                    else:\n","                        #print(\"Entered :\", pvals[\"Cols\"][0])\n","                        selected_cols.append(pvals[\"Cols\"][0])\n","                        other_cols.remove(pvals[\"Cols\"][0])            \n","\n","                else:\n","                    #print(\"Break : Significance Level\")\n","                    break\n","\n","            model = self.regressor(y, X_new[selected_cols])\n","            if self._elimination_criteria == \"aic\":\n","                criteria = model.aic\n","            elif self._elimination_criteria == \"bic\":\n","                criteria = model.bic\n","            elif self._elimination_criteria == \"r2\" and model_type ==\"linear\":\n","                criteria = model.rsquared\n","            elif self._elimination_criteria == \"adjr2\" and model_type ==\"linear\":\n","                criteria = model.rsquared_adj\n","\n","            # print(model.summary())\n","            # print(\"AIC: \"+str(model.aic))\n","            # print(\"BIC: \"+str(model.bic))\n","            # print(\"Final Variables:\", selected_cols)\n","        \n","        else: \n","            print(\"\\nWrong Strategy type: \"+ self._strategy +\"'\\nChoose another one.'\")\n","            \n","        X_new = X.copy()\n","        selected_cols.remove('intercept')\n","        X_new = X_new[selected_cols]        \n","\n","        return X_new"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Oc_QfdQ6aJuG","colab_type":"code","colab":{}},"source":["# df_test = new_data.copy()\n","# print(len(df_test.columns))\n","# target = df_raw[\"EnrollmentDuration\"]\n","\n","# instance = FeatureSelectorTransformerModels(targetColumn = y)\n","# new = instance.fit_transform(X = df_test)\n","# print(len(new.columns))\n","# new.head(10)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L-NwJCQDYiJY","colab_type":"text"},"source":["### Feature selection by feature characteristics"]},{"cell_type":"code","metadata":{"id":"5lARqYPnYWor","colab_type":"code","colab":{}},"source":["class FeatureSelectorTransformerAttributes( BaseEstimator, TransformerMixin ):\n","    \"\"\"\n","    Class for performing feature selection for machine learning or data preprocessing.\n","    \n","    Implements five different methods to identify features for removal \n","    \n","        1. Find columns with a missing percentage greater than a specified threshold\n","        2. Find columns with a single unique value\n","        3. Find collinear variables with a correlation greater than a specified correlation coefficient\n","        4. Find features with 0.0 feature importance from a gradient boosting machine (gbm)\n","        5. Find low importance features that do not contribute to a specified cumulative feature importance from the gbm\n","        \n","    Parameters\n","    --------\n","    \n","        target : array or series, default = None\n","            Array of target labels for training the machine learning model to find feature importances. \n","            These can be either binary labels (if ml_task is 'classification') or \n","            continuous targets (if ml_task is 'regression').\n","            If no labels are provided, then the feature importance based methods are not available.\n","        missing_threshold : float between 0 and 1\n","            Percentage of missing values of a feature above which the feature is nominated for exclusion.\n","            Default value = 0.6\n","        correlation_threshold : float between 0 and 1\n","            Value of the Pearson correlation cofficient for identifying correlation features\n","            Percentage of feature correlation above which a feature is eligible for exclusion.\n","            Default value = 0.98\n","        ml_task : string\n","            The machine learning task, either 'classification' or 'regression'\n","            Default value = \"regression\"\n","        cumulative_importance : float between 0 and 1\n","            The fraction of cumulative importance to account for\n","            \n","\n","        \n","    Attributes\n","    --------\n","    \n","    ops : dict\n","        Dictionary of operations run and features identified for removal\n","        \n","    missing_stats : dataframe\n","        The fraction of missing values for all features\n","    \n","    record_missing : dataframe\n","        The fraction of missing values for features with missing fraction above threshold\n","        \n","    unique_stats : dataframe\n","        Number of unique values for all features\n","    \n","    record_single_unique : dataframe\n","        Records the features that have a single unique value\n","        \n","    corr_matrix : dataframe\n","        All correlations between all features in the data\n","    \n","    record_collinear : dataframe\n","        Records the pairs of collinear variables with a correlation coefficient above the threshold\n","        \n","    feature_importances : dataframe\n","        All feature importances from the gradient boosting machine\n","    \n","    record_zero_importance : dataframe\n","        Records the zero importance features in the data according to the gbm\n","    \n","    record_low_importance : dataframe\n","        Records the lowest importance features not needed to reach the threshold of cumulative importance according to the gbm\n","    \n","    \n","    Notes\n","    --------\n","    \n","        - All 5 operations can be run with the `identify_all` method.\n","        - If using feature importances, one-hot encoding is used for categorical variables which creates new columns\n","\n","      https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0\n","    \n","    \"\"\"\n","    \n","    #Class constructor method that takes in a list of values as its argument\n","    def __init__(self, \n","                 target, \n","                 missing_threshold = 0.6,\n","                 correlation_threshold = 0.98,\n","                 ml_task = \"regression\",\n","                 cumulative_importance = 0.99\n","                ):\n","        \n","        self.one_hot_features = None\n","        self.ml_task = ml_task\n","        \n","        # Dataframes recording information about features to remove\n","        self.record_missing = None\n","        self.record_single_unique = None\n","        self.record_collinear = None\n","        self.record_zero_importance = None\n","        self.record_low_importance = None\n","        \n","        self.missing_stats = None\n","        self.unique_stats = None\n","        self.corr_matrix = None\n","        self.feature_importances = None\n","        \n","        # Dictionary to hold removal operations\n","        self.ops = {}\n","        \n","        self.one_hot_correlated = False\n","        \n","        self.target = target\n","        \n","        # Thresholds\n","        self.missing_threshold = missing_threshold\n","        self.correlation_threshold = correlation_threshold\n","        self.cumulative_importance = cumulative_importance\n","\n","        \n","    def fit( self, X, y = None ):\n","        self.base_features = list(X.columns)\n","        return self\n","\n","    def identify_missing(self, X):\n","        \"\"\"Find the features with a fraction of missing values above `missing_threshold`\"\"\"\n","\n","        # Calculate the fraction of missing in each column \n","        missing_series = X.isnull().sum() / X.shape[0]\n","        self.missing_stats = pd.DataFrame(missing_series).rename(columns = {'index': 'feature', 0: 'missing_fraction'})\n","\n","        # Sort with highest number of missing values on top\n","        self.missing_stats = self.missing_stats.sort_values('missing_fraction', ascending = False)\n","\n","        # Find the columns with a missing percentage above the threshold\n","        record_missing = pd.DataFrame(\n","            missing_series[missing_series > self.missing_threshold]).reset_index().rename(columns = {'index': 'feature', \n","                                                                                                     0: 'missing_fraction'})\n","        to_drop = list(record_missing['feature'])\n","\n","        self.record_missing = record_missing\n","        self.ops['missing'] = to_drop\n","        \n","        # print('%d features with greater than %0.2f missing values.\\n' % (len(self.ops['missing']), self.missing_threshold))\n","   \n","\n","    def identify_single_unique(self, X):\n","        \"\"\"Finds features with only a single unique value. NaNs do not count as a unique value. \"\"\"\n","\n","        # Calculate the unique counts in each column\n","        unique_counts = X.nunique()\n","        self.unique_stats = pd.DataFrame(unique_counts).rename(columns = {'index': 'feature', 0: 'nunique'})\n","        self.unique_stats = self.unique_stats.sort_values('nunique', ascending = True)\n","        \n","        # Find the columns with only one unique count\n","        record_single_unique = pd.DataFrame(unique_counts[unique_counts == 1]).reset_index().rename(columns = {'index': 'feature', \n","                                                                                                                0: 'nunique'})\n","\n","        to_drop = list(record_single_unique['feature'])\n","    \n","        self.record_single_unique = record_single_unique\n","        self.ops['single_unique'] = to_drop\n","        \n","        # print('%d features with a single unique value.\\n' % len(self.ops['single_unique']))\n","        \n","    \n","    def identify_collinear(self, X, one_hot=False):\n","        \"\"\"\n","        Finds collinear features based on the correlation coefficient between features. \n","        For each pair of features with a correlation coefficient greather than `correlation_threshold`,\n","        only one of the pair is identified for removal. \n","        Using code adapted from: https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n","        \n","        Parameters\n","        --------\n","        one_hot : boolean, default = False\n","            Whether to one-hot encode the features before calculating the correlation coefficients\n","        \"\"\"\n","        self.one_hot_correlated = one_hot\n","        \n","         # Calculate the correlations between every column\n","        if one_hot:\n","            \n","            # One hot encoding\n","            features = pd.get_dummies(X)\n","            self.one_hot_features = [column for column in features.columns if column not in self.base_features]\n","\n","            # Add one hot encoded data to original data\n","            self.data_all = pd.concat([features[self.one_hot_features], X], axis = 1)\n","            \n","            corr_matrix = pd.get_dummies(features).corr()\n","\n","        else:\n","            corr_matrix = X.corr()\n","        \n","        self.corr_matrix = corr_matrix\n","    \n","        # Extract the upper triangle of the correlation matrix\n","        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n","        \n","        # Select the features with correlations above the threshold\n","        # Need to use the absolute value\n","        to_drop = [column for column in upper.columns if any(upper[column].abs() > self.correlation_threshold)]\n","\n","        # Dataframe to hold correlated pairs\n","        record_collinear = pd.DataFrame(columns = ['drop_feature', 'corr_feature', 'corr_value'])\n","\n","        # Iterate through the columns to drop to record pairs of correlated features\n","        for column in to_drop:\n","\n","            # Find the correlated features\n","            corr_features = list(upper.index[upper[column].abs() > self.correlation_threshold])\n","\n","            # Find the correlated values\n","            corr_values = list(upper[column][upper[column].abs() > self.correlation_threshold])\n","            drop_features = [column for _ in range(len(corr_features))]    \n","\n","            # Record the information (need a temp df for now)\n","            temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,\n","                                             'corr_feature': corr_features,\n","                                             'corr_value': corr_values})\n","\n","            # Add to dataframe\n","            record_collinear = record_collinear.append(temp_df, ignore_index = True)\n","\n","        self.record_collinear = record_collinear\n","        self.ops['collinear'] = to_drop\n","        \n","        # print('%d features with a correlation magnitude greater than %0.2f.\\n' % (len(self.ops['collinear']), self.correlation_threshold))\n","\n","        \n","    def identify_zero_importance(self, X, eval_metric=\"l2\", \n","                                 n_iterations=10, early_stopping = True):\n","        \"\"\"\n","        \n","        Identify the features with zero importance according to a gradient boosting machine.\n","        The gbm can be trained with early stopping using a validation set to prevent overfitting. \n","        The feature importances are averaged over `n_iterations` to reduce variance. \n","        \n","        Uses the LightGBM implementation (http://lightgbm.readthedocs.io/en/latest/index.html)\n","        Parameters \n","        --------\n","        eval_metric : string\n","            Evaluation metric to use for the gradient boosting machine for early stopping. Must be\n","            provided if `early_stopping` is True\n","        n_iterations : int, default = 10\n","            Number of iterations to train the gradient boosting machine\n","            \n","        early_stopping : boolean, default = True\n","            Whether or not to use early stopping with a validation set when training\n","        \n","        \n","        Notes\n","        --------\n","        \n","        - Features are one-hot encoded to handle the categorical variables before training.\n","        - The gbm is not optimized for any particular task and might need some hyperparameter tuning\n","        - Feature importances, including zero importance features, can change across runs\n","        \"\"\"\n","\n","        if early_stopping and eval_metric is None:\n","            raise ValueError(\"\"\"eval metric must be provided with early stopping. Examples include \"auc\" for classification or\n","                             \"l2\" for regression.\"\"\")\n","            \n","        if self.target is None:\n","            raise ValueError(\"No training labels provided.\")\n","        \n","        # One hot encoding\n","        features = pd.get_dummies(X)\n","        self.one_hot_features = [column for column in features.columns if column not in self.base_features]\n","\n","        # Add one hot encoded data to original data\n","        self.data_all = pd.concat([features[self.one_hot_features], X], axis = 1)\n","\n","        # Extract feature names\n","        feature_names = list(features.columns)\n","\n","        # Convert to np array\n","        features = np.array(features)\n","        target = np.array(self.target).reshape((-1, ))\n","\n","        # Empty array for feature importances\n","        feature_importance_values = np.zeros(len(feature_names))\n","        \n","        # print('Training Gradient Boosting Model\\n')\n","        \n","        # Iterate through each fold\n","        for _ in range(n_iterations):\n","\n","            if self.ml_task == 'classification':\n","                model = lgb.LGBMClassifier(n_estimators=1000, learning_rate = 0.05, verbose = -1)\n","\n","            elif self.ml_task == 'regression':\n","                model = lgb.LGBMRegressor(n_estimators=1000, learning_rate = 0.05, verbose = -1)\n","\n","            else:\n","                raise ValueError('Task must be either \"classification\" or \"regression\"')\n","                \n","            # If training using early stopping need a validation set\n","            if early_stopping:\n","                \n","                train_features, valid_features, train_labels, valid_labels = train_test_split(features, \n","                                                                                              target, \n","                                                                                              test_size = 0.33,\n","                                                                                              random_state=42)\n","\n","                # Train the model with early stopping\n","                model.fit(train_features, train_labels, eval_metric = eval_metric,\n","                          eval_set = [(valid_features, valid_labels)],\n","                          early_stopping_rounds = 100, verbose = -1)\n","                \n","                # Clean up memory\n","                gc.enable()\n","                del train_features, train_labels, valid_features, valid_labels\n","                gc.collect()\n","                \n","            else:\n","                model.fit(features, target)\n","\n","            # Record the feature importances\n","            feature_importance_values += model.feature_importances_ / n_iterations\n","\n","        feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n","\n","        # Sort features according to importance\n","        feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index(drop = True)\n","\n","        # Normalize the feature importances to add up to one\n","        feature_importances['normalized_importance'] = feature_importances['importance'] / feature_importances['importance'].sum()\n","        feature_importances['cumulative_importance'] = np.cumsum(feature_importances['normalized_importance'])\n","\n","        # Extract the features with zero importance\n","        record_zero_importance = feature_importances[feature_importances['importance'] == 0.0]\n","        \n","        to_drop = list(record_zero_importance['feature'])\n","\n","        self.feature_importances = feature_importances\n","        self.record_zero_importance = record_zero_importance\n","        self.ops['zero_importance'] = to_drop\n","        \n","        # print('\\n%d features with zero importance after one-hot encoding.\\n' % len(self.ops['zero_importance']))\n","                     \n","            \n","    def identify_low_importance(self, X):\n","        \"\"\"\n","        Finds the lowest importance features not needed to account for `cumulative_importance` fraction\n","        of the total feature importance from the gradient boosting machine. As an example, if cumulative\n","        importance is set to 0.95, this will retain only the most important features needed to \n","        reach 95% of the total feature importance. The identified features are those not needed.\n","        \"\"\"\n","        \n","        # The feature importances need to be calculated before running\n","        if self.feature_importances is None:\n","            raise NotImplementedError(\"\"\"Feature importances have not yet been determined. \n","                                         Call the `identify_zero_importance` method first.\"\"\")\n","            \n","        # Make sure most important features are on top\n","        self.feature_importances = self.feature_importances.sort_values('cumulative_importance')\n","\n","        # Identify the features not needed to reach the cumulative_importance\n","        record_low_importance = self.feature_importances[self.feature_importances['cumulative_importance'] > \n","                                                         self.cumulative_importance]\n","\n","        to_drop = list(record_low_importance['feature'])\n","\n","        self.record_low_importance = record_low_importance\n","        self.ops['low_importance'] = to_drop\n","    \n","        # print('%d features required for cumulative importance of %0.2f after one hot encoding.' % \n","        #       (len(self.feature_importances) - len(self.record_low_importance), self.cumulative_importance))\n","        # print('%d features do not contribute to cumulative importance of %0.2f.\\n' % (len(self.ops['low_importance']),\n","        #                                                                                        self.cumulative_importance))\n","        \n","    def remove(self, X, methods, keep_one_hot = True):\n","        \"\"\"\n","        Remove the features from the data according to the specified methods.\n","        \n","        Parameters\n","        --------\n","            methods : 'all' or list of methods\n","                If methods == 'all', any methods that have identified features will be used\n","                Otherwise, only the specified methods will be used.\n","                Can be one of ['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance']\n","            keep_one_hot : boolean, default = True\n","                Whether or not to keep one-hot encoded features\n","                \n","        Return\n","        --------\n","            data : dataframe\n","                Dataframe with identified features removed\n","                \n","        \n","        Notes \n","        --------\n","            - If feature importances are used, the one-hot encoded columns will be added to the data (and then may be removed)\n","            - Check the features that will be removed before transforming data!\n","        \n","        \"\"\"\n","        \n","        \n","        features_to_drop = []\n","      \n","        if methods == 'all':\n","            \n","            # Need to use one-hot encoded data as well\n","            data = self.data_all\n","                                          \n","           # print('{} methods have been run\\n'.format(list(self.ops.keys())))\n","            \n","            # Find the unique features to drop\n","            features_to_drop = set(list(chain(*list(self.ops.values()))))\n","            \n","        else:\n","            # Need to use one-hot encoded data as well\n","            if 'zero_importance' in methods or 'low_importance' in methods or self.one_hot_correlated:\n","                data = self.data_all\n","                \n","            else:\n","                data = X\n","                \n","            # Iterate through the specified methods\n","            for method in methods:\n","                \n","                # Check to make sure the method has been run\n","                if method not in self.ops.keys():\n","                    raise NotImplementedError('%s method has not been run' % method)\n","                    \n","                # Append the features identified for removal\n","                else:\n","                    features_to_drop.append(self.ops[method])\n","        \n","            # Find the unique features to drop\n","            features_to_drop = set(list(chain(*features_to_drop)))\n","            \n","        features_to_drop = list(features_to_drop)\n","            \n","        if not keep_one_hot:\n","            \n","            if self.one_hot_features is None:\n","                print('Data has not been one-hot encoded')\n","            else:\n","                             \n","                features_to_drop = list(set(features_to_drop) | set(self.one_hot_features))\n","       \n","        # Remove the features and return the data\n","        data = data.drop(columns = features_to_drop)\n","        self.removed_features = features_to_drop\n","        \n","        # if not keep_one_hot:\n","        # \tprint('Removed %d features including one-hot features.' % len(features_to_drop))\n","        # else:\n","        # \tprint('Removed %d features.' % len(features_to_drop))\n","        \n","        return data\n","        \n","    def reset_plot(self):\n","        plt.rcParams = plt.rcParamsDefault\n","\n","    \n","    def transform(self, X , y = None ):\n","        \n","        X_new = X.copy()\n","        #1. Find columns with a missing percentage greater than a specified threshold\n","        self.identify_missing(X_new)\n","        # Find features identified for removal by accessing the ops attribute of the FeatureSelector\n","        missing_features = self.ops['missing']\n","        missing_features[:5]\n","        \n","        #2. Find columns with a single unique value\n","        self.identify_single_unique(X_new)\n","        \n","        #3. Find collinear variables with a correlation greater than a specified correlation coefficient\n","        # Identify collinear features\n","        self.identify_collinear(X = X_new)        \n","        # list of collinear features to remove\n","        collinear_features = self.ops['collinear']\n","        # dataframe of collinear features\n","        self.record_collinear.head()        \n","        \n","        #4. Find features with 0.0 feature importance from a gradient boosting machine (gbm)\n","        # Pass in the appropriate parameters\n","        self.identify_zero_importance(X = X_new,\n","                                      eval_metric = 'auc',\n","                                      n_iterations = 10,\n","                                      early_stopping = True)\n","        # list of zero importance features\n","        zero_importance_features = self.ops['zero_importance']\n","        \n","        #5. Find low importance features that do not contribute to a specified cumulative feature importance from the gbm\n","        self.identify_low_importance(X = X_new)\n","        self.feature_importances.head(10)\n","        \n","        # Remove the features from all methods \n","        # To also remove the one-hot encoded features that are created during machine learning, set \"keep_one_hot\" to True\n","        X_new = self.remove(X = X_new, methods = 'all', keep_one_hot=False)\n","                          \n","\n","        return X_new"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j0_T7jmcZB5m","colab_type":"code","colab":{}},"source":["# df_test = new_data.copy()\n","\n","# display(df_test.describe())\n","\n","# target = df_raw[\"EnrollmentDuration\"]\n","# print(target)\n","# instance = FeatureSelectorTransformerAttributes(target = target)\n","# new = instance.fit_transform(X = df_test)\n","# new.head(10)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"afQPhqN6mtJh","colab_type":"text"},"source":["##Standard Scaler for numerical values"]},{"cell_type":"code","metadata":{"id":"Px9rtNnhmwz7","colab_type":"code","colab":{}},"source":["class StandardScalerTransformer(TransformerMixin):\n","    \"\"\" Applies StandardScaler on data and returns a dataframe \"\"\"\n","\n","    def __init__(self):\n","        self.ss = None\n","        self.mean_ = None\n","        self.scale_ = None\n","\n","    def fit(self, X, y=None):\n","        self.ss = StandardScaler()\n","        self.ss.fit(X)\n","        self.mean_ = pd.Series(self.ss.mean_, index=X.columns)\n","        self.scale_ = pd.Series(self.ss.scale_, index=X.columns)\n","        return self\n","\n","    def transform(self, X):\n","        Xss = self.ss.transform(X)\n","        Xscaled = pd.DataFrame(Xss, index=X.index, columns=X.columns)\n","        return Xscaled"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9RoJcE2DHl_O","colab_type":"text"},"source":["##Target Transformers"]},{"cell_type":"code","metadata":{"id":"nzv6SwVVoXEt","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import KBinsDiscretizer\n","\n","class LabelEncoder(TransformerMixin):\n","    \"\"\"\n","    Strategy:\n","        equal_interval - same bin size (needs frequency=interval size)\n","        equal_frequency - same number of occurrences in each bin (needs n_bins and labels)\n","    \"\"\"\n","    def __init__(self, strategy='equal_frequency', n_bins = 5, labels = ['Very Short', 'Short', 'Medium', 'Long', 'Very Long'], frequency = 12):\n","        self.strategy = strategy\n","        self.n_bins = n_bins\n","        self.labels = labels\n","        self.frequency = frequency\n","\n","    def fit(self, X):\n","        if self.strategy == 'equal_interval':\n","            print(X.max())\n","            interval_range = pd.interval_range(start=0, freq=self.frequency, end=int(X.max())+self.frequency)\n","            self.groups, bin_edges = pd.cut(X[X.columns[0]], bins=interval_range, retbins=True, labels=False)\n","            self.results_table = pd.DataFrame(zip(bin_edges, range(0,len(interval_range))), columns=['Group', 'Label'])\n","            self.results = []\n","            for value in X[X.columns[0]]:\n","                for index, data in self.results_table.iterrows():\n","                    if value in data.Group: self.results.append(data.Label)\n","            display(self.results_table)\n","        else:\n","           self.results, bin_edges = pd.qcut(X[X.columns[0]], q=self.n_bins, labels=False, retbins=True)\n","           self.results_table = pd.DataFrame(zip(bin_edges, self.labels, range(0,len(self.labels))), columns=['Start Value', 'Label', 'Code'])\n","           display(self.results_table)\n","        return self\n","\n","    def transform(self, X):\n","        df_test = X.copy()\n","        df_test['new'] = self.results\n","        display(df_test)\n","        X_new = pd.DataFrame(self.results, index=X.index, columns=X.columns)\n","        return X_new"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3WyruHXslOms","colab_type":"code","colab":{}},"source":["df_test = df_raw.loc[:, ['EnrollmentDuration']].copy()\n","\n","display(df_test.describe())\n","\n","instance = LabelEncoder(strategy='equal_frequency')\n","new = instance.fit_transform(df_test)\n","display(new)\n","#display(new['EnrollmentDuration'].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JDuazTex-z1w","colab_type":"text"},"source":["##Training, Test, and Validation Sets\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"IgUj5mAoJQxV","colab_type":"code","colab":{}},"source":["#Write Transformer to integrate split into our pipeline\n","class DataSplit( BaseEstimator, TransformerMixin ):\n","  def _init_( self, debug=False ):\n","        self._debug = debug\n","\n","  def fit( self, X, y = None ):\n","        return self  \n","\n","  def transform( self, X, y = None ):\n","        #Set seed\n","        import random\n","        random.seed(2020)\n","\n","        #Split data to X and y\n","        y_new = X[\"EnrollmentDuration\"]\n","        X_new = X.drop(X[\"EnrollmentDuration\"])\n","\n","        #Split into test and train\n","        X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.1, random_state=42)\n","\n","        #Split further into train test and validation\n","        V_train, V_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","\n","        return X_train, y_train, X_test, y_test, V_train, V_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BDd1yQuNX5wh","colab_type":"code","cellView":"both","colab":{}},"source":["class DataSplit( BaseEstimator, TransformerMixin ):\n","  \"\"\"\n","  Strategy:\n","            random_sampling - randomly put trials into train and test\n","            stratified_sampling - make sure trials are evenl distributed in test and train set on a certain feature\n","            timeseries_sampling - avoid using future data to train model, split train an tets by StartDate\n","  \"\"\"\n","  def __init__( self, strategy = \"random_sampling\", debug=False ):\n","        self.strategy = strategy\n","        self._debug = debug\n","\n","  def fit( self, X, y = None ):\n","        return self  \n","\n","  def transform( self, X, y = None ):\n","        #Set seed\n","        import random\n","        random.seed(2020)\n","\n","        #1 Random Sampling\n","        if self.strategy== \"random_sampling\":\n","\n","            #Split data to X and y\n","            y_new = X[\"EnrollmentDuration\"]\n","            X_new = X.drop(X[\"EnrollmentDuration\"])\n","\n","            #Split into test and train\n","            X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.1, random_state=42)\n","\n","            #Split further into train test and validation\n","            V_train, V_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","\n","        #2 Stratified Sampling\n","        if self.strategy==\"stratified_sampling\":\n","\n","            X_new = X.copy()\n","            X_new[\"EnrollmentDurationBins\"]=0\n","\n","            X_new['EnrollmentDurationBins'].loc[X_new['EnrollmentDuration']<13]=0\n","            X_new['EnrollmentDurationBins'].loc[(X_new['EnrollmentDuration']>=13) & (X_new['EnrollmentDuration']<23)]=1\n","            X_new['EnrollmentDurationBins'].loc[(X_new['EnrollmentDuration']>=23) & (X_new['EnrollmentDuration']<37)]=2\n","            X_new['EnrollmentDurationBins'].loc[(X_new['EnrollmentDuration']>=37) & (X_new['EnrollmentDuration']<60)]=3\n","            X_new['EnrollmentDurationBins'].loc[X_new['EnrollmentDuration']>60]=4\n","\n","            y_new = X[\"EnrollmentDuration\"]\n","            X_new = X_new.drop(X[\"EnrollmentDuration\"])\n","\n","            #Split into test and train\n","            X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, stratify=new_data[\"EnrollmentDurationBins\"], test_size=0.1, random_state=42)\n","\n","            #Split further into train test and validation\n","            V_train, V_test, y_train, y_test = train_test_split(X_train, y_train, stratify=new_data[\"EnrollmentDurationBins\"], test_size=0.2, random_state=42)\n","\n","        #3 Time Series Sampling\n","        if self.strategy==\"timeseries_sampling\":\n","            X_new = X.sort_values(by=\"StartYear\")\n","\n","            y_new= X_new[\"EnrollmentDuration\"]\n","            X_new = X_new.drop(\"EnrollmentDuration\", axis =1)\n","\n","            #Time series split\n","            train_size = int(len(X_new) * 0.9)\n","            X_train, X_test = X_new[0:train_size], X_new[train_size:len(X_new)]\n","            y_train, y_test = y_new[0:train_size], y_new[train_size:len(y_new)]\n","            \n","            validation_size = int(len(X_train) * 0.8)\n","            V_train, V_test = X_train[0:validation_size], X_train[validation_size:len(X_train)]\n","            v_train, v_test = y_train[0:validation_size], y_train[validation_size:len(y_train)]\n","\n","        return X_train, y_train, X_test, y_test, V_train, V_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UE8y4W44HWMa","colab_type":"code","colab":{}},"source":["df_raw.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W3YZJ-zubVxc","colab_type":"text"},"source":["#Final Pipeline"]},{"cell_type":"code","metadata":{"id":"OAib0gqvo8hD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":610},"executionInfo":{"status":"ok","timestamp":1599213917978,"user_tz":-120,"elapsed":256285,"user":{"displayName":"陳維儀","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjADfM2DPAarNhBjfMwzTdbc4NzhB0rPW1yeU7ETNM=s64","userId":"06691591176760802855"}},"outputId":"2dca00a4-6a32-4763-d39f-1c6dc77b178e"},"source":["TARGET = ['EnrollmentDuration']\n","UNIQUE_FEATS = ['ConditionMeshId']\n","CAT_SINGLE_FEATS = ['HealthyVolunteers', 'Gender', 'IsFDARegulatedDrug', 'IsFDARegulatedDevice', 'DesignPrimaryPurpose', 'EnrollmentType', 'OrgClass','DesignAllocation','DesignInterventionModel']\n","CAT_MULTIPLE_FEATS = ['Phase', 'StdAge', 'CollaboratorClass', 'LeadSponsorClass', 'ConditionBrowseBranchAbbrev','InterventionType','ArmGroupType','BaselineDenomCountGroupId','BaselineMeasureDispersionType']\n","CAT_MULTIPLE_TOP_FEATS1 = ['LocationCountry', 'LocationCity', 'LocationFacility']\n","CAT_MULTIPLE_TOP_FEATS2 = ['Condition', 'ConditionAncestorTerm','Keyword']\n","NUM_FEATS = ['EnrollmentCount']\n","AGE_FEATS =['MaximumAge','MinimumAge']\n","TO_COUNT_FEATS = ['Condition', 'ConditionAncestorTerm', 'CollaboratorClass', 'EligibilityCriteria', 'LocationFacility', 'LocationCity', 'LocationCountry', 'ArmGroupLabel']\n","TIME_FEAT = ['StartDate']\n","TEXTUAL_FEATS1 = ['InterventionName', 'OrgFullName','LeadSponsorName', 'CollaboratorName', 'EligibilityCriteria', 'ArmGroupDescription', 'ArmGroupInterventionName', 'ArmGroupLabel', 'OfficialTitle', 'BriefSummary']\n","TEXTUAL_FEATS2 = ['EventsTimeFrame', 'FlowDropWithdrawType', 'FlowGroupDescription', 'FlowGroupTitle', 'FlowMilestoneType', 'FlowPeriodTitle', 'FlowRecruitmentDetails', 'BaselineCategoryTitle', 'BaselineClassTitle', 'BaselineGroupDescription', 'BaselineGroupTitle', \n","                    'BaselineMeasureTitle', 'BaselineMeasureUnitOfMeasure']\n","WORLDRANK_FEATS = ['LocationFacility']\n","REGIONAL_FEATS = ['LocationCity', 'LocationState', 'MaximumAge', 'MinimumAge']\n","POPULATION_FEAT = ['LocationPopulationDensity']\n","\n","ALL_FEATURES = list(set(UNIQUE_FEATS + CAT_SINGLE_FEATS + CAT_MULTIPLE_FEATS + CAT_MULTIPLE_TOP_FEATS1 +  CAT_MULTIPLE_TOP_FEATS2 + NUM_FEATS + TO_COUNT_FEATS + TEXTUAL_FEATS1 + TEXTUAL_FEATS2 + AGE_FEATS  + TIME_FEAT + POPULATION_FEAT + WORLDRANK_FEATS + REGIONAL_FEATS))\n","\n","# display(df_raw.head(5))\n","\n","pipeline = Pipeline([\n","    ('features', FeatureUnion([\n","        ('target', Pipeline([\n","            ('extract', FeatureSelector(TARGET))\n","        ])),\n","        ('categoricals_single', Pipeline([                 \n","           ('extract', FeatureSelector(CAT_SINGLE_FEATS)),\n","           ('cat_fill', MissingStringsTransformer(strategy='most_frequent')),\n","           ('single_one_hot_encoding', SingleOneHotEncoder()),\n","           ('excluder', FeatureExcluder(CAT_SINGLE_FEATS))\n","        ])),\n","        ('categoricals_multiple', Pipeline([\n","            ('extract', FeatureSelector(CAT_MULTIPLE_FEATS)),\n","            ('multiple_one_hot_encoding', MultipleOneHotEncoder()),\n","            ('excluder', FeatureExcluder(CAT_MULTIPLE_FEATS))\n","        ])),\n","        ('categoricals_top1', Pipeline([\n","            ('extract', FeatureSelector(CAT_MULTIPLE_TOP_FEATS1)),\n","            ('multiple_one_hot_encoding', MultipleTopOneHotEncoder(strategie=\"top\", top=50)),\n","            ('excluder', FeatureExcluder(CAT_MULTIPLE_TOP_FEATS1))\n","        ])),\n","        ('categoricals_top2', Pipeline([\n","            ('extract', FeatureSelector(CAT_MULTIPLE_TOP_FEATS2)),\n","            ('multiple_one_hot_encoding', MultipleTopOneHotEncoder(strategie=\"top\", top=100)),\n","            ('excluder', FeatureExcluder(CAT_MULTIPLE_TOP_FEATS2))\n","        ])),\n","        ('startMonth', Pipeline([\n","            ('extract', FeatureSelector(TIME_FEAT)),      \n","            ('startMonth', StartMonthTransformer()),\n","            ('excluder', FeatureExcluder(TIME_FEAT))\n","        ])),\n","        ('startYear', Pipeline([\n","            ('extract', FeatureSelector(TIME_FEAT)),      \n","            ('startYear', StartYearTransformer()),\n","            ('excluder', FeatureExcluder(TIME_FEAT))\n","        ])),\n","        ('Age_features',Pipeline([\n","            ('extract', FeatureSelector(AGE_FEATS)),\n","            ('cat_fill', MissingStringsTransformer(strategy='most_frequent')),\n","            ('toYear', ToYearTransformer()),\n","            ('excluder', FeatureExcluder(AGE_FEATS))                      \n","        ])),\n","        ('counting_features', Pipeline([\n","            ('extract', FeatureSelector(TO_COUNT_FEATS)),\n","            ('counter', DistinctCounter()),\n","            ('excluder', FeatureExcluder(TO_COUNT_FEATS))\n","        ])),\n","        ('textual_features', Pipeline([\n","            ('extract', FeatureSelector(TEXTUAL_FEATS1)),\n","            ('counter', TextualFeatureTransformer( n_keywords = 35)),\n","            ('excluder', FeatureExcluder(TEXTUAL_FEATS1))\n","        ])),\n","        ('textual_features', Pipeline([\n","            ('extract', FeatureSelector(TEXTUAL_FEATS2)),\n","            ('counter', TextualFeatureTransformer( n_keywords = 15)),\n","            ('excluder', FeatureExcluder(TEXTUAL_FEATS2))\n","        ])),\n","        ('numerics', Pipeline([\n","            ('extract', FeatureSelector(NUM_FEATS)),\n","            ('nr_fill', MissingValuesTransformer(imputer = 'KNNImputer', n_neighbors = 5, weights = \"uniform\")),\n","            ('excluder', FeatureExcluder(NUM_FEATS))\n","        ])),\n","        ('special', Pipeline([\n","            ('extract', FeatureSelector(UNIQUE_FEATS)),\n","            ('meshid_transformer', MeshIDTransformer()),\n","            ('excluder', FeatureExcluder(UNIQUE_FEATS))\n","        ])),\n","         ('worldrank', Pipeline([\n","             ('extract', FeatureSelector(WORLDRANK_FEATS)),\n","             ('worldrank_transformer', WorldRankTransformer(worldrankData = df_worldrank)),\n","             ('excluder', FeatureExcluder(WORLDRANK_FEATS))\n","         ])),\n","        ('regional', Pipeline([\n","            ('extract', FeatureSelector(REGIONAL_FEATS)),\n","            ('regional_transformer', RegionalAgeTransformer(countrylist=countrylist, df_regional=df_regional)),\n","            ('excluder', FeatureExcluder(REGIONAL_FEATS))\n","        ])),\n","        ('addFeatures', Pipeline([\n","            ('extract', FeatureSelector(ALL_FEATURES))\n","        ]))\n","        \n","    ])),\n","    ('patients_distribution', PatientsDistributionTransformer()),\n","    # Below is the combination of all alternative transformers. parameters including:\n","    # - \"transformer\" one of three: => 'totalCombine', 'perCity', 'perCountry'\n","    # - \"strategy\": => 'simple', 'weighted'\n","    # - \"mean\": => 'trials', 'worldwide'\n","    # Strategy and mean are only need if transformer = 'totalCombine'\n","    ('location_transformation', LocationDataTransformer(df_dbcountry, transformer='totalCombine', strategy='weighted', mean='worldwide')),\n","    ('excluder', FeatureExcluder(ALL_FEATURES))\n","])\n","\n","df_copy = df_raw.copy()\n","new_data = pipeline.fit_transform(df_copy[list(set(TARGET + ALL_FEATURES))])\n","new_data.to_csv(f\"pipeline_output.csv\", sep=\";\")\n","\n","print(f\"Length of new_data: {len(new_data)}\")\n","print(f\"Number of features: {len(new_data.columns)}\")\n","display(new_data.head(5))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  self._setitem_with_indexer(indexer, value)\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:92: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  self._setitem_with_indexer(indexer, value)\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:92: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"},{"output_type":"stream","text":["Length of new_data: 982\n","Number of features: 1043\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>EnrollmentDuration</th>\n","      <th>DesignAllocation=Non-Randomized</th>\n","      <th>DesignAllocation=Randomized</th>\n","      <th>DesignInterventionModel=Crossover Assignment</th>\n","      <th>DesignInterventionModel=Factorial Assignment</th>\n","      <th>DesignInterventionModel=Parallel Assignment</th>\n","      <th>DesignInterventionModel=Sequential Assignment</th>\n","      <th>DesignInterventionModel=Single Group Assignment</th>\n","      <th>DesignPrimaryPurpose=Diagnostic</th>\n","      <th>DesignPrimaryPurpose=Prevention</th>\n","      <th>DesignPrimaryPurpose=Supportive Care</th>\n","      <th>DesignPrimaryPurpose=Treatment</th>\n","      <th>EnrollmentType=Actual</th>\n","      <th>EnrollmentType=Anticipated</th>\n","      <th>Gender=All</th>\n","      <th>Gender=Female</th>\n","      <th>Gender=Male</th>\n","      <th>HealthyVolunteers=Accepts Healthy Volunteers</th>\n","      <th>HealthyVolunteers=No</th>\n","      <th>IsFDARegulatedDevice=No</th>\n","      <th>IsFDARegulatedDevice=Yes</th>\n","      <th>IsFDARegulatedDrug=No</th>\n","      <th>IsFDARegulatedDrug=Yes</th>\n","      <th>OrgClass=FED</th>\n","      <th>OrgClass=INDUSTRY</th>\n","      <th>OrgClass=NETWORK</th>\n","      <th>OrgClass=NIH</th>\n","      <th>OrgClass=OTHER</th>\n","      <th>Phase=Phase 2</th>\n","      <th>Phase=Phase 3</th>\n","      <th>Phase=Phase 1</th>\n","      <th>StdAge=Child</th>\n","      <th>StdAge=Older Adult</th>\n","      <th>StdAge=Adult</th>\n","      <th>CollaboratorClass=NIH</th>\n","      <th>CollaboratorClass=NETWORK</th>\n","      <th>CollaboratorClass=UNKNOWN</th>\n","      <th>CollaboratorClass=FED</th>\n","      <th>CollaboratorClass=OTHER</th>\n","      <th>CollaboratorClass=INDUSTRY</th>\n","      <th>...</th>\n","      <th>main_country_unemploymentRate</th>\n","      <th>avg_hospitalBed</th>\n","      <th>min_hospitalBed</th>\n","      <th>max_hospitalBed</th>\n","      <th>main_country_hospitalBed</th>\n","      <th>avg_healthExpenditure</th>\n","      <th>min_healthExpenditure</th>\n","      <th>max_healthExpenditure</th>\n","      <th>main_country_healthExpenditure</th>\n","      <th>avg_density</th>\n","      <th>min_density</th>\n","      <th>max_density</th>\n","      <th>main_country_density</th>\n","      <th>avg_fertilityRate</th>\n","      <th>min_fertilityRate</th>\n","      <th>max_fertilityRate</th>\n","      <th>main_country_fertilityRate</th>\n","      <th>avg_medianAge</th>\n","      <th>min_medianAge</th>\n","      <th>max_medianAge</th>\n","      <th>main_country_medianAge</th>\n","      <th>avg_migrantsNet</th>\n","      <th>min_migrantsNet</th>\n","      <th>max_migrantsNet</th>\n","      <th>main_country_migrantsNet</th>\n","      <th>avg_sizeInKm2</th>\n","      <th>min_sizeInKm2</th>\n","      <th>max_sizeInKm2</th>\n","      <th>main_country_sizeInKm2</th>\n","      <th>avg_urbanPopulation</th>\n","      <th>min_urbanPopulation</th>\n","      <th>max_urbanPopulation</th>\n","      <th>main_country_urbanPopulation</th>\n","      <th>avg_worldshare</th>\n","      <th>min_worldshare</th>\n","      <th>max_worldshare</th>\n","      <th>main_country_worldshare</th>\n","      <th>avg_city_population</th>\n","      <th>min_city_population</th>\n","      <th>max_city_population</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>17.0</td>\n","      <td>17.0</td>\n","      <td>17.0</td>\n","      <td>17.0</td>\n","      <td>36.0</td>\n","      <td>36.0</td>\n","      <td>36.0</td>\n","      <td>36.0</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>954806.0</td>\n","      <td>954806.0</td>\n","      <td>954806.0</td>\n","      <td>954806.0</td>\n","      <td>9147420.0</td>\n","      <td>9147420.0</td>\n","      <td>9147420.0</td>\n","      <td>9147420.0</td>\n","      <td>83.0</td>\n","      <td>83.0</td>\n","      <td>83.0</td>\n","      <td>83.0</td>\n","      <td>4.25</td>\n","      <td>4.25</td>\n","      <td>4.25</td>\n","      <td>4.25</td>\n","      <td>3588.069664</td>\n","      <td>158.630005</td>\n","      <td>11377.091797</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>172</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>17.0</td>\n","      <td>17.0</td>\n","      <td>17.0</td>\n","      <td>17.0</td>\n","      <td>36.0</td>\n","      <td>36.0</td>\n","      <td>36.0</td>\n","      <td>36.0</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>954806.0</td>\n","      <td>954806.0</td>\n","      <td>954806.0</td>\n","      <td>954806.0</td>\n","      <td>9147420.0</td>\n","      <td>9147420.0</td>\n","      <td>9147420.0</td>\n","      <td>9147420.0</td>\n","      <td>83.0</td>\n","      <td>83.0</td>\n","      <td>83.0</td>\n","      <td>83.0</td>\n","      <td>4.25</td>\n","      <td>4.25</td>\n","      <td>4.25</td>\n","      <td>4.25</td>\n","      <td>1717.821167</td>\n","      <td>1717.821167</td>\n","      <td>1717.821167</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>39</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>17.0</td>\n","      <td>17.0</td>\n","      <td>17.0</td>\n","      <td>17.0</td>\n","      <td>36.0</td>\n","      <td>36.0</td>\n","      <td>36.0</td>\n","      <td>36.0</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>954806.0</td>\n","      <td>954806.0</td>\n","      <td>954806.0</td>\n","      <td>954806.0</td>\n","      <td>9147420.0</td>\n","      <td>9147420.0</td>\n","      <td>9147420.0</td>\n","      <td>9147420.0</td>\n","      <td>83.0</td>\n","      <td>83.0</td>\n","      <td>83.0</td>\n","      <td>83.0</td>\n","      <td>4.25</td>\n","      <td>4.25</td>\n","      <td>4.25</td>\n","      <td>4.25</td>\n","      <td>2541.599437</td>\n","      <td>61.538631</td>\n","      <td>11377.091797</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>93</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>17.0</td>\n","      <td>17.0</td>\n","      <td>17.0</td>\n","      <td>17.0</td>\n","      <td>36.0</td>\n","      <td>36.0</td>\n","      <td>36.0</td>\n","      <td>36.0</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>954806.0</td>\n","      <td>954806.0</td>\n","      <td>954806.0</td>\n","      <td>954806.0</td>\n","      <td>9147420.0</td>\n","      <td>9147420.0</td>\n","      <td>9147420.0</td>\n","      <td>9147420.0</td>\n","      <td>83.0</td>\n","      <td>83.0</td>\n","      <td>83.0</td>\n","      <td>83.0</td>\n","      <td>4.25</td>\n","      <td>4.25</td>\n","      <td>4.25</td>\n","      <td>4.25</td>\n","      <td>241.777069</td>\n","      <td>241.777069</td>\n","      <td>241.777069</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>123</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>17.0</td>\n","      <td>17.0</td>\n","      <td>17.0</td>\n","      <td>17.0</td>\n","      <td>36.0</td>\n","      <td>36.0</td>\n","      <td>36.0</td>\n","      <td>36.0</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>1.8</td>\n","      <td>954806.0</td>\n","      <td>954806.0</td>\n","      <td>954806.0</td>\n","      <td>954806.0</td>\n","      <td>9147420.0</td>\n","      <td>9147420.0</td>\n","      <td>9147420.0</td>\n","      <td>9147420.0</td>\n","      <td>83.0</td>\n","      <td>83.0</td>\n","      <td>83.0</td>\n","      <td>83.0</td>\n","      <td>4.25</td>\n","      <td>4.25</td>\n","      <td>4.25</td>\n","      <td>4.25</td>\n","      <td>7901.219238</td>\n","      <td>7901.219238</td>\n","      <td>7901.219238</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 1043 columns</p>\n","</div>"],"text/plain":["    EnrollmentDuration  ...  max_city_population\n","0                   37  ...         11377.091797\n","1                  172  ...          1717.821167\n","2                   39  ...         11377.091797\n","15                  93  ...           241.777069\n","16                 123  ...          7901.219238\n","\n","[5 rows x 1043 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"FWtvznKbK-XS","colab_type":"code","colab":{}},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","# %cd /content/drive/My\\ Drive/Team\\ Project\\ (Sovanta)/Google\\ Collab/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WGdGcYfXLc--","colab_type":"code","colab":{}},"source":["new_data.to_csv(\"dataframe_after_preprocessing.csv\", index=False)\n","\n","# please also run this code to save pipeline settings (needed for backend of our web application)\n","import joblib\n","joblib.dump(pipeline, 'final_pipeline.joblib')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N0pO0DlzJokZ","colab_type":"text"},"source":["# Final Pipeline (to use in target and categorical encoding)\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Ympb4vP8JzdO","colab_type":"code","colab":{}},"source":["class TargetConditionTransformer( TransformerMixin, BaseEstimator ):\n","    def __init__( self, debug=False):\n","        self._debug = debug\n","\n","    def fit( self, X, y = None ):\n","        return self \n","    \n","    def transform(self, X , y = None ):\n","\n","        X_new = pd.DataFrame(X)\n","        X_new=X_new.reset_index()\n","        \n","        X_new[\"Condition1\"]=0\n","        X_new[\"Condition2\"]=0\n","\n","        for i, row in X_new.iterrows():\n","          X_new[\"Condition1\"].iloc[i]= X_new[\"Condition\"].iloc[i][0]\n","          if len(X_new[\"Condition\"].iloc[i])>1:\n","            X_new[\"Condition2\"].iloc[i]= X_new[\"Condition\"].iloc[i][1]\n","          else:\n","            X_new[\"Condition2\"].iloc[i]= \"No second Condition\"\n","        return X_new"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0mtoJxNwJtKu","colab_type":"code","colab":{}},"source":["TARGET = ['EnrollmentDuration']\n","UNIQUE_FEATS = ['ConditionMeshId']\n","CAT_SINGLE_FEATS = ['HealthyVolunteers', 'Gender', 'IsFDARegulatedDrug', 'IsFDARegulatedDevice', 'DesignPrimaryPurpose', 'EnrollmentType', 'OrgClass','DesignAllocation','DesignInterventionModel']\n","CAT_MULTIPLE_FEATS = ['Phase', 'StdAge', 'CollaboratorClass', 'LeadSponsorClass', 'ConditionBrowseBranchAbbrev','InterventionType','ArmGroupType','BaselineDenomCountGroupId','BaselineMeasureDispersionType']\n","CAT_MULTIPLE_TOP_FEATS1 = ['LocationCountry', 'LocationCity', 'LocationFacility']\n","CAT_MULTIPLE_TOP_FEATS2 = ['Condition', 'ConditionAncestorTerm','Keyword']\n","NUM_FEATS = ['EnrollmentCount']\n","AGE_FEATS =['MaximumAge','MinimumAge']\n","TO_COUNT_FEATS = ['Condition', 'ConditionAncestorTerm', 'CollaboratorClass', 'EligibilityCriteria', 'LocationFacility', 'LocationCity', 'LocationCountry', 'ArmGroupLabel']\n","TIME_FEAT = ['StartDate']\n","TEXTUAL_FEATS1 = ['InterventionName', 'OrgFullName','LeadSponsorName', 'CollaboratorName', 'EligibilityCriteria', 'ArmGroupDescription', 'ArmGroupInterventionName', 'ArmGroupLabel', 'OfficialTitle', 'BriefSummary']\n","TEXTUAL_FEATS2 = ['EventsTimeFrame', 'FlowDropWithdrawType', 'FlowGroupDescription', 'FlowGroupTitle', 'FlowMilestoneType', 'FlowPeriodTitle', 'FlowRecruitmentDetails', 'BaselineCategoryTitle', 'BaselineClassTitle', 'BaselineGroupDescription', 'BaselineGroupTitle', \n","                    'BaselineMeasureTitle', 'BaselineMeasureUnitOfMeasure']\n","WORLDRANK_FEATS = ['LocationFacility']\n","REGIONAL_FEATS = ['LocationCity', 'LocationState', 'MaximumAge', 'MinimumAge']\n","POPULATION_FEAT = ['LocationPopulationDensity']\n","\n","ALL_FEATURES = list(set(UNIQUE_FEATS +  CAT_MULTIPLE_FEATS + CAT_MULTIPLE_TOP_FEATS1 +  CAT_MULTIPLE_TOP_FEATS2 + NUM_FEATS + TO_COUNT_FEATS + TEXTUAL_FEATS + AGE_FEATS  + TIME_FEAT + POPULATION_FEAT + WORLDRANK_FEATS + REGIONAL_FEATS))\n","\n","# display(df_raw.head(5))\n","\n","pipeline = Pipeline([\n","    ('features', FeatureUnion([\n","        ('target', Pipeline([\n","            ('extract', FeatureSelector(TARGET))\n","        ])),\n","        ('targetcondition', Pipeline([ \n","        ('extract', FeatureSelector('Condition')),\n","        ('target_Condition',TargetConditionTransformer())\n","        ])),\n","        ('categoricals_single', Pipeline([                 \n","           ('extract', FeatureSelector(CAT_SINGLE_FEATS)),\n","           ('cat_fill', MissingStringsTransformer(strategy='most_frequent'))\n","        ])),\n","          ('categoricals_multiple', Pipeline([\n","            ('extract', FeatureSelector(CAT_MULTIPLE_FEATS)),\n","            ('multiple_one_hot_encoding', MultipleOneHotEncoder()),\n","            ('excluder', FeatureExcluder(CAT_MULTIPLE_FEATS))\n","        ])),\n","        ('categoricals_top1', Pipeline([\n","            ('extract', FeatureSelector(CAT_MULTIPLE_TOP_FEATS1)),\n","            ('multiple_one_hot_encoding', MultipleTopOneHotEncoder(strategie=\"top\", top=50)),\n","            ('excluder', FeatureExcluder(CAT_MULTIPLE_TOP_FEATS1))\n","        ])),\n","        ('categoricals_top2', Pipeline([\n","            ('extract', FeatureSelector(CAT_MULTIPLE_TOP_FEATS2)),\n","            ('multiple_one_hot_encoding', MultipleTopOneHotEncoder(strategie=\"top\", top=100)),\n","            ('excluder', FeatureExcluder(CAT_MULTIPLE_TOP_FEATS2))\n","        ])),\n","        ('startMonth', Pipeline([\n","            ('extract', FeatureSelector(TIME_FEAT)),      \n","            ('startMonth', StartMonthTransformer()),\n","            ('excluder', FeatureExcluder(TIME_FEAT))\n","        ])),\n","        ('startYear', Pipeline([\n","            ('extract', FeatureSelector(TIME_FEAT)),      \n","            ('startYear', StartYearTransformer()),\n","            ('excluder', FeatureExcluder(TIME_FEAT))\n","        ])),\n","        ('Age_features',Pipeline([\n","            ('extract', FeatureSelector(AGE_FEATS)),\n","            ('cat_fill', MissingStringsTransformer(strategy='most_frequent')),\n","            ('toYear', ToYearTransformer()),\n","            ('excluder', FeatureExcluder(AGE_FEATS))                      \n","        ])),\n","        ('counting_features', Pipeline([\n","            ('extract', FeatureSelector(TO_COUNT_FEATS)),\n","            ('counter', DistinctCounter()),\n","            ('excluder', FeatureExcluder(TO_COUNT_FEATS))\n","        ])),\n","        ('textual_features', Pipeline([\n","            ('extract', FeatureSelector(TEXTUAL_FEATS1)),\n","            ('counter', TextualFeatureTransformer( n_keywords = 35)),\n","            ('excluder', FeatureExcluder(TEXTUAL_FEATS1))\n","        ])),\n","        ('textual_features', Pipeline([\n","            ('extract', FeatureSelector(TEXTUAL_FEATS2)),\n","            ('counter', TextualFeatureTransformer( n_keywords = 15)),\n","            ('excluder', FeatureExcluder(TEXTUAL_FEATS2))\n","        ])),\n","        ('numerics', Pipeline([\n","            ('extract', FeatureSelector(NUM_FEATS)),\n","            ('nr_fill', MissingValuesTransformer(imputer = 'KNNImputer', n_neighbors = 5, weights = \"uniform\")),\n","            ('excluder', FeatureExcluder(NUM_FEATS))\n","        ])),\n","        ('special', Pipeline([\n","            ('extract', FeatureSelector(UNIQUE_FEATS)),\n","            ('meshid_transformer', MeshIDTransformer()),\n","            ('excluder', FeatureExcluder(UNIQUE_FEATS))\n","        ])),\n","         ('worldrank', Pipeline([\n","             ('extract', FeatureSelector(WORLDRANK_FEATS)),\n","             ('worldrank_transformer', WorldRankTransformer(worldrankData = df_worldrank)),\n","             ('excluder', FeatureExcluder(WORLDRANK_FEATS))\n","         ])),\n","        ('regional', Pipeline([\n","            ('extract', FeatureSelector(REGIONAL_FEATS)),\n","            ('regional_transformer', RegionalAgeTransformer()),\n","            ('excluder', FeatureExcluder(REGIONAL_FEATS))\n","        ])),\n","        ('addFeatures', Pipeline([\n","            ('extract', FeatureSelector(ALL_FEATURES))\n","        ]))\n","        \n","    ])),\n","    ('patients_distribution', PatientsDistributionTransformer()),\n","    # Below is the combination of all alternative transformers. parameters including:\n","    # - \"transformer\" one of three: => 'totalCombine', 'perCity', 'perCountry'\n","    # - \"strategy\": => 'simple', 'weighted'\n","    # - \"mean\": => 'trials', 'worldwide'\n","    # Strategy and mean are only need if transformer = 'totalCombine'\n","    ('location_transformation', LocationDataTransformer(df_dbcountry, transformer='totalCombine', strategy='weighted', mean='worldwide')),\n","    ('excluder', FeatureExcluder(ALL_FEATURES))\n","])\n","\n","df_copy = df_raw.copy()\n","target_data = pipeline.fit_transform(df_copy[list(set(TARGET + ALL_FEATURES + CAT_SINGLE_FEATS ))])\n","target_data = target_data.drop(columns = ['index'])\n","target_data.to_csv(f\"pipeline_target_output.csv\", sep=\";\")\n","\n","print(f\"Length of target_data: {len(target_data)}\")\n","print(f\"Number of features: {len(target_data.columns)}\")\n","display(target_data.head(5))"],"execution_count":null,"outputs":[]}]}